{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f25489ee",
   "metadata": {},
   "source": [
    "### Query Enhancement – Query Expansion Techniques\n",
    "\n",
    "In a RAG pipeline, the quality of the query sent to the retriever determines how good the retrieved context is — and therefore, how accurate the LLM’s final answer will be.\n",
    "\n",
    "That’s where Query Expansion / Enhancement comes in.\n",
    "\n",
    "#### 🎯 What is Query Enhancement?\n",
    "Query enhancement refers to techniques used to improve or reformulate the user query to retrieve better, more relevant documents from the knowledge base.\n",
    "It is especially useful when:\n",
    "\n",
    "- The original query is short, ambiguous, or under-specified\n",
    "- You want to broaden the scope to catch synonyms, related phrases, or spelling variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9ff6ab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9dbe7b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K = 5\n",
    "TOP_J = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "410d3b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "## step1 : Load and split the dataset\n",
    "loader = TextLoader(\"langchain_crewai_dataset.txt\")\n",
    "raw_docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(raw_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1365028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic splitter\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", model_kwargs={\"device\": \"cuda\"})\n",
    "splitter = SemanticChunker(embedding_model)\n",
    "chunks = splitter.split_documents(raw_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "791ba3d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "984258ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "def build_faiss_with_progress(docs, embedding_model, batch_size=32, normalize=True):\n",
    "    \"\"\"\n",
    "    Construye un índice FAISS desde documentos mostrando una barra de progreso.\n",
    "\n",
    "    Args:\n",
    "        docs (List[Document]): Lista de documentos de LangChain.\n",
    "        embedding_model: Instancia de HuggingFaceEmbeddings u otro modelo compatible.\n",
    "        batch_size (int): Tamaño de lote para embeddings.\n",
    "        normalize (bool): Si normalizar embeddings (coseno). Equivale a normalize_L2=True.\n",
    "\n",
    "    Returns:\n",
    "        FAISS: Vectorstore listo para usar como retriever.\n",
    "    \"\"\"\n",
    "    texts = [d.page_content for d in docs]\n",
    "    embeddings = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generando embeddings\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        batch_emb = embedding_model.embed_documents(batch)\n",
    "        embeddings.extend(batch_emb)\n",
    "\n",
    "    # Crear FAISS desde embeddings precomputados\n",
    "    vectorstore = FAISS.from_embeddings(\n",
    "        list(zip(texts, embeddings)),\n",
    "        embedding=embedding_model,\n",
    "        normalize_L2=normalize\n",
    "    )\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f001ff8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando embeddings: 100%|██████████| 16/16 [00:00<00:00, 27.08it/s]\n"
     ]
    }
   ],
   "source": [
    "### step 2: Vector Store\n",
    "embedding_model=HuggingFaceEmbeddings(\n",
    "    model_name=\"google/embeddinggemma-300m\",\n",
    "    model_kwargs={\"device\": \"cuda\"}  # <--- GPU\n",
    ")\n",
    "vectorstore=build_faiss_with_progress(chunks, embedding_model, batch_size=16, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "feb3461e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x70d67829fdd0>, search_kwargs={'k': 25})"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## step 3:MMR Retriever\n",
    "retriever=vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":25})\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "aabd1f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever, TFIDFRetriever \n",
    "\n",
    "sparse_retriever=TFIDFRetriever.from_documents(chunks)\n",
    "sparse_retriever.k=25 ##top-J documents to retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2e60fe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "hybrid_retriever=EnsembleRetriever(\n",
    "    retrievers=[retriever,sparse_retriever],\n",
    "    weights=[0.7,0.3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4a7e107e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates: 30\n",
      "page_content='One of the standout features of LangChain is its support for agents. Agents use LLMs to reason about which tool to call, what input to provide, and how to process the output. LangChain agents can execute multi-step tasks, integrating with tools like web search, calculators, code execution' metadata={'source': 'langchain_crewai_dataset.txt'}\n"
     ]
    }
   ],
   "source": [
    "candidates = hybrid_retriever.invoke(\"What is Langchain?\")\n",
    "print(f\"Number of candidates: {len(candidates)}\")\n",
    "print(candidates[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9de06531",
   "metadata": {},
   "outputs": [],
   "source": [
    "## re-rank with cross-encoder\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "import numpy as np\n",
    "\n",
    "reranker  = CrossEncoder(\"BAAI/bge-reranker-large\", device=\"cuda\")\n",
    "\n",
    "def rerank_with_crossencoder(query, docs, top_k=5):\n",
    "    \"\"\"\n",
    "    Reordena documentos con un CrossEncoder.\n",
    "    \n",
    "    Args:\n",
    "        query (str): la consulta.\n",
    "        docs (List[Document]): lista de LangChain Document.\n",
    "        top_k (int): cuántos documentos devolver.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[Document, float]]: documentos ordenados con sus scores.\n",
    "    \"\"\"\n",
    "    # 2) Crear pares (query, documento)\n",
    "    pairs = [(query, d.page_content) for d in docs]\n",
    "\n",
    "    # 3) Obtener puntuaciones\n",
    "    scores = reranker.predict(pairs)\n",
    "\n",
    "    # 4) Ordenar por score descendente\n",
    "    ranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return ranked[:top_k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7357d73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the standout features of LangChain is its support for agents. Agents use LLMs to reason about which tool to call, what input to provide, and how to process the output. LangChain agents can execute multi-step tasks, integrating with tools like web search, calculators, code execution\n",
      "-----\n",
      "LangChain integrates seamlessly with vector databases like FAISS, Chroma, Pinecone, and Weaviate, enabling semantic search within large document corpora. This capability is especially important in Retrieval-Augmented Generation (RAG), where external knowledge is fetched and injected into the LLM\n",
      "-----\n",
      "CrewAI is compatible with LangChain agents and tools, allowing hybrid systems where LangChain handles retrieval and tool wrapping, while CrewAI manages role-based collaboration. (v9)\n",
      "-----\n",
      "CrewAI is compatible with LangChain agents and tools, allowing hybrid systems where LangChain handles retrieval and tool wrapping, while CrewAI manages role-based collaboration. (v5)\n",
      "-----\n",
      "CrewAI is compatible with LangChain agents and tools, allowing hybrid systems where LangChain handles retrieval and tool wrapping, while CrewAI manages role-based collaboration. (v6)\n",
      "-----\n",
      "CrewAI is compatible with LangChain agents and tools, allowing hybrid systems where LangChain handles retrieval and tool wrapping, while CrewAI manages role-based collaboration. (v8)\n",
      "-----\n",
      "CrewAI is compatible with LangChain agents and tools, allowing hybrid systems where LangChain handles retrieval and tool wrapping, while CrewAI manages role-based collaboration. (v4)\n",
      "-----\n",
      "CrewAI is compatible with LangChain agents and tools, allowing hybrid systems where LangChain handles retrieval and tool wrapping, while CrewAI manages role-based collaboration. (v1)\n",
      "-----\n",
      "CrewAI is compatible with LangChain agents and tools, allowing hybrid systems where LangChain handles retrieval and tool wrapping, while CrewAI manages role-based collaboration. (v3)\n",
      "-----\n",
      "and dynamically communicating with one another. (v10)\n",
      "-----\n",
      "like web search, calculators, code execution environments, and custom APIs. (v8)\n",
      "-----\n",
      "like web search, calculators, code execution environments, and custom APIs. (v4)\n",
      "-----\n",
      "like web search, calculators, code execution environments, and custom APIs. (v9)\n",
      "-----\n",
      "like web search, calculators, code execution environments, and custom APIs. (v6)\n",
      "-----\n",
      "like web search, calculators, code execution environments, and custom APIs. (v7)\n",
      "-----\n",
      "like web search, calculators, code execution environments, and custom APIs. (v5)\n",
      "-----\n",
      "like web search, calculators, code execution environments, and custom APIs. (v1)\n",
      "-----\n",
      "like web search, calculators, code execution environments, and custom APIs. (v3)\n",
      "-----\n",
      "like web search, calculators, code execution environments, and custom APIs. (v2)\n",
      "-----\n",
      "like web search, calculators, code execution environments, and custom APIs. (v10)\n",
      "-----\n",
      "easy to compose and reuse chains using standard patterns like Stuff, Map-Reduce, and Refine. (v9)\n",
      "-----\n",
      "easy to compose and reuse chains using standard patterns like Stuff, Map-Reduce, and Refine. (v7)\n",
      "-----\n",
      "easy to compose and reuse chains using standard patterns like Stuff, Map-Reduce, and Refine. (v8)\n",
      "-----\n",
      "easy to compose and reuse chains using standard patterns like Stuff, Map-Reduce, and Refine. (v1)\n",
      "-----\n",
      "and agent orchestration. Developers can use LangChain to create end-to-end pipelines that connect LLMs with tools, APIs, vector databases, and other knowledge sources. (v1)\n",
      "-----\n",
      "and agent orchestration. Developers can use LangChain to create end-to-end pipelines that connect LLMs with tools, APIs, vector databases, and other knowledge sources. (v9)\n",
      "-----\n",
      "easy to compose and reuse chains using standard patterns like Stuff, Map-Reduce, and Refine. (v3)\n",
      "-----\n",
      "CrewAI is compatible with LangChain agents and tools, allowing hybrid systems where LangChain handles retrieval and tool wrapping, while CrewAI manages role-based collaboration. (v7)\n",
      "-----\n",
      "CrewAI is compatible with LangChain agents and tools, allowing hybrid systems where LangChain handles retrieval and tool wrapping, while CrewAI manages role-based collaboration. (v10)\n",
      "-----\n",
      "CrewAI is compatible with LangChain agents and tools, allowing hybrid systems where LangChain handles retrieval and tool wrapping, while CrewAI manages role-based collaboration. (v2)\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for c in candidates:\n",
    "    print(c.page_content)\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "92c8b9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Score: 0.0337] and agent orchestration. Developers can use LangChain to create end-to-end pipelines that connect LLMs with tools, APIs, vector databases, and other knowledge sources. (v1)\n",
      "[Score: 0.0325] and agent orchestration. Developers can use LangChain to create end-to-end pipelines that connect LLMs with tools, APIs, vector databases, and other knowledge sources. (v9)\n",
      "[Score: 0.0169] One of the standout features of LangChain is its support for agents. Agents use LLMs to reason about which tool to call, what input to provide, and how to process the output. LangChain agents can execute multi-step tasks, integrating with tools like web search, calculators, code execution\n",
      "[Score: 0.0090] LangChain integrates seamlessly with vector databases like FAISS, Chroma, Pinecone, and Weaviate, enabling semantic search within large document corpora. This capability is especially important in Retrieval-Augmented Generation (RAG), where external knowledge is fetched and injected into the LLM\n",
      "[Score: 0.0060] like web search, calculators, code execution environments, and custom APIs. (v7)\n"
     ]
    }
   ],
   "source": [
    "# Re-rankear con cross-encoder\n",
    "reranked = rerank_with_crossencoder(\"What is Langchain?\", candidates, top_k=5)\n",
    "\n",
    "# Mostrar resultados\n",
    "for doc, score in reranked:\n",
    "    print(f\"[Score: {score:.4f}] {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8d492cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a platform that allows developers to create end-to-end pipelines connecting Large Language Models (LLMs) with tools, APIs, vector databases, and other knowledge sources. It also supports agents that use LLMs to reason about tasks and execute multi-step processes. Additionally, LangChain integrates seamlessly with vector databases for semantic search within large document corpora.\n"
     ]
    }
   ],
   "source": [
    "# generar respuesta con LLM usando los documentos re-rankeados\n",
    "llm = init_chat_model(\"gpt-3.5-turbo\", temperature=0, max_tokens=512)\n",
    "prompt_template = \"\"\"Eres un asistente útil y preciso. Usa la siguiente información para responder a la pregunta al final.\n",
    "{context}\n",
    "Pregunta: {question}\n",
    "Respuesta:\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    "    output_parser=StrOutputParser()\n",
    ")\n",
    "chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "response = chain.invoke({\n",
    "    \"context\": [doc for doc, score in reranked],\n",
    "    \"question\": \"What is Langchain?\"\n",
    "})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14e8f2fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7c421dd94550>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7c421dbe43d0>, root_client=<openai.OpenAI object at 0x7c421df73d50>, root_async_client=<openai.AsyncOpenAI object at 0x7c421dbe40d0>, model_name='o4-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## step 4 : LLM and Prompt\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm=init_chat_model(\"openai:o4-mini\")\n",
    "llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "116e2cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='\\nYou are a helpful assistant. Expand the following query to improve document retrieval by adding relevant synonyms, technical terms, and useful context.\\n\\nOriginal query: \"{query}\"\\n\\nExpanded query:\\n')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7c421dd94550>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7c421dbe43d0>, root_client=<openai.OpenAI object at 0x7c421df73d50>, root_async_client=<openai.AsyncOpenAI object at 0x7c421dbe40d0>, model_name='o4-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query expansion\n",
    "query_expansion_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant. Expand the following query to improve document retrieval by adding relevant synonyms, technical terms, and useful context.\n",
    "\n",
    "Original query: \"{query}\"\n",
    "\n",
    "Expanded query:\n",
    "\"\"\")\n",
    "\n",
    "query_expansion_chain=query_expansion_prompt| llm | StrOutputParser()\n",
    "query_expansion_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d629dd9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“LangChain memory” OR “Lang Chain memory management” OR “Langchain conversational memory” OR “persistent context storage” OR “stateful agent memory” OR “session storage for LLMs” OR “memory modules” OR “memory backends” OR “ConversationBufferMemory” OR “ConversationSummaryMemory” OR “EntityMemory” OR “CombinedMemory” OR “memory store” OR “vector memory store” OR “Redis memory” OR “SQLite memory” OR “in-memory cache” OR “retrieval-augmented generation” OR “chat history retrieval” OR “context window management” OR “LLM memory engineering” OR “Python LangChain memory examples”'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_expansion_chain.invoke({\"query\":\"Langchain memory\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec6e3ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG answering prompt\n",
    "answer_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm=llm,prompt=answer_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c57e726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Full RAG pipeline with query expansion\n",
    "rag_pipeline = (\n",
    "    RunnableMap({\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"context\": lambda x: retriever.invoke(query_expansion_chain.invoke({\"query\": x[\"input\"]}))\n",
    "    })\n",
    "    | document_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddebe80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded query:\n",
      "“(LangChain OR “Lang Chain”) AND (“memory support” OR memory OR “memory module” OR “conversational memory” OR “session memory” OR context OR “state management” OR “context persistence”) AND (types OR categories OR implementations OR modules OR backends OR patterns OR architectures) AND (examples OR e.g. OR such as OR including) AND (ConversationBufferMemory OR ConversationSummaryMemory OR CombinedMemory OR VectorStoreRetrieverMemory OR “external vector store” OR “knowledge base” OR Redis OR PostgreSQL OR Pinecone OR Chroma OR Milvus OR Weaviate) AND (ephemeral OR persistent OR summary OR buffer OR retrieval‐augmented)\n",
      "✅ Answer:\n",
      " LangChain currently ships two “plug-and-play” memory back-ends for chat-based agents:\n",
      "\n",
      "1. ConversationBufferMemory  \n",
      "   – Keeps a running buffer of the full back-and-forth.  \n",
      "2. ConversationSummaryMemory  \n",
      "   – Actively summarizes older turns into a concise recap to stay within token limits.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run query\n",
    "query = {\"input\": \"What types of memory does LangChain support?\"}\n",
    "print(query_expansion_chain.invoke({\"query\":query}))\n",
    "response = rag_pipeline.invoke(query)\n",
    "print(\"✅ Answer:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efd86621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded query:\n",
      "\n",
      "(\"CrewAI agents\" OR \"Crew AI agents\" OR \"autonomous crew management agents\" OR \"AI-based crew coordination agents\" OR \"virtual crew assistants\" OR \"autonomous crew assistants\" OR \"crew management software agents\" OR \"AI crew scheduler\" OR \"mult i-agent system for crew planning\" OR \"agent-based modeling for crew assignment\" OR \"reinforcement learning crew scheduler\" OR \"distributed crew coordination agents\")  \n",
      "AND  \n",
      "(\"crew scheduling\" OR \"workforce management\" OR \"staff rostering\" OR \"resource allocation\" OR \"operations planning\")  \n",
      "AND  \n",
      "(\"airline crew\" OR \"maritime crew\" OR \"railway staff\" OR \"hospital nursing staff\" OR \"logistics teams\")  \n",
      "AND  \n",
      "(\"machine learning\" OR \"multi-agent systems (MAS)\" OR \"reinforcement learning\" OR \"natural language processing\" OR \"decentralized AI\" OR \"software agents architecture\")\n",
      "✅ Answer:\n",
      " CrewAI agents are semi-autonomous, role-based workers in a multi-agent system.  Key characteristics include:  \n",
      "• Defined Role – e.g. researcher, planner or executor  \n",
      "• Purpose & Goal – each agent has its own mission that feeds into the overall crew objective  \n",
      "• Toolset – a specified collection of APIs, data sources or utilities it can call on  \n",
      "• Structured Workflow – agents collaborate in orchestrated steps rather than acting in isolation  \n",
      "• Task Adherence – the framework monitors progress so every agent stays on-task and contributes meaningfully to the shared outcome.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run query\n",
    "query = {\"input\": \"CrewAI agents?\"}\n",
    "print(query_expansion_chain.invoke({\"query\":query}))\n",
    "response = rag_pipeline.invoke(query)\n",
    "print(\"✅ Answer:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd03d32d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
