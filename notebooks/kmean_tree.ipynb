{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91a255d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8ae424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_normalize(X: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "    return X / np.maximum(norms, eps)\n",
    "\n",
    "def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    # a, b: (d,)\n",
    "    return float(a @ b / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a829e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spherical_kmeans(X: np.ndarray, k: int, iters: int = 20, seed: int = 0) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    X: (n,d) asumimos L2-normalizado; devuelve (labels, centroids(L2=1))\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # init++ simple: elige k puntos aleatorios\n",
    "    centroids = X[rng.choice(n, size=k, replace=False)].copy()\n",
    "\n",
    "    for _ in range(iters):\n",
    "        # asignación\n",
    "        sims = X @ centroids.T               # (n,k)\n",
    "        labels = np.argmax(sims, axis=1)     # (n,)\n",
    "        # actualización\n",
    "        new_centroids = np.zeros_like(centroids)\n",
    "        counts = np.bincount(labels, minlength=k)\n",
    "        for j in range(k):\n",
    "            idx = (labels == j)\n",
    "            if counts[j] == 0:\n",
    "                # reinit a un punto aleatorio si vacío\n",
    "                new_centroids[j] = X[rng.integers(0, n)]\n",
    "            else:\n",
    "                c = X[idx].mean(axis=0)\n",
    "                # normaliza para mantener métrica de coseno\n",
    "                nc = np.linalg.norm(c)\n",
    "                new_centroids[j] = c / (nc + 1e-12)\n",
    "        if np.allclose(new_centroids, centroids, atol=1e-6):\n",
    "            centroids = new_centroids\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "    # asignación final\n",
    "    sims = X @ centroids.T\n",
    "    labels = np.argmax(sims, axis=1)\n",
    "    return labels, l2_normalize(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98070885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _cluster_anisotropy(Xc: np.ndarray, eps: float = 1e-8) -> float:\n",
    "    \"\"\"\n",
    "    Xc: puntos del cluster centrados (n_k, d) en float32.\n",
    "    Devuelve A = 1 - sphericity in [0,1).\n",
    "    \"\"\"\n",
    "    if Xc.shape[0] <= 1:\n",
    "        return 0.0\n",
    "    # Covarianza vía momentos (sin formar la matriz completa)\n",
    "    # tr(Sigma) = sum_j Var_j ;  tr(Sigma^2) = ||Sigma||_F^2\n",
    "    # Calculamos Sigma explícita solo si d es moderada; para d muy grande,\n",
    "    # puedes estimar ||Sigma||_F^2 por muestreo.\n",
    "    C = np.cov(Xc, rowvar=False)\n",
    "    tr1 = np.trace(C)\n",
    "    tr2 = np.sum(C * C)  # Frobenius^2\n",
    "    d = C.shape[0]\n",
    "    if tr2 <= eps:\n",
    "        return 0.0\n",
    "    sphericity = (tr1 * tr1) / (d * tr2 + eps)\n",
    "    sphericity = float(np.clip(sphericity, 0.0, 1.0))\n",
    "    return 1.0 - sphericity  # anisotropía\n",
    "\n",
    "def spherical_kmeans_iso(X: np.ndarray, k: int, iters: int = 20, seed: int = 0,\n",
    "                         lambda_iso: float = 0.02, reg_mu: float = 1e-3):\n",
    "    \"\"\"\n",
    "    Spherical K-Means con sesgo de asignación por anisotropía de cluster.\n",
    "    X: (n,d) L2-normalizado.\n",
    "    k: nº de clusters.\n",
    "    lambda_iso: peso del regularizador (0 => K-Means estándar).\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # init: elige k puntos aleatorios como centroides\n",
    "    centroids = X[rng.choice(n, size=k, replace=False)].copy()\n",
    "\n",
    "    for _ in range(iters):\n",
    "        # ----- ASIGNACIÓN con sesgo por anisotropía -----\n",
    "        # Similitud coseno equivale a distancia euclídea en L2=1:\n",
    "        # argmax cos <=> argmin ||x - c||^2 = 2(1 - cos) (constante por x)\n",
    "        sims = X @ centroids.T                    # (n,k)\n",
    "        # sesgo por cluster (igual para todos los puntos, cambia cada iter.)\n",
    "        # estimamos anisotropía del cluster \"actual\" usando las asignaciones previas;\n",
    "        # en la 1ª iteración no tenemos labels: usa 0.\n",
    "        if _ == 0 or 'labels' not in locals():\n",
    "            bias = np.zeros(k, dtype=np.float32)\n",
    "        else:\n",
    "            bias = np.zeros(k, dtype=np.float32)\n",
    "            for j in range(k):\n",
    "                idx = (labels == j)\n",
    "                if not np.any(idx):\n",
    "                    bias[j] = 0.0\n",
    "                else:\n",
    "                    Xj = X[idx]\n",
    "                    mu = Xj.mean(axis=0, dtype=np.float32)\n",
    "                    Xc = (Xj - mu).astype(np.float32)\n",
    "                    A = _cluster_anisotropy(Xc)\n",
    "                    bias[j] = lambda_iso * A\n",
    "        # Convertimos sesgo a espacio “distancia”: distancia_eff ≈ 2(1 - cos) + bias\n",
    "        # Como 2 y el término constante no afectan al argmin, basta con:\n",
    "        eff = -sims + bias  # menor es mejor\n",
    "        labels = np.argmin(eff, axis=1)\n",
    "\n",
    "        # ----- ACTUALIZACIÓN de centroides -----\n",
    "        new_centroids = np.zeros_like(centroids)\n",
    "        counts = np.bincount(labels, minlength=k)\n",
    "        for j in range(k):\n",
    "            idx = (labels == j)\n",
    "            if counts[j] == 0:\n",
    "                new_centroids[j] = X[rng.integers(0, n)]\n",
    "            else:\n",
    "                c = X[idx].mean(axis=0)\n",
    "                # opcional: pequeña retracción hacia 0 para evitar medios muy sesgados\n",
    "                if reg_mu > 0:\n",
    "                    c = c * (1.0 - reg_mu)\n",
    "                new_centroids[j] = c / (np.linalg.norm(c) + 1e-12)\n",
    "\n",
    "        # parada por convergencia de centroides\n",
    "        if np.allclose(new_centroids, centroids, atol=1e-6):\n",
    "            centroids = new_centroids\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "\n",
    "    # asignación final\n",
    "    sims = X @ centroids.T\n",
    "    labels = np.argmax(sims, axis=1)\n",
    "    return labels, (centroids / (np.linalg.norm(centroids, axis=1, keepdims=True) + 1e-12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12215762",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Node:\n",
    "    is_leaf: bool\n",
    "    centroids: Optional[np.ndarray] = None     # (B,d) si interno\n",
    "    children: List[\"Node\"] = field(default_factory=list)\n",
    "    idxs: Optional[np.ndarray] = None          # ids en hoja\n",
    "    nid: int = -1\n",
    "\n",
    "class KMeansTree:\n",
    "    def __init__(self, B: int = 8, max_depth: int = 3, min_leaf_size: int = 256, kmeans_iters: int = 20, seed: int = 0, iso: bool = False):\n",
    "        \"\"\"\n",
    "        B: branching factor\n",
    "        max_depth: profundidad máxima del árbol\n",
    "        min_leaf_size: tamaño mínimo para no seguir dividiendo\n",
    "        \"\"\"\n",
    "        self.B = B\n",
    "        self.max_depth = max_depth\n",
    "        self.min_leaf_size = min_leaf_size\n",
    "        self.kmeans_iters = kmeans_iters\n",
    "        self.seed = seed\n",
    "        self._next_id = 0\n",
    "        self.iso = iso\n",
    "        self.root: Optional[Node] = None\n",
    "        self.X: Optional[np.ndarray] = None  # embeddings normalizados\n",
    "        self._id2leaf = None   # dict: int -> Node\n",
    "        self._id2path = None   # dict: int -> tuple(int, ...)\n",
    "\n",
    "    # -------- build --------\n",
    "    # --- helpers internos ---\n",
    "    def _build_id_maps(self):\n",
    "        \"\"\"Construye mapas doc_id -> hoja y doc_id -> path (tupla de índices hijo).\"\"\"\n",
    "        self._id2leaf = {}\n",
    "        self._id2path = {}\n",
    "\n",
    "        def dfs(node, path_prefix):\n",
    "            if node.is_leaf:\n",
    "                if node.idxs is None:\n",
    "                    return\n",
    "                for doc_id in node.idxs.tolist():\n",
    "                    self._id2leaf[doc_id] = node\n",
    "                    self._id2path[doc_id] = tuple(path_prefix)\n",
    "                return\n",
    "            # recorre hijos guardando el índice de hijo en la ruta\n",
    "            for child_idx, ch in enumerate(node.children):\n",
    "                dfs(ch, path_prefix + [child_idx])\n",
    "\n",
    "        dfs(self.root, [])\n",
    "\n",
    "    def fit(self, X: np.ndarray):\n",
    "        \"\"\"\n",
    "        X: (n,d) embeddings (se normalizan internamente a L2=1)\n",
    "        \"\"\"\n",
    "        X = l2_normalize(X.astype(np.float32))\n",
    "        self.X = X\n",
    "        n = X.shape[0]\n",
    "        idxs = np.arange(n, dtype=np.int64)\n",
    "        self.root = self._build_recursive(idxs, depth=0, seed=self.seed)\n",
    "        self._build_id_maps()\n",
    "\n",
    "    def _new_node(self, **kwargs):\n",
    "        node = Node(**kwargs)\n",
    "        node.nid = self._next_id\n",
    "        self._next_id += 1\n",
    "        return node \n",
    "\n",
    "    def _build_recursive(self, idxs: np.ndarray, depth: int, seed: int) -> Node:\n",
    "        # condición de hoja\n",
    "        if depth >= self.max_depth or len(idxs) <= self.min_leaf_size:\n",
    "            return self._new_node(is_leaf=True, idxs=idxs)\n",
    "\n",
    "        X_sub = self.X[idxs]\n",
    "        k = min(self.B, max(1, len(idxs)))  # por si hay pocos puntos\n",
    "        if k == 1:\n",
    "            return self._new_node(is_leaf=True, idxs=idxs)\n",
    "\n",
    "        if self.iso:\n",
    "            labels, centroids = spherical_kmeans_iso(X_sub, k=k, iters=self.kmeans_iters, seed=seed)\n",
    "        else:\n",
    "            labels, centroids = spherical_kmeans(X_sub, k=k, iters=self.kmeans_iters, seed=seed)\n",
    "        children = []\n",
    "        for j in range(k):\n",
    "            child_idxs = idxs[labels == j]\n",
    "            if len(child_idxs) == 0:\n",
    "                # crea hoja vacía para mantener aridad (opcional)\n",
    "                children.append(self._new_node(is_leaf=True, idxs=np.array([], dtype=np.int64)))\n",
    "            else:\n",
    "                children.append(self._build_recursive(child_idxs, depth + 1, seed + j + 1))\n",
    "\n",
    "        # Si hay menos hijos que B por pocos datos, centramos en k real\n",
    "        node = self._new_node(is_leaf=False, centroids=centroids[:k], children=children)\n",
    "        return node\n",
    "    \n",
    "    # --- API pública ---\n",
    "    def get_leaf_path(self, doc_id: int):\n",
    "        \"\"\"\n",
    "        Devuelve la ruta desde la raíz hasta la hoja que contiene doc_id\n",
    "        como lista de índices de hijo [i0, i1, ..., iL-1].\n",
    "        Lanza KeyError si doc_id no existe.\n",
    "        \"\"\"\n",
    "        if self._id2path is None:\n",
    "            raise RuntimeError(\"El índice aún no ha sido construido. Llama a fit() primero.\")\n",
    "        try:\n",
    "            return list(self._id2path[int(doc_id)])\n",
    "        except KeyError:\n",
    "            raise KeyError(f\"doc_id {doc_id} no existe en este árbol\")\n",
    "\n",
    "    def get_leaf_node(self, doc_id: int):\n",
    "        \"\"\"\n",
    "        Devuelve el objeto Node (hoja) que contiene doc_id.\n",
    "        Lanza KeyError si doc_id no existe.\n",
    "        \"\"\"\n",
    "        if self._id2leaf is None:\n",
    "            raise RuntimeError(\"El índice aún no ha sido construido. Llama a fit() primero.\")\n",
    "        try:\n",
    "            return self._id2leaf[int(doc_id)]\n",
    "        except KeyError:\n",
    "            raise KeyError(f\"doc_id {doc_id} no existe en este árbol\")\n",
    "\n",
    "    # -------- query --------\n",
    "    def query(self, q: np.ndarray, topN: int = 10) -> List[Tuple[int, float]]:\n",
    "        \"\"\"\n",
    "        q: (d,) embedding de consulta (se normaliza)\n",
    "        Devuelve lista de (idx, similitud) ordenada desc.\n",
    "        \"\"\"\n",
    "        assert self.root is not None and self.X is not None, \"Primero llama a fit()\"\n",
    "        q = l2_normalize(q.reshape(1, -1).astype(np.float32))[0]\n",
    "        leaf = self._route_greedy(q, self.root)\n",
    "        if leaf.idxs.size == 0:\n",
    "            return []\n",
    "        X_leaf = self.X[leaf.idxs]  # ya normalizados\n",
    "        sims = X_leaf @ q\n",
    "        # topN parcial eficiente\n",
    "        k = min(topN, sims.size)\n",
    "        top_idx = np.argpartition(-sims, k - 1)[:k]\n",
    "        pairs = list(zip(leaf.idxs[top_idx].tolist(), sims[top_idx].tolist()))\n",
    "        # ordena exacto\n",
    "        pairs.sort(key=lambda t: -t[1])\n",
    "        return pairs\n",
    "\n",
    "    def _route_greedy(self, q: np.ndarray, node: Node) -> Node:\n",
    "        while not node.is_leaf:\n",
    "            C = node.centroids  # (k,d)\n",
    "            sims = C @ q        # coseno (centroides y q están L2=1)\n",
    "            j = int(np.argmax(sims))\n",
    "            node = node.children[j]\n",
    "        return node\n",
    "    \n",
    "    def print_tree(self, max_children: int = 8):\n",
    "        assert self.root is not None\n",
    "        def _rec(node, depth):\n",
    "            indent = \"  \" * depth\n",
    "            if node.is_leaf:\n",
    "                size = 0 if node.idxs is None else len(node.idxs)\n",
    "                print(f\"{indent}• [Leaf nid={node.nid}] size={size}\")\n",
    "            else:\n",
    "                k = 0 if node.centroids is None else node.centroids.shape[0]\n",
    "                print(f\"{indent}◦ [Inner nid={node.nid}] k={k}\")\n",
    "                # por si hay muchos hijos, recorta la muestra visual\n",
    "                children = node.children\n",
    "                if len(children) > max_children:\n",
    "                    head = children[:max_children//2]\n",
    "                    tail = children[-max_children//2:]\n",
    "                    for ch in head: _rec(ch, depth+1)\n",
    "                    print(f\"{indent}  ... ({len(children)-len(head)-len(tail)} hijos omitidos) ...\")\n",
    "                    for ch in tail: _rec(ch, depth+1)\n",
    "                else:\n",
    "                    for ch in children:\n",
    "                        _rec(ch, depth+1)\n",
    "        _rec(self.root, 0)\n",
    "\n",
    "    def tree_stats(self):\n",
    "        assert self.root is not None\n",
    "        num_nodes = 0\n",
    "        num_leaves = 0\n",
    "        max_depth = 0\n",
    "        leaf_sizes = []\n",
    "\n",
    "        def _rec(node, depth):\n",
    "            nonlocal num_nodes, num_leaves, max_depth\n",
    "            num_nodes += 1\n",
    "            max_depth = max(max_depth, depth)\n",
    "            if node.is_leaf:\n",
    "                num_leaves += 1\n",
    "                leaf_sizes.append(0 if node.idxs is None else len(node.idxs))\n",
    "            else:\n",
    "                for ch in node.children:\n",
    "                    _rec(ch, depth+1)\n",
    "        _rec(self.root, 0)\n",
    "        leaf_sizes = np.array(leaf_sizes)\n",
    "        return {\n",
    "            \"num_nodes\": num_nodes,\n",
    "            \"num_leaves\": num_leaves,\n",
    "            \"max_depth\": max_depth,\n",
    "            \"leaf_size_min\": int(leaf_sizes.min()) if len(leaf_sizes) else 0,\n",
    "            \"leaf_size_med\": float(np.median(leaf_sizes)) if len(leaf_sizes) else 0,\n",
    "            \"leaf_size_max\": int(leaf_sizes.max()) if len(leaf_sizes) else 0,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5884bfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansTreeBeamSearch(KMeansTree):\n",
    "    def __init__(\n",
    "        self,\n",
    "        B: int = 8,\n",
    "        max_depth: int = 3,\n",
    "        min_leaf_size: int = 256,\n",
    "        kmeans_iters: int = 20,\n",
    "        seed: int = 0,\n",
    "        iso: bool = False,\n",
    "        # --- nuevos parámetros de búsqueda ---\n",
    "        beam: int = 3,              # ancho máximo del beam\n",
    "        tau_margin: float = 0.02,   # margen adaptativo s1-s2\n",
    "        last_level_probe: int = 2,  # nº extra de hojas a probar en el último nivel\n",
    "        last_level_delta: float = 0.01  # umbral de similitud respecto al mejor centroide\n",
    "    ):\n",
    "        super().__init__(B, max_depth, min_leaf_size, kmeans_iters, seed, iso)\n",
    "        # búsqueda mejorada\n",
    "        self.beam = max(1, beam)\n",
    "        self.tau_margin = float(tau_margin)\n",
    "        self.last_level_probe = max(0, last_level_probe)\n",
    "        self.last_level_delta = float(last_level_delta)\n",
    "\n",
    "     # -------- routing mejorado --------\n",
    "    def _route_adaptive_beam(self, q: np.ndarray) -> List[Node]:\n",
    "        assert self.root is not None\n",
    "        frontier = [self.root]\n",
    "        depth = 0\n",
    "\n",
    "        while True:\n",
    "            new_frontier: List[Node] = []\n",
    "            all_leaves = True\n",
    "\n",
    "            for node in frontier:\n",
    "                if node.is_leaf:\n",
    "                    new_frontier.append(node)\n",
    "                    continue\n",
    "\n",
    "                all_leaves = False\n",
    "                C = node.centroids  # (k,d)\n",
    "                sims = C @ q\n",
    "                order = np.argsort(-sims)\n",
    "\n",
    "                s1 = sims[order[0]]\n",
    "                s2 = sims[order[1]] if len(order) > 1 else -1.0\n",
    "                local_beam = 1 if (s1 - s2) >= self.tau_margin else self.beam\n",
    "\n",
    "                # ¿sus hijos son hojas? (último salto)\n",
    "                next_is_leaf = all(ch.is_leaf for ch in node.children)\n",
    "\n",
    "                if not next_is_leaf:\n",
    "                    chosen = order[:local_beam]\n",
    "                    new_frontier.extend([node.children[i] for i in chosen])\n",
    "                else:\n",
    "                    # multi-probe: añade hasta last_level_probe extras\n",
    "                    chosen = order[:local_beam]\n",
    "                    # añade vecinos cercanos por umbral Δ\n",
    "                    if self.last_level_probe > 0:\n",
    "                        s_best = sims[order[0]]\n",
    "                        extra = []\n",
    "                        for j in order[local_beam:]:\n",
    "                            if s_best - sims[j] <= self.last_level_delta:\n",
    "                                extra.append(j)\n",
    "                                if len(extra) >= self.last_level_probe:\n",
    "                                    break\n",
    "                        chosen = np.concatenate([chosen, np.array(extra, dtype=int)]) if len(extra) else chosen\n",
    "                    new_frontier.extend([node.children[i] for i in chosen])\n",
    "\n",
    "            frontier = new_frontier\n",
    "            depth += 1\n",
    "            if all_leaves or depth >= self.max_depth:\n",
    "                break\n",
    "\n",
    "            # si por beam nos quedamos sin nodos (poco probable), salimos\n",
    "            if len(frontier) == 0:\n",
    "                break\n",
    "\n",
    "        # aquí frontier debe ser una lista de hojas candidatas\n",
    "        # quitamos duplicadas por si algún nodo se añadió dos veces\n",
    "        unique_leaves = []\n",
    "        seen = set()\n",
    "        for lf in frontier:\n",
    "            key = id(lf)\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                unique_leaves.append(lf)\n",
    "        return unique_leaves\n",
    "\n",
    "    # -------- búsqueda sobre hojas candidatas --------\n",
    "    def query(self, q: np.ndarray, topN: int = 10) -> List[Tuple[int, float]]:\n",
    "        assert self.root is not None and self.X is not None, \"Primero llama a fit()\"\n",
    "        q = l2_normalize(q.reshape(1, -1).astype(np.float32))[0]\n",
    "        leaves = self._route_adaptive_beam(q)\n",
    "\n",
    "        cand_ids = []\n",
    "        for leaf in leaves:\n",
    "            if leaf.idxs is None or leaf.idxs.size == 0:\n",
    "                continue\n",
    "            cand_ids.extend(leaf.idxs.tolist())\n",
    "        if not cand_ids:\n",
    "            return []\n",
    "\n",
    "        cand_ids = np.unique(np.array(cand_ids, dtype=np.int64))\n",
    "        sims = self.X[cand_ids] @ q\n",
    "        k = min(topN, sims.size)\n",
    "        top_idx = np.argpartition(-sims, k - 1)[:k]\n",
    "        pairs = list(zip(cand_ids[top_idx].tolist(), sims[top_idx].tolist()))\n",
    "        pairs.sort(key=lambda t: -t[1])\n",
    "        return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68fc704a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vector store contains 128518 vectors\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    model_kwargs={\"device\": \"cuda\"}\n",
    ")\n",
    "\n",
    "loaded_vectorstore=FAISS.load_local(\n",
    "    \"../data/db/parliament_db/parliament_all_docs_embeddings_Qwen_Qwen3-Embedding-0.6B_chunked_max_length-512\",\n",
    "    model_embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "print(f\"Loaded vector store contains {loaded_vectorstore.index.ntotal} vectors\")\n",
    "\n",
    "n = loaded_vectorstore.index.ntotal\n",
    "d = loaded_vectorstore.index.d  # dimensión de los embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fab484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metadata of the index\n",
    "pos_to_store_id = [loaded_vectorstore.index_to_docstore_id[i] for i in range(n)]\n",
    "\n",
    "doc_ids = []\n",
    "for store_id in pos_to_store_id:\n",
    "    doc = loaded_vectorstore.docstore.search(store_id)  # recupera el Document\n",
    "    md = getattr(doc, \"metadata\", {}) if doc is not None else {}\n",
    "    # Ajusta la prioridad de claves según cómo lo guardaste\n",
    "    for key in (\"id\", \"doc_id\", \"document_id\", \"uid\"):\n",
    "        if key in md:\n",
    "            doc_ids.append(md[key])\n",
    "            break\n",
    "    else:\n",
    "        # Si no hay id en metadata, usa el store_id como fallback\n",
    "        doc_ids.append(store_id)\n",
    "pos_to_doc_id = {i: doc_ids[i] for i in range(n)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b25bc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All embeddings shape: (128518, 1024)\n"
     ]
    }
   ],
   "source": [
    "X = np.array([emb for emb in loaded_vectorstore.index.reconstruct_n(0, loaded_vectorstore.index.ntotal)])\n",
    "print(f\"All embeddings shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f395f334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "◦ [Inner nid=72] k=8\n",
      "  ◦ [Inner nid=8] k=8\n",
      "    • [Leaf nid=0] size=594\n",
      "    • [Leaf nid=1] size=3136\n",
      "    • [Leaf nid=2] size=1249\n",
      "    • [Leaf nid=3] size=1909\n",
      "    • [Leaf nid=4] size=479\n",
      "    • [Leaf nid=5] size=2469\n",
      "    • [Leaf nid=6] size=1801\n",
      "    • [Leaf nid=7] size=2191\n",
      "  ◦ [Inner nid=17] k=8\n",
      "    • [Leaf nid=9] size=3439\n",
      "    • [Leaf nid=10] size=2498\n",
      "    • [Leaf nid=11] size=1334\n",
      "    • [Leaf nid=12] size=3851\n",
      "    • [Leaf nid=13] size=3697\n",
      "    • [Leaf nid=14] size=2002\n",
      "    • [Leaf nid=15] size=2383\n",
      "    • [Leaf nid=16] size=3642\n",
      "  ◦ [Inner nid=26] k=8\n",
      "    • [Leaf nid=18] size=2447\n",
      "    • [Leaf nid=19] size=2663\n",
      "    • [Leaf nid=20] size=2466\n",
      "    • [Leaf nid=21] size=2789\n",
      "    • [Leaf nid=22] size=1950\n",
      "    • [Leaf nid=23] size=2146\n",
      "    • [Leaf nid=24] size=2012\n",
      "    • [Leaf nid=25] size=1211\n",
      "  ◦ [Inner nid=35] k=8\n",
      "    • [Leaf nid=27] size=2009\n",
      "    • [Leaf nid=28] size=1512\n",
      "    • [Leaf nid=29] size=859\n",
      "    • [Leaf nid=30] size=1144\n",
      "    • [Leaf nid=31] size=910\n",
      "    • [Leaf nid=32] size=1664\n",
      "    • [Leaf nid=33] size=1865\n",
      "    • [Leaf nid=34] size=1389\n",
      "  ◦ [Inner nid=44] k=8\n",
      "    • [Leaf nid=36] size=1360\n",
      "    • [Leaf nid=37] size=2990\n",
      "    • [Leaf nid=38] size=1056\n",
      "    • [Leaf nid=39] size=1766\n",
      "    • [Leaf nid=40] size=1709\n",
      "    • [Leaf nid=41] size=1593\n",
      "    • [Leaf nid=42] size=2497\n",
      "    • [Leaf nid=43] size=2869\n",
      "  ◦ [Inner nid=53] k=8\n",
      "    • [Leaf nid=45] size=1339\n",
      "    • [Leaf nid=46] size=4111\n",
      "    • [Leaf nid=47] size=1348\n",
      "    • [Leaf nid=48] size=4477\n",
      "    • [Leaf nid=49] size=3135\n",
      "    • [Leaf nid=50] size=4304\n",
      "    • [Leaf nid=51] size=2388\n",
      "    • [Leaf nid=52] size=1808\n",
      "  ◦ [Inner nid=62] k=8\n",
      "    • [Leaf nid=54] size=405\n",
      "    • [Leaf nid=55] size=2414\n",
      "    • [Leaf nid=56] size=2614\n",
      "    • [Leaf nid=57] size=1560\n",
      "    • [Leaf nid=58] size=901\n",
      "    • [Leaf nid=59] size=2367\n",
      "    • [Leaf nid=60] size=1327\n",
      "    • [Leaf nid=61] size=1442\n",
      "  ◦ [Inner nid=71] k=8\n",
      "    • [Leaf nid=63] size=1660\n",
      "    • [Leaf nid=64] size=2520\n",
      "    • [Leaf nid=65] size=579\n",
      "    • [Leaf nid=66] size=1959\n",
      "    • [Leaf nid=67] size=1130\n",
      "    • [Leaf nid=68] size=1031\n",
      "    • [Leaf nid=69] size=683\n",
      "    • [Leaf nid=70] size=1466\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'num_nodes': 73,\n",
       " 'num_leaves': 64,\n",
       " 'max_depth': 2,\n",
       " 'leaf_size_min': 405,\n",
       " 'leaf_size_med': 1887.0,\n",
       " 'leaf_size_max': 4477}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = KMeansTreeBeamSearch(B=8, max_depth=2, min_leaf_size=5, kmeans_iters=20, seed=42, iso=False)\n",
    "tree.fit(X)\n",
    "tree.print_tree(max_children=10)\n",
    "tree.tree_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9421921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ruta: [7, 3] id: 73\n",
      "tamaño hoja: 1959\n"
     ]
    }
   ],
   "source": [
    "doc_id = 3\n",
    "path = tree.get_leaf_path(doc_id)   # p.ej. [0, 3, 1]\n",
    "leaf = tree.get_leaf_node(doc_id)   # nodo hoja que contiene ese id\n",
    "print(\"ruta:\", path, \"id:\", \"\".join(str(x) for x in path))\n",
    "print(\"tamaño hoja:\", len(leaf.idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "305496d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'question', 'response', 'cost', 'documents', 'type', 'retrieved_pks', 'oracle_context', 'formatted_context'],\n",
       "        num_rows: 614\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'question', 'response', 'cost', 'documents', 'type', 'retrieved_pks', 'oracle_context', 'formatted_context'],\n",
       "        num_rows: 161\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'id', 'response', 'type', 'retrieved_pks', 'oracle_context', 'injected_oracle', 'formatted_context', 'documents'],\n",
       "        num_rows: 205\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"../data/processed/parliament_qa\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09f7531c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 614/614 [00:00<00:00, 870.58 examples/s]\n",
      "Map: 100%|██████████| 161/161 [00:00<00:00, 875.39 examples/s]\n",
      "Map: 100%|██████████| 205/205 [00:00<00:00, 746.72 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# create labels in dataset using the tree\n",
    "def assign_labels(example):\n",
    "    doc_id = example['id']\n",
    "    id = next((k for k, v in pos_to_doc_id.items() if v == doc_id), None)\n",
    "    try:\n",
    "        path = tree.get_leaf_path(id)\n",
    "        label = \"\".join(str(x) for x in path)\n",
    "    except KeyError:\n",
    "        label = \"unknown\"\n",
    "    return {\"label\": label}\n",
    "\n",
    "dataset = dataset.map(assign_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb9141dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(dataset['train'][:]['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1c1f11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 205/205 [00:12<00:00, 16.74it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "labels_real = []\n",
    "labels_predicted = []\n",
    "topN = 100\n",
    "for idx in tqdm(range(len(dataset['test']))):\n",
    "    query = dataset['test'][idx]['question']\n",
    "    id_real = dataset['test'][idx]['id']\n",
    "    query_emb = np.array(model_embeddings.embed_query(query))\n",
    "    results = tree.query(query_emb, topN=topN)\n",
    "    labels_predicted.append([pos_to_doc_id[idx] for idx, sim in results])\n",
    "    labels_real.append([id_real])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92f2351e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking Metrics:\n",
      "  MRR: 0.5204\n",
      "  mAP: 0.5204\n",
      "  AvgRank: 4.5036\n",
      "  CMC@1: 0.4732\n",
      "  Recall@k (macro)@1: 0.4732\n",
      "  Precision@k (macro)@1: 0.4732\n",
      "  Accuracy@1: 0.4732\n",
      "  F1@k (macro)@1: 0.4732\n",
      "  CMC@5: 0.5902\n",
      "  Recall@k (macro)@5: 0.5902\n",
      "  Precision@k (macro)@5: 0.1180\n",
      "  Accuracy@5: 0.5902\n",
      "  F1@k (macro)@5: 0.1967\n",
      "  CMC@10: 0.6000\n",
      "  Recall@k (macro)@10: 0.6000\n",
      "  Precision@k (macro)@10: 0.0600\n",
      "  Accuracy@10: 0.6000\n",
      "  F1@k (macro)@10: 0.1091\n",
      "  CMC@20: 0.6293\n",
      "  Recall@k (macro)@20: 0.6293\n",
      "  Precision@k (macro)@20: 0.0315\n",
      "  Accuracy@20: 0.6293\n",
      "  F1@k (macro)@20: 0.0599\n",
      "  CMC@100: 0.6780\n",
      "  Recall@k (macro)@100: 0.6780\n",
      "  Precision@k (macro)@100: 0.0068\n",
      "  Accuracy@100: 0.6780\n",
      "  F1@k (macro)@100: 0.0134\n"
     ]
    }
   ],
   "source": [
    "from ranking_metrics import calc_ranking_metrics\n",
    "\n",
    "metrics = calc_ranking_metrics(labels_predicted, labels_real, ks=[1, 5, 10, 20, 100], one_relevant_per_query=True)\n",
    "\n",
    "print(\"Ranking Metrics:\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568fca96",
   "metadata": {},
   "source": [
    "## LLM for routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23ef5780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, concatenate_datasets\n",
    "dataset_clf = {}\n",
    "dataset_clf['train'] = dataset['train'].remove_columns([col for col in dataset['train'].column_names if col not in ['question', 'label']])\n",
    "dataset_clf['validation'] = dataset['validation'].remove_columns([col for col in dataset['validation'].column_names if col not in ['question', 'label']])\n",
    "dataset_clf['test'] = dataset['test'].remove_columns([col for col in dataset['test'].column_names if col not in ['question', 'label']])\n",
    "\n",
    "dataset_clf = DatasetDict(dataset_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fc5f61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 614\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 161\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 205\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_clf = dataset_clf.rename_column(\"question\", \"text\")\n",
    "dataset_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b1b942b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['4', '3', '0', '1', '2']\n",
      "Number of labels: 5\n"
     ]
    }
   ],
   "source": [
    "all_data = concatenate_datasets([dataset_clf['train'], dataset_clf['validation'], dataset_clf['test']])\n",
    "labels_list = all_data.unique('label')\n",
    "print(f\"Labels: {labels_list}\")\n",
    "num_labels = len(labels_list)\n",
    "print(f\"Number of labels: {num_labels}\")\n",
    "\n",
    "# map labels to integers\n",
    "label_to_id = {label: i for i, label in enumerate(labels_list)}\n",
    "def map_labels(example):\n",
    "    return {\n",
    "        \"label\": label_to_id[example['label']]\n",
    "    }\n",
    "dataset_clf = dataset_clf.map(map_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d69b9c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_clf[\"train\"] = concatenate_datasets([dataset_clf['train'], dataset_clf['validation']]).shuffle(seed=42)                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "31b7b01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, AutoConfig, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer, DataCollatorWithPadding, EarlyStoppingCallback\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9cdf9189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-0.6B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "39722a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_r = 64\n",
    "lora_alpha = lora_r * 2\n",
    "lora_dropout = 0.\n",
    "lora_bias = \"none\"\n",
    "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"down_proj\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ec693e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    inference_mode=False,\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=lora_bias,\n",
    "    target_modules=target_modules\n",
    ")\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e3ed1547",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.base_model.model.score.parameters():\n",
    "    p.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6854369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "FOLDER_AUTORE = \"../data/processed/parliament_all_docs\"\n",
    "dataset_indexing = load_from_disk(FOLDER_AUTORE)[\"all\"]\n",
    "dataset_indexing = dataset_indexing.rename_column(\"PK\", \"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b6aee545",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_indexing = dataset_indexing.map(assign_labels)\n",
    "# labels to integers\n",
    "dataset_indexing = dataset_indexing.map(lambda example: {\"label\": int(example['label'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "391bfbd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dataset_indexing['label'][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fb366995",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 161/161 [00:00<00:00, 13892.42 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=MAX_LENGTH, padding=False)\n",
    "\n",
    "# tokenize test from dataset\n",
    "tokenized_datasets = dataset_clf.map(preprocess, batched=True)\n",
    "tokenized_datasets_indexing = dataset_indexing.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8684fd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets_indexing = tokenized_datasets_indexing.remove_columns([\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "61ddbdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8)\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    # Restar el máximo para evitar overflow\n",
    "    x_shift = x - np.max(x, axis=axis, keepdims=True)\n",
    "    exps = np.exp(x_shift)\n",
    "    return exps / np.sum(exps, axis=axis, keepdims=True)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = (preds == labels).mean()\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "35b8c7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e8567318",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_indexing = TrainingArguments(\n",
    "    output_dir=f\"models/parlamento_clf_{model_name.replace('/', '_')}_indexing\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.05,\n",
    "    warmup_ratio=0.2,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"no\",        # <-- no guarda checkpoints ni el modelo final\n",
    "    eval_steps=10,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=False,  # <-- desactivado porque no hay checkpoints\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    seed=SEED,\n",
    "    label_smoothing_factor=0.01,\n",
    ")\n",
    "\n",
    "training_args_query = TrainingArguments(\n",
    "    output_dir=f\"models/parlamento_clf_{model_name.replace('/', '_')}_query\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.05,\n",
    "    warmup_ratio=0.2,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"no\",        # <-- no guarda checkpoints ni el modelo final\n",
    "    eval_steps=10,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=False,  # <-- desactivado porque no hay checkpoints\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    seed=SEED,\n",
    "    label_smoothing_factor=0.01,\n",
    ")\n",
    "\n",
    "# IDs de tokens\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.bos_token_id = tokenizer.bos_token_id\n",
    "\n",
    "trainer_indexing = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_indexing,\n",
    "    train_dataset=tokenized_datasets_indexing,\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    #callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "trainer_query = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_query,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    #callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5180c2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 33,040,384 || all params: 629,090,304 || trainable%: 5.2521\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()  # Verificar parámetros entrenables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0e78ec62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Indexing Epoch 1/2 ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2792' max='2792' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2792/2792 17:51, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.652400</td>\n",
       "      <td>2.162805</td>\n",
       "      <td>0.478049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.695500</td>\n",
       "      <td>2.097874</td>\n",
       "      <td>0.492683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.598800</td>\n",
       "      <td>1.993405</td>\n",
       "      <td>0.492683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.427800</td>\n",
       "      <td>1.882646</td>\n",
       "      <td>0.497561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.512100</td>\n",
       "      <td>1.806089</td>\n",
       "      <td>0.492683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.446100</td>\n",
       "      <td>1.801856</td>\n",
       "      <td>0.502439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.327800</td>\n",
       "      <td>1.808368</td>\n",
       "      <td>0.502439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.204000</td>\n",
       "      <td>1.822186</td>\n",
       "      <td>0.487805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.425400</td>\n",
       "      <td>1.837120</td>\n",
       "      <td>0.492683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.245300</td>\n",
       "      <td>1.843272</td>\n",
       "      <td>0.492683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.627100</td>\n",
       "      <td>1.792836</td>\n",
       "      <td>0.502439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.294000</td>\n",
       "      <td>1.750613</td>\n",
       "      <td>0.492683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.272300</td>\n",
       "      <td>1.748131</td>\n",
       "      <td>0.502439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.183700</td>\n",
       "      <td>1.782637</td>\n",
       "      <td>0.497561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.337200</td>\n",
       "      <td>1.799863</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.450500</td>\n",
       "      <td>1.731647</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.255600</td>\n",
       "      <td>1.787230</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.404300</td>\n",
       "      <td>1.789491</td>\n",
       "      <td>0.497561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.405000</td>\n",
       "      <td>1.864346</td>\n",
       "      <td>0.492683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.541400</td>\n",
       "      <td>1.928176</td>\n",
       "      <td>0.482927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.445500</td>\n",
       "      <td>1.914179</td>\n",
       "      <td>0.482927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.449500</td>\n",
       "      <td>1.908976</td>\n",
       "      <td>0.473171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.186100</td>\n",
       "      <td>1.834405</td>\n",
       "      <td>0.502439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.222100</td>\n",
       "      <td>1.842629</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.239900</td>\n",
       "      <td>1.838797</td>\n",
       "      <td>0.502439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>1.861045</td>\n",
       "      <td>0.482927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.378800</td>\n",
       "      <td>1.924091</td>\n",
       "      <td>0.458537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.300700</td>\n",
       "      <td>1.865274</td>\n",
       "      <td>0.473171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.258800</td>\n",
       "      <td>1.787120</td>\n",
       "      <td>0.487805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.445500</td>\n",
       "      <td>1.778720</td>\n",
       "      <td>0.492683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.435200</td>\n",
       "      <td>1.805377</td>\n",
       "      <td>0.482927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.367900</td>\n",
       "      <td>1.876292</td>\n",
       "      <td>0.473171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.358000</td>\n",
       "      <td>1.797836</td>\n",
       "      <td>0.502439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.387000</td>\n",
       "      <td>1.841013</td>\n",
       "      <td>0.497561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.146600</td>\n",
       "      <td>1.898923</td>\n",
       "      <td>0.482927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.263900</td>\n",
       "      <td>1.846599</td>\n",
       "      <td>0.492683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.286200</td>\n",
       "      <td>1.829687</td>\n",
       "      <td>0.492683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.111200</td>\n",
       "      <td>1.768261</td>\n",
       "      <td>0.497561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.216000</td>\n",
       "      <td>1.676363</td>\n",
       "      <td>0.517073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.207400</td>\n",
       "      <td>1.727012</td>\n",
       "      <td>0.517073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.315900</td>\n",
       "      <td>1.878796</td>\n",
       "      <td>0.473171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.260400</td>\n",
       "      <td>2.072167</td>\n",
       "      <td>0.443902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.324100</td>\n",
       "      <td>1.909850</td>\n",
       "      <td>0.463415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.371700</td>\n",
       "      <td>1.860940</td>\n",
       "      <td>0.458537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.143200</td>\n",
       "      <td>1.886173</td>\n",
       "      <td>0.482927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.174800</td>\n",
       "      <td>2.005213</td>\n",
       "      <td>0.453659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.115400</td>\n",
       "      <td>1.974321</td>\n",
       "      <td>0.453659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.264600</td>\n",
       "      <td>2.016195</td>\n",
       "      <td>0.424390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.398000</td>\n",
       "      <td>2.045770</td>\n",
       "      <td>0.414634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.354400</td>\n",
       "      <td>1.797742</td>\n",
       "      <td>0.492683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.184100</td>\n",
       "      <td>1.753687</td>\n",
       "      <td>0.487805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.214200</td>\n",
       "      <td>2.016805</td>\n",
       "      <td>0.439024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.143800</td>\n",
       "      <td>2.090698</td>\n",
       "      <td>0.419512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.212300</td>\n",
       "      <td>1.915236</td>\n",
       "      <td>0.458537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.195500</td>\n",
       "      <td>1.813127</td>\n",
       "      <td>0.492683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.231000</td>\n",
       "      <td>1.905789</td>\n",
       "      <td>0.468293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.197500</td>\n",
       "      <td>1.866786</td>\n",
       "      <td>0.482927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.169800</td>\n",
       "      <td>2.018447</td>\n",
       "      <td>0.424390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.679500</td>\n",
       "      <td>2.031934</td>\n",
       "      <td>0.414634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.212300</td>\n",
       "      <td>1.835364</td>\n",
       "      <td>0.443902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>1.778919</td>\n",
       "      <td>0.478049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.278200</td>\n",
       "      <td>1.816528</td>\n",
       "      <td>0.492683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.359400</td>\n",
       "      <td>2.139661</td>\n",
       "      <td>0.414634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.394300</td>\n",
       "      <td>2.013583</td>\n",
       "      <td>0.458537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.136800</td>\n",
       "      <td>1.950923</td>\n",
       "      <td>0.448780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>1.937315</td>\n",
       "      <td>0.453659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.228200</td>\n",
       "      <td>2.110195</td>\n",
       "      <td>0.414634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.567400</td>\n",
       "      <td>1.919608</td>\n",
       "      <td>0.434146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.413900</td>\n",
       "      <td>1.838991</td>\n",
       "      <td>0.419512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.236400</td>\n",
       "      <td>1.998429</td>\n",
       "      <td>0.414634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.272800</td>\n",
       "      <td>2.068871</td>\n",
       "      <td>0.409756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.283400</td>\n",
       "      <td>1.838739</td>\n",
       "      <td>0.458537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.178900</td>\n",
       "      <td>1.753559</td>\n",
       "      <td>0.478049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.149900</td>\n",
       "      <td>1.878489</td>\n",
       "      <td>0.409756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.191700</td>\n",
       "      <td>2.196239</td>\n",
       "      <td>0.375610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.375700</td>\n",
       "      <td>2.193450</td>\n",
       "      <td>0.380488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.195200</td>\n",
       "      <td>2.135241</td>\n",
       "      <td>0.370732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.340100</td>\n",
       "      <td>1.906065</td>\n",
       "      <td>0.390244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.159400</td>\n",
       "      <td>1.958717</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.193000</td>\n",
       "      <td>2.065127</td>\n",
       "      <td>0.380488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.304600</td>\n",
       "      <td>2.095399</td>\n",
       "      <td>0.380488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>1.902260</td>\n",
       "      <td>0.414634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.425500</td>\n",
       "      <td>1.763365</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.318200</td>\n",
       "      <td>1.756752</td>\n",
       "      <td>0.414634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.306400</td>\n",
       "      <td>1.785753</td>\n",
       "      <td>0.434146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.895636</td>\n",
       "      <td>0.424390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.178900</td>\n",
       "      <td>2.082130</td>\n",
       "      <td>0.390244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.235400</td>\n",
       "      <td>1.953136</td>\n",
       "      <td>0.419512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.282100</td>\n",
       "      <td>1.773447</td>\n",
       "      <td>0.473171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.241400</td>\n",
       "      <td>1.758222</td>\n",
       "      <td>0.487805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.150400</td>\n",
       "      <td>1.847649</td>\n",
       "      <td>0.473171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.186000</td>\n",
       "      <td>1.895724</td>\n",
       "      <td>0.443902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.422500</td>\n",
       "      <td>2.052365</td>\n",
       "      <td>0.414634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.231800</td>\n",
       "      <td>2.275768</td>\n",
       "      <td>0.356098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.491000</td>\n",
       "      <td>2.012789</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.218300</td>\n",
       "      <td>1.843424</td>\n",
       "      <td>0.419512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.293400</td>\n",
       "      <td>1.865553</td>\n",
       "      <td>0.404878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.160800</td>\n",
       "      <td>1.908435</td>\n",
       "      <td>0.375610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.107000</td>\n",
       "      <td>2.027870</td>\n",
       "      <td>0.360976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>2.187662</td>\n",
       "      <td>0.356098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.261100</td>\n",
       "      <td>2.041863</td>\n",
       "      <td>0.356098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.227700</td>\n",
       "      <td>2.122088</td>\n",
       "      <td>0.365854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.111100</td>\n",
       "      <td>2.215338</td>\n",
       "      <td>0.395122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.551700</td>\n",
       "      <td>2.009347</td>\n",
       "      <td>0.404878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.307600</td>\n",
       "      <td>1.945127</td>\n",
       "      <td>0.385366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.295000</td>\n",
       "      <td>2.122080</td>\n",
       "      <td>0.365854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.263000</td>\n",
       "      <td>2.211989</td>\n",
       "      <td>0.360976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.522700</td>\n",
       "      <td>1.978400</td>\n",
       "      <td>0.385366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.277200</td>\n",
       "      <td>2.296658</td>\n",
       "      <td>0.346341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.176300</td>\n",
       "      <td>2.343002</td>\n",
       "      <td>0.346341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>0.234100</td>\n",
       "      <td>2.309428</td>\n",
       "      <td>0.336585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.284400</td>\n",
       "      <td>2.244941</td>\n",
       "      <td>0.317073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>0.331700</td>\n",
       "      <td>2.386869</td>\n",
       "      <td>0.302439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.196000</td>\n",
       "      <td>2.420465</td>\n",
       "      <td>0.331707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.231100</td>\n",
       "      <td>2.180846</td>\n",
       "      <td>0.390244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.335800</td>\n",
       "      <td>1.978165</td>\n",
       "      <td>0.380488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>0.209700</td>\n",
       "      <td>2.087936</td>\n",
       "      <td>0.365854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.128300</td>\n",
       "      <td>2.125365</td>\n",
       "      <td>0.351220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>0.154600</td>\n",
       "      <td>2.057830</td>\n",
       "      <td>0.351220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.069300</td>\n",
       "      <td>2.064107</td>\n",
       "      <td>0.341463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>0.205300</td>\n",
       "      <td>2.039647</td>\n",
       "      <td>0.365854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>2.191113</td>\n",
       "      <td>0.375610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>0.187800</td>\n",
       "      <td>2.206767</td>\n",
       "      <td>0.360976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.259300</td>\n",
       "      <td>2.002443</td>\n",
       "      <td>0.385366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.262000</td>\n",
       "      <td>2.047843</td>\n",
       "      <td>0.380488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.241500</td>\n",
       "      <td>1.883144</td>\n",
       "      <td>0.424390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>0.185900</td>\n",
       "      <td>1.970794</td>\n",
       "      <td>0.385366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.151300</td>\n",
       "      <td>2.105212</td>\n",
       "      <td>0.360976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>0.091700</td>\n",
       "      <td>2.076378</td>\n",
       "      <td>0.351220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.197800</td>\n",
       "      <td>2.184503</td>\n",
       "      <td>0.326829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>0.305600</td>\n",
       "      <td>2.097941</td>\n",
       "      <td>0.341463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.135100</td>\n",
       "      <td>2.027680</td>\n",
       "      <td>0.395122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>0.211800</td>\n",
       "      <td>2.246767</td>\n",
       "      <td>0.346341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.148700</td>\n",
       "      <td>2.278885</td>\n",
       "      <td>0.326829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.208200</td>\n",
       "      <td>2.155392</td>\n",
       "      <td>0.336585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.187100</td>\n",
       "      <td>2.174536</td>\n",
       "      <td>0.346341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>2.327626</td>\n",
       "      <td>0.331707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.265800</td>\n",
       "      <td>2.299423</td>\n",
       "      <td>0.331707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>0.261600</td>\n",
       "      <td>2.469872</td>\n",
       "      <td>0.356098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.422500</td>\n",
       "      <td>2.396324</td>\n",
       "      <td>0.346341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>0.132600</td>\n",
       "      <td>2.352939</td>\n",
       "      <td>0.356098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>0.139000</td>\n",
       "      <td>2.338038</td>\n",
       "      <td>0.356098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>0.084200</td>\n",
       "      <td>2.246212</td>\n",
       "      <td>0.365854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.067200</td>\n",
       "      <td>2.313148</td>\n",
       "      <td>0.351220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.151800</td>\n",
       "      <td>2.376892</td>\n",
       "      <td>0.346341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>0.157900</td>\n",
       "      <td>2.212926</td>\n",
       "      <td>0.360976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>0.143500</td>\n",
       "      <td>2.128403</td>\n",
       "      <td>0.375610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>0.093300</td>\n",
       "      <td>2.283144</td>\n",
       "      <td>0.341463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>0.174400</td>\n",
       "      <td>2.412256</td>\n",
       "      <td>0.331707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.067700</td>\n",
       "      <td>2.522915</td>\n",
       "      <td>0.331707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>0.241700</td>\n",
       "      <td>2.361742</td>\n",
       "      <td>0.346341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.365600</td>\n",
       "      <td>2.250232</td>\n",
       "      <td>0.356098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>0.258300</td>\n",
       "      <td>2.274055</td>\n",
       "      <td>0.336585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>0.215700</td>\n",
       "      <td>2.185210</td>\n",
       "      <td>0.331707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.193900</td>\n",
       "      <td>2.129142</td>\n",
       "      <td>0.341463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.172000</td>\n",
       "      <td>2.062180</td>\n",
       "      <td>0.317073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>0.122300</td>\n",
       "      <td>2.202538</td>\n",
       "      <td>0.321951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>2.282397</td>\n",
       "      <td>0.326829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>0.073000</td>\n",
       "      <td>2.262864</td>\n",
       "      <td>0.331707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.059800</td>\n",
       "      <td>2.184728</td>\n",
       "      <td>0.336585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>0.195400</td>\n",
       "      <td>2.220451</td>\n",
       "      <td>0.331707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>0.154500</td>\n",
       "      <td>2.140580</td>\n",
       "      <td>0.317073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>0.066000</td>\n",
       "      <td>2.137624</td>\n",
       "      <td>0.312195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>0.186400</td>\n",
       "      <td>2.220957</td>\n",
       "      <td>0.321951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.124000</td>\n",
       "      <td>2.217770</td>\n",
       "      <td>0.326829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>0.241400</td>\n",
       "      <td>2.141488</td>\n",
       "      <td>0.326829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>0.153800</td>\n",
       "      <td>2.093611</td>\n",
       "      <td>0.341463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.083900</td>\n",
       "      <td>2.122485</td>\n",
       "      <td>0.321951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690</td>\n",
       "      <td>0.154100</td>\n",
       "      <td>2.203303</td>\n",
       "      <td>0.317073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.085900</td>\n",
       "      <td>2.195902</td>\n",
       "      <td>0.307317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710</td>\n",
       "      <td>0.154900</td>\n",
       "      <td>2.311642</td>\n",
       "      <td>0.292683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>0.248800</td>\n",
       "      <td>2.449401</td>\n",
       "      <td>0.312195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1730</td>\n",
       "      <td>0.065400</td>\n",
       "      <td>2.382781</td>\n",
       "      <td>0.321951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>0.154700</td>\n",
       "      <td>2.230645</td>\n",
       "      <td>0.312195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.209400</td>\n",
       "      <td>2.202116</td>\n",
       "      <td>0.326829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>2.229419</td>\n",
       "      <td>0.326829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770</td>\n",
       "      <td>0.225500</td>\n",
       "      <td>2.309141</td>\n",
       "      <td>0.312195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>0.062300</td>\n",
       "      <td>2.362145</td>\n",
       "      <td>0.287805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1790</td>\n",
       "      <td>0.061500</td>\n",
       "      <td>2.384900</td>\n",
       "      <td>0.287805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.098900</td>\n",
       "      <td>2.305625</td>\n",
       "      <td>0.302439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810</td>\n",
       "      <td>0.077100</td>\n",
       "      <td>2.299214</td>\n",
       "      <td>0.278049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>0.212300</td>\n",
       "      <td>2.363935</td>\n",
       "      <td>0.278049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1830</td>\n",
       "      <td>0.227500</td>\n",
       "      <td>2.384178</td>\n",
       "      <td>0.312195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.202900</td>\n",
       "      <td>2.292734</td>\n",
       "      <td>0.312195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.145200</td>\n",
       "      <td>2.230989</td>\n",
       "      <td>0.297561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>0.210600</td>\n",
       "      <td>2.242225</td>\n",
       "      <td>0.278049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1870</td>\n",
       "      <td>0.211400</td>\n",
       "      <td>2.246662</td>\n",
       "      <td>0.282927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>0.163200</td>\n",
       "      <td>2.202293</td>\n",
       "      <td>0.287805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1890</td>\n",
       "      <td>0.151100</td>\n",
       "      <td>2.252654</td>\n",
       "      <td>0.302439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.110900</td>\n",
       "      <td>2.292308</td>\n",
       "      <td>0.312195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1910</td>\n",
       "      <td>0.148000</td>\n",
       "      <td>2.308812</td>\n",
       "      <td>0.321951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.309300</td>\n",
       "      <td>2.245295</td>\n",
       "      <td>0.321951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1930</td>\n",
       "      <td>0.115400</td>\n",
       "      <td>2.138762</td>\n",
       "      <td>0.317073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>0.223600</td>\n",
       "      <td>2.216728</td>\n",
       "      <td>0.317073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.127900</td>\n",
       "      <td>2.356188</td>\n",
       "      <td>0.321951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>0.134800</td>\n",
       "      <td>2.353782</td>\n",
       "      <td>0.317073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970</td>\n",
       "      <td>0.140800</td>\n",
       "      <td>2.329353</td>\n",
       "      <td>0.312195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>0.188200</td>\n",
       "      <td>2.294104</td>\n",
       "      <td>0.312195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990</td>\n",
       "      <td>0.156700</td>\n",
       "      <td>2.378645</td>\n",
       "      <td>0.307317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.148000</td>\n",
       "      <td>2.444925</td>\n",
       "      <td>0.317073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2010</td>\n",
       "      <td>0.168200</td>\n",
       "      <td>2.507590</td>\n",
       "      <td>0.326829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>0.077700</td>\n",
       "      <td>2.416923</td>\n",
       "      <td>0.312195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2030</td>\n",
       "      <td>0.090500</td>\n",
       "      <td>2.399586</td>\n",
       "      <td>0.312195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>0.063700</td>\n",
       "      <td>2.398657</td>\n",
       "      <td>0.297561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.284800</td>\n",
       "      <td>2.399007</td>\n",
       "      <td>0.287805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>0.141500</td>\n",
       "      <td>2.386571</td>\n",
       "      <td>0.287805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2070</td>\n",
       "      <td>0.131200</td>\n",
       "      <td>2.378076</td>\n",
       "      <td>0.287805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.127800</td>\n",
       "      <td>2.425116</td>\n",
       "      <td>0.297561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2090</td>\n",
       "      <td>0.208400</td>\n",
       "      <td>2.453004</td>\n",
       "      <td>0.307317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.174600</td>\n",
       "      <td>2.457925</td>\n",
       "      <td>0.302439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2110</td>\n",
       "      <td>0.084900</td>\n",
       "      <td>2.452302</td>\n",
       "      <td>0.302439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>0.215000</td>\n",
       "      <td>2.431509</td>\n",
       "      <td>0.302439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2130</td>\n",
       "      <td>0.126900</td>\n",
       "      <td>2.380467</td>\n",
       "      <td>0.287805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2140</td>\n",
       "      <td>0.132000</td>\n",
       "      <td>2.339144</td>\n",
       "      <td>0.282927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.158800</td>\n",
       "      <td>2.327897</td>\n",
       "      <td>0.278049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.127100</td>\n",
       "      <td>2.375967</td>\n",
       "      <td>0.282927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2170</td>\n",
       "      <td>0.320900</td>\n",
       "      <td>2.456403</td>\n",
       "      <td>0.307317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>0.111300</td>\n",
       "      <td>2.530442</td>\n",
       "      <td>0.317073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2190</td>\n",
       "      <td>0.223200</td>\n",
       "      <td>2.537992</td>\n",
       "      <td>0.312195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.139200</td>\n",
       "      <td>2.569932</td>\n",
       "      <td>0.297561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2210</td>\n",
       "      <td>0.108600</td>\n",
       "      <td>2.585639</td>\n",
       "      <td>0.297561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>0.178700</td>\n",
       "      <td>2.563708</td>\n",
       "      <td>0.302439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2230</td>\n",
       "      <td>0.082500</td>\n",
       "      <td>2.522711</td>\n",
       "      <td>0.312195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.144300</td>\n",
       "      <td>2.508257</td>\n",
       "      <td>0.307317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.182100</td>\n",
       "      <td>2.517380</td>\n",
       "      <td>0.307317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2260</td>\n",
       "      <td>0.072600</td>\n",
       "      <td>2.509455</td>\n",
       "      <td>0.307317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2270</td>\n",
       "      <td>0.121800</td>\n",
       "      <td>2.493010</td>\n",
       "      <td>0.302439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>0.121800</td>\n",
       "      <td>2.498646</td>\n",
       "      <td>0.307317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2290</td>\n",
       "      <td>0.199300</td>\n",
       "      <td>2.501875</td>\n",
       "      <td>0.307317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.178100</td>\n",
       "      <td>2.464674</td>\n",
       "      <td>0.302439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2310</td>\n",
       "      <td>0.066400</td>\n",
       "      <td>2.453080</td>\n",
       "      <td>0.297561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>0.186800</td>\n",
       "      <td>2.443712</td>\n",
       "      <td>0.297561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2330</td>\n",
       "      <td>0.153000</td>\n",
       "      <td>2.437275</td>\n",
       "      <td>0.297561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>0.088000</td>\n",
       "      <td>2.450774</td>\n",
       "      <td>0.297561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.109900</td>\n",
       "      <td>2.465329</td>\n",
       "      <td>0.297561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>0.192400</td>\n",
       "      <td>2.485710</td>\n",
       "      <td>0.302439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2370</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>2.478897</td>\n",
       "      <td>0.297561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2380</td>\n",
       "      <td>0.061500</td>\n",
       "      <td>2.478157</td>\n",
       "      <td>0.297561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2390</td>\n",
       "      <td>0.059600</td>\n",
       "      <td>2.474096</td>\n",
       "      <td>0.297561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.099000</td>\n",
       "      <td>2.478126</td>\n",
       "      <td>0.297561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2410</td>\n",
       "      <td>0.148200</td>\n",
       "      <td>2.474722</td>\n",
       "      <td>0.297561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2420</td>\n",
       "      <td>0.153800</td>\n",
       "      <td>2.485684</td>\n",
       "      <td>0.297561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2430</td>\n",
       "      <td>0.175300</td>\n",
       "      <td>2.494037</td>\n",
       "      <td>0.302439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2440</td>\n",
       "      <td>0.203200</td>\n",
       "      <td>2.497366</td>\n",
       "      <td>0.302439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.243300</td>\n",
       "      <td>2.501351</td>\n",
       "      <td>0.302439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2460</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>2.507506</td>\n",
       "      <td>0.302439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2470</td>\n",
       "      <td>0.200900</td>\n",
       "      <td>2.501715</td>\n",
       "      <td>0.302439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2480</td>\n",
       "      <td>0.130300</td>\n",
       "      <td>2.489938</td>\n",
       "      <td>0.302439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2490</td>\n",
       "      <td>0.194600</td>\n",
       "      <td>2.484600</td>\n",
       "      <td>0.297561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.124600</td>\n",
       "      <td>2.482868</td>\n",
       "      <td>0.297561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2510</td>\n",
       "      <td>0.104500</td>\n",
       "      <td>2.476329</td>\n",
       "      <td>0.297561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>0.143800</td>\n",
       "      <td>2.476646</td>\n",
       "      <td>0.297561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2530</td>\n",
       "      <td>0.555700</td>\n",
       "      <td>2.459002</td>\n",
       "      <td>0.302439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2540</td>\n",
       "      <td>0.187900</td>\n",
       "      <td>2.455333</td>\n",
       "      <td>0.302439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.097300</td>\n",
       "      <td>2.461253</td>\n",
       "      <td>0.302439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>0.322700</td>\n",
       "      <td>2.473740</td>\n",
       "      <td>0.297561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2570</td>\n",
       "      <td>0.218800</td>\n",
       "      <td>2.477802</td>\n",
       "      <td>0.297561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2580</td>\n",
       "      <td>0.307400</td>\n",
       "      <td>2.479845</td>\n",
       "      <td>0.297561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2590</td>\n",
       "      <td>0.134200</td>\n",
       "      <td>2.480423</td>\n",
       "      <td>0.297561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.292900</td>\n",
       "      <td>2.481293</td>\n",
       "      <td>0.297561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2610</td>\n",
       "      <td>0.236200</td>\n",
       "      <td>2.482189</td>\n",
       "      <td>0.297561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2620</td>\n",
       "      <td>0.105800</td>\n",
       "      <td>2.482485</td>\n",
       "      <td>0.297561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2630</td>\n",
       "      <td>0.144800</td>\n",
       "      <td>2.486483</td>\n",
       "      <td>0.297561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>0.362500</td>\n",
       "      <td>2.489856</td>\n",
       "      <td>0.297561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.271200</td>\n",
       "      <td>2.489718</td>\n",
       "      <td>0.297561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2660</td>\n",
       "      <td>0.339500</td>\n",
       "      <td>2.488018</td>\n",
       "      <td>0.297561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2670</td>\n",
       "      <td>0.178600</td>\n",
       "      <td>2.485810</td>\n",
       "      <td>0.297561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2680</td>\n",
       "      <td>0.180300</td>\n",
       "      <td>2.482270</td>\n",
       "      <td>0.302439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2690</td>\n",
       "      <td>0.256900</td>\n",
       "      <td>2.480160</td>\n",
       "      <td>0.297561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.118600</td>\n",
       "      <td>2.479402</td>\n",
       "      <td>0.297561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2710</td>\n",
       "      <td>0.190500</td>\n",
       "      <td>2.478341</td>\n",
       "      <td>0.302439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2720</td>\n",
       "      <td>0.381000</td>\n",
       "      <td>2.476922</td>\n",
       "      <td>0.302439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2730</td>\n",
       "      <td>0.210100</td>\n",
       "      <td>2.477132</td>\n",
       "      <td>0.297561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2740</td>\n",
       "      <td>0.242200</td>\n",
       "      <td>2.476540</td>\n",
       "      <td>0.302439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.327900</td>\n",
       "      <td>2.476518</td>\n",
       "      <td>0.302439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>0.220300</td>\n",
       "      <td>2.477140</td>\n",
       "      <td>0.302439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2770</td>\n",
       "      <td>0.126400</td>\n",
       "      <td>2.476748</td>\n",
       "      <td>0.302439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2780</td>\n",
       "      <td>0.138000</td>\n",
       "      <td>2.477083</td>\n",
       "      <td>0.302439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2790</td>\n",
       "      <td>0.233800</td>\n",
       "      <td>2.477111</td>\n",
       "      <td>0.302439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Query Epoch 1/2 ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='970' max='970' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [970/970 04:51, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.299500</td>\n",
       "      <td>2.298790</td>\n",
       "      <td>0.326829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.675600</td>\n",
       "      <td>2.028748</td>\n",
       "      <td>0.424390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.236800</td>\n",
       "      <td>2.404466</td>\n",
       "      <td>0.375610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.144300</td>\n",
       "      <td>2.297593</td>\n",
       "      <td>0.453659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.071400</td>\n",
       "      <td>2.120533</td>\n",
       "      <td>0.536585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.067000</td>\n",
       "      <td>2.122047</td>\n",
       "      <td>0.570732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.063100</td>\n",
       "      <td>2.096563</td>\n",
       "      <td>0.536585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.063800</td>\n",
       "      <td>2.127107</td>\n",
       "      <td>0.502439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.063700</td>\n",
       "      <td>2.113527</td>\n",
       "      <td>0.551220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.082400</td>\n",
       "      <td>1.946852</td>\n",
       "      <td>0.560976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.061200</td>\n",
       "      <td>2.280354</td>\n",
       "      <td>0.512195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.060500</td>\n",
       "      <td>2.570710</td>\n",
       "      <td>0.443902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.066200</td>\n",
       "      <td>2.410082</td>\n",
       "      <td>0.468293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.127000</td>\n",
       "      <td>2.239544</td>\n",
       "      <td>0.502439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.062700</td>\n",
       "      <td>2.165550</td>\n",
       "      <td>0.497561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.064700</td>\n",
       "      <td>2.332948</td>\n",
       "      <td>0.492683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.061700</td>\n",
       "      <td>2.116201</td>\n",
       "      <td>0.497561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.097500</td>\n",
       "      <td>2.070553</td>\n",
       "      <td>0.521951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.063900</td>\n",
       "      <td>2.146914</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.060200</td>\n",
       "      <td>2.103691</td>\n",
       "      <td>0.512195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.063600</td>\n",
       "      <td>2.616612</td>\n",
       "      <td>0.492683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.069300</td>\n",
       "      <td>2.169775</td>\n",
       "      <td>0.473171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.064500</td>\n",
       "      <td>1.913986</td>\n",
       "      <td>0.570732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.145400</td>\n",
       "      <td>2.332758</td>\n",
       "      <td>0.536585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.062900</td>\n",
       "      <td>2.529468</td>\n",
       "      <td>0.497561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.060900</td>\n",
       "      <td>2.688840</td>\n",
       "      <td>0.487805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.069200</td>\n",
       "      <td>2.650211</td>\n",
       "      <td>0.492683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.145600</td>\n",
       "      <td>2.479869</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.060700</td>\n",
       "      <td>2.295822</td>\n",
       "      <td>0.531707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.060400</td>\n",
       "      <td>2.294182</td>\n",
       "      <td>0.531707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.072700</td>\n",
       "      <td>2.201689</td>\n",
       "      <td>0.546341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.082200</td>\n",
       "      <td>2.379347</td>\n",
       "      <td>0.536585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.137100</td>\n",
       "      <td>2.395619</td>\n",
       "      <td>0.502439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>2.843453</td>\n",
       "      <td>0.487805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.070100</td>\n",
       "      <td>2.797256</td>\n",
       "      <td>0.463415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.123000</td>\n",
       "      <td>2.673298</td>\n",
       "      <td>0.458537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.078700</td>\n",
       "      <td>2.437599</td>\n",
       "      <td>0.526829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.071700</td>\n",
       "      <td>2.082003</td>\n",
       "      <td>0.541463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.152800</td>\n",
       "      <td>2.620297</td>\n",
       "      <td>0.478049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.124400</td>\n",
       "      <td>2.592183</td>\n",
       "      <td>0.473171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.067600</td>\n",
       "      <td>2.776450</td>\n",
       "      <td>0.482927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.160700</td>\n",
       "      <td>3.171436</td>\n",
       "      <td>0.414634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.060600</td>\n",
       "      <td>2.459920</td>\n",
       "      <td>0.487805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.073100</td>\n",
       "      <td>2.860654</td>\n",
       "      <td>0.429268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.063100</td>\n",
       "      <td>2.461910</td>\n",
       "      <td>0.541463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.067600</td>\n",
       "      <td>2.561523</td>\n",
       "      <td>0.517073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.118100</td>\n",
       "      <td>3.048059</td>\n",
       "      <td>0.448780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.061800</td>\n",
       "      <td>2.430949</td>\n",
       "      <td>0.541463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.078500</td>\n",
       "      <td>2.292284</td>\n",
       "      <td>0.570732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.075800</td>\n",
       "      <td>2.272190</td>\n",
       "      <td>0.546341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.063900</td>\n",
       "      <td>2.485482</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.120300</td>\n",
       "      <td>2.492549</td>\n",
       "      <td>0.482927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.122800</td>\n",
       "      <td>2.471286</td>\n",
       "      <td>0.497561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.058600</td>\n",
       "      <td>2.376184</td>\n",
       "      <td>0.521951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.059000</td>\n",
       "      <td>2.415669</td>\n",
       "      <td>0.512195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.059500</td>\n",
       "      <td>2.408346</td>\n",
       "      <td>0.521951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.059300</td>\n",
       "      <td>2.533870</td>\n",
       "      <td>0.512195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.059100</td>\n",
       "      <td>2.704865</td>\n",
       "      <td>0.458537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.059700</td>\n",
       "      <td>2.694665</td>\n",
       "      <td>0.473171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.078500</td>\n",
       "      <td>2.606044</td>\n",
       "      <td>0.497561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.058200</td>\n",
       "      <td>2.481117</td>\n",
       "      <td>0.512195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.058100</td>\n",
       "      <td>2.488649</td>\n",
       "      <td>0.517073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.058200</td>\n",
       "      <td>2.488751</td>\n",
       "      <td>0.512195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.058200</td>\n",
       "      <td>2.484600</td>\n",
       "      <td>0.512195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.058100</td>\n",
       "      <td>2.519428</td>\n",
       "      <td>0.502439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.058600</td>\n",
       "      <td>2.491898</td>\n",
       "      <td>0.521951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.058200</td>\n",
       "      <td>2.493945</td>\n",
       "      <td>0.517073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.058400</td>\n",
       "      <td>2.551337</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.058600</td>\n",
       "      <td>2.553535</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>2.550951</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>2.557760</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>2.551992</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>2.548454</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.058100</td>\n",
       "      <td>2.541738</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>2.549716</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>2.565633</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.058100</td>\n",
       "      <td>2.566658</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>2.555089</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>2.554543</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.058100</td>\n",
       "      <td>2.547199</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>2.552201</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>2.557347</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>2.569965</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>2.573686</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>2.569794</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>2.567829</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>2.566864</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.058400</td>\n",
       "      <td>2.565687</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>2.566813</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>2.565545</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>2.564987</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>2.565424</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>2.564884</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>2.565016</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>2.564723</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.058200</td>\n",
       "      <td>2.565356</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>2.564482</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    print(f\"=== Indexing Epoch {epoch+1}/{2} ===\")\n",
    "    trainer_indexing.train()\n",
    "    print(f\"=== Query Epoch {epoch+1}/{2} ===\")\n",
    "    trainer_query.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "161480b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics after training: {'eval_loss': 2.5644822120666504, 'eval_accuracy': 0.5073170731707317, 'eval_runtime': 1.5834, 'eval_samples_per_second': 129.465, 'eval_steps_per_second': 16.42, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer_query.evaluate()\n",
    "print(\"Evaluation metrics after training:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "57b948ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2595, 1024)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs = tree.get_leaf_node(0).idxs\n",
    "# get embeddings of those idxs\n",
    "X_leaf = X[idxs]\n",
    "X_leaf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874d991d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "52e6d7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def query_llm(llm_clf, q: np.ndarray, q_text: str, tokenizer, topN: int = 10, topK: int = 1) -> List[Tuple[int, float]]:\n",
    "    q = l2_normalize(q.reshape(1, -1).astype(np.float32))[0]\n",
    "    # predice la etiqueta con el clasificador\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(q_text, return_tensors=\"pt\", truncation=True, max_length=512).to(llm_clf.model.device)\n",
    "        outputs = llm_clf.model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        pred_label_id = int(torch.argmax(logits, dim=-1).cpu().numpy()[0])\n",
    "\n",
    "    leaves = [tree.get_leaf_node(pred_label_id)]\n",
    "    cand_ids = []\n",
    "    for leaf in leaves:\n",
    "        if leaf.idxs is None or leaf.idxs.size == 0:\n",
    "            continue\n",
    "        cand_ids.extend(leaf.idxs.tolist())\n",
    "    if not cand_ids:\n",
    "        return []\n",
    "\n",
    "    cand_ids = np.unique(np.array(cand_ids, dtype=np.int64))\n",
    "    sims = X[cand_ids] @ q\n",
    "    k = min(topN, sims.size)\n",
    "    top_idx = np.argpartition(-sims, k - 1)[:k]\n",
    "    pairs = list(zip(cand_ids[top_idx].tolist(), sims[top_idx].tolist()))\n",
    "    pairs.sort(key=lambda t: -t[1])\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b13b48ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 205/205 [00:24<00:00,  8.54it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "labels_real = []\n",
    "labels_predicted = []\n",
    "topN = 100\n",
    "for idx in tqdm(range(len(dataset['test']))):\n",
    "    query = dataset['test'][idx]['question']\n",
    "    id_real = dataset['test'][idx]['id']\n",
    "    query_emb = np.array(model_embeddings.embed_query(query))\n",
    "    results = query_llm(trainer_query, query_emb, query, tokenizer, topN=topN)\n",
    "    labels_predicted.append([pos_to_doc_id[idx] for idx, sim in results])\n",
    "    labels_real.append([id_real])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "63a738c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking Metrics:\n",
      "  MRR: 0.3385\n",
      "  mAP: 0.3385\n",
      "  AvgRank: 6.0698\n",
      "  CMC@1: 0.3122\n",
      "  Recall@k (macro)@1: 0.3122\n",
      "  Precision@k (macro)@1: 0.3122\n",
      "  Accuracy@1: 0.3122\n",
      "  F1@k (macro)@1: 0.3122\n",
      "  CMC@5: 0.3659\n",
      "  Recall@k (macro)@5: 0.3659\n",
      "  Precision@k (macro)@5: 0.0732\n",
      "  Accuracy@5: 0.3659\n",
      "  F1@k (macro)@5: 0.1220\n",
      "  CMC@10: 0.3756\n",
      "  Recall@k (macro)@10: 0.3756\n",
      "  Precision@k (macro)@10: 0.0376\n",
      "  Accuracy@10: 0.3756\n",
      "  F1@k (macro)@10: 0.0683\n",
      "  CMC@20: 0.3854\n",
      "  Recall@k (macro)@20: 0.3854\n",
      "  Precision@k (macro)@20: 0.0193\n",
      "  Accuracy@20: 0.3854\n",
      "  F1@k (macro)@20: 0.0367\n",
      "  CMC@100: 0.4195\n",
      "  Recall@k (macro)@100: 0.4195\n",
      "  Precision@k (macro)@100: 0.0042\n",
      "  Accuracy@100: 0.4195\n",
      "  F1@k (macro)@100: 0.0083\n"
     ]
    }
   ],
   "source": [
    "metrics = calc_ranking_metrics(labels_predicted, labels_real, ks=[1, 5, 10, 20, 100], one_relevant_per_query=True)\n",
    "\n",
    "print(\"Ranking Metrics:\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698b1a63",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6d24486e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "◦ [Inner nid=5] k=5\n",
      "  • [Leaf nid=0] size=1662\n",
      "  • [Leaf nid=1] size=2758\n",
      "  • [Leaf nid=2] size=2595\n",
      "  • [Leaf nid=3] size=2504\n",
      "  • [Leaf nid=4] size=1643\n",
      "Estadísticas del árbol con beam search: {'num_nodes': 6, 'num_leaves': 5, 'max_depth': 1, 'leaf_size_min': 1643, 'leaf_size_med': 2504.0, 'leaf_size_max': 2758}\n"
     ]
    }
   ],
   "source": [
    "tree_beam = KMeansTreeBeamSearch(B=5, max_depth=1, min_leaf_size=5, kmeans_iters=20, seed=42, iso=True)\n",
    "tree_beam.fit(X)\n",
    "tree_beam.print_tree(max_children=10)\n",
    "stats = tree_beam.tree_stats()\n",
    "print(\"Estadísticas del árbol con beam search:\", stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "11434071",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 205/205 [00:09<00:00, 22.60it/s]\n"
     ]
    }
   ],
   "source": [
    "labels_real = []\n",
    "labels_predicted = []\n",
    "topN = 100\n",
    "for idx in tqdm(range(len(dataset['test']))):\n",
    "    query = dataset['test'][idx]['question']\n",
    "    id_real = dataset['test'][idx]['id']\n",
    "    query_emb = np.array(model_embeddings.embed_query(query))\n",
    "    results = tree_beam.query(query_emb, topN=topN)\n",
    "    labels_predicted.append([pos_to_doc_id[idx] for idx, sim in results])\n",
    "    labels_real.append([id_real])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "18e2015d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking Metrics:\n",
      "  MRR: 0.5339\n",
      "  mAP: 0.5339\n",
      "  AvgRank: 6.3406\n",
      "  CMC@1: 0.4927\n",
      "  Recall@k (macro)@1: 0.4927\n",
      "  Precision@k (macro)@1: 0.4927\n",
      "  Accuracy@1: 0.4927\n",
      "  F1@k (macro)@1: 0.4927\n",
      "  CMC@5: 0.5805\n",
      "  Recall@k (macro)@5: 0.5805\n",
      "  Precision@k (macro)@5: 0.1161\n",
      "  Accuracy@5: 0.5805\n",
      "  F1@k (macro)@5: 0.1935\n",
      "  CMC@10: 0.6098\n",
      "  Recall@k (macro)@10: 0.6098\n",
      "  Precision@k (macro)@10: 0.0610\n",
      "  Accuracy@10: 0.6098\n",
      "  F1@k (macro)@10: 0.1109\n",
      "  CMC@20: 0.6293\n",
      "  Recall@k (macro)@20: 0.6293\n",
      "  Precision@k (macro)@20: 0.0315\n",
      "  Accuracy@20: 0.6293\n",
      "  F1@k (macro)@20: 0.0599\n",
      "  CMC@100: 0.6732\n",
      "  Recall@k (macro)@100: 0.6732\n",
      "  Precision@k (macro)@100: 0.0067\n",
      "  Accuracy@100: 0.6732\n",
      "  F1@k (macro)@100: 0.0133\n"
     ]
    }
   ],
   "source": [
    "metrics = calc_ranking_metrics(labels_predicted, labels_real, ks=[1, 5, 10, 20, 100], one_relevant_per_query=True)\n",
    "\n",
    "print(\"Ranking Metrics:\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968625b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameter combinations to try: 700\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "VALUES_B = [5, 10, 20, 40, 80]\n",
    "VALUES_MAX_DEPTH = [1, 2, 3, 4]\n",
    "VALUES_MIN_LEAF_SIZE = [5, 10, 20, 40, 80, 100, 200]\n",
    "VALUES_KMEANS_ITERS = [5, 10, 20, 40, 80]\n",
    "# contruir combinaciones en una lista\n",
    "param_combinations = list(product(VALUES_B, VALUES_MAX_DEPTH, VALUES_MIN_LEAF_SIZE, VALUES_KMEANS_ITERS))\n",
    "print(f\"Total parameter combinations to try: {len(param_combinations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e546924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/700 [00:10<2:01:32, 10.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR:0.4763 @ B=5, max_depth=1, min_leaf_size=5, kmeans_iters=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/700 [00:20<2:01:21, 10.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR:0.4910 @ B=5, max_depth=1, min_leaf_size=5, kmeans_iters=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/700 [00:30<1:56:27, 10.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR:0.5315 @ B=5, max_depth=1, min_leaf_size=5, kmeans_iters=20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/700 [01:09<2:14:19, 11.61s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m])):\n\u001b[1;32m     10\u001b[0m     query \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m][idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 11\u001b[0m     query_emb \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     12\u001b[0m     results \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mquery(query_emb, topN\u001b[38;5;241m=\u001b[39mtopN)\n\u001b[1;32m     13\u001b[0m     labels_predicted\u001b[38;5;241m.\u001b[39mappend([pos_to_doc_id[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx, sim \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_huggingface/embeddings/huggingface.py:173\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.embed_query\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute query embeddings using a HuggingFace transformer model.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    166\u001b[0m \n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    168\u001b[0m embed_kwargs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery_encode_kwargs\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery_encode_kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_kwargs\n\u001b[1;32m    172\u001b[0m )\n\u001b[0;32m--> 173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_kwargs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_huggingface/embeddings/huggingface.py:131\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings._embed\u001b[0;34m(self, texts, encode_kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m     sentence_transformers\u001b[38;5;241m.\u001b[39mSentenceTransformer\u001b[38;5;241m.\u001b[39mstop_multi_process_pool(pool)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(embeddings, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    138\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected embeddings to be a Tensor or a numpy array, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot a list instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:1051\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[0m\n\u001b[1;32m   1048\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m-> 1051\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1053\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:1132\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m             module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[1;32m   1127\u001b[0m         module_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1128\u001b[0m             key: value\n\u001b[1;32m   1129\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1130\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mforward_kwargs)\n\u001b[1;32m   1131\u001b[0m         }\n\u001b[0;32m-> 1132\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:234\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features, **kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n\u001b[1;32m    228\u001b[0m trans_features \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    229\u001b[0m     key: value\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    232\u001b[0m }\n\u001b[0;32m--> 234\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m token_embeddings \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    236\u001b[0m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token_embeddings\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/generic.py:1064\u001b[0m, in \u001b[0;36mcheck_model_inputs.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1061\u001b[0m                 module\u001b[38;5;241m.\u001b[39mforward \u001b[38;5;241m=\u001b[39m make_capture_wrapper(module, original_forward, key, specs\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   1062\u001b[0m                 monkey_patched_layers\u001b[38;5;241m.\u001b[39mappend((module, original_forward))\n\u001b[0;32m-> 1064\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;66;03m# Restore original forward methods\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module, original_forward \u001b[38;5;129;01min\u001b[39;00m monkey_patched_layers:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/qwen3/modeling_qwen3.py:410\u001b[0m, in \u001b[0;36mQwen3Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(hidden_states, position_ids)\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers]:\n\u001b[0;32m--> 410\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(hidden_states)\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[1;32m    423\u001b[0m     last_hidden_state\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    424\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    425\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/qwen3/modeling_qwen3.py:260\u001b[0m, in \u001b[0;36mQwen3DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 260\u001b[0m hidden_states, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/qwen3/modeling_qwen3.py:229\u001b[0m, in \u001b[0;36mQwen3Attention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m attn_output, attn_weights \u001b[38;5;241m=\u001b[39m attention_interface(\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    218\u001b[0m     query_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    226\u001b[0m )\n\u001b[1;32m    228\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m--> 229\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mo_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, attn_weights\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_mrr = 0.0\n",
    "for b, depth, min_size, iters in tqdm(param_combinations):\n",
    "    #print(f\"Probando B={b}, max_depth={depth}, min_leaf_size={min_size}, kmeans_iters={iters}\")\n",
    "    tree = KMeansTreeBeamSearch(B=b, max_depth=depth, min_leaf_size=min_size, kmeans_iters=iters, seed=42)\n",
    "    tree.fit(X)\n",
    "    \n",
    "    labels_predicted = []\n",
    "\n",
    "    for idx in range(len(dataset['test'])):\n",
    "        query = dataset['test'][idx]['question']\n",
    "        query_emb = np.array(model.embed_query(query))\n",
    "        results = tree.query(query_emb, topN=topN)\n",
    "        labels_predicted.append([pos_to_doc_id[idx] for idx, sim in results])\n",
    "    \n",
    "    metrics = calc_ranking_metrics(labels_predicted, labels_real, ks=[1, 5, 10, 20, 100], one_relevant_per_query=True)\n",
    "    \n",
    "    if metrics['MRR'] > best_mrr:\n",
    "        best_mrr = metrics['MRR']\n",
    "        print(f\"MRR:{metrics['MRR']:.4f} @ B={b}, max_depth={depth}, min_leaf_size={min_size}, kmeans_iters={iters}\")\n",
    "\n",
    "print(f\"Best MRR: {best_mrr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615181d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/205 [00:00<?, ?it/s]/tmp/ipykernel_1162906/3315405819.py:6: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(query)\n",
      "100%|██████████| 205/205 [00:06<00:00, 33.28it/s]\n"
     ]
    }
   ],
   "source": [
    "labels_predicted = []\n",
    "retriever=loaded_vectorstore.as_retriever(search_kwargs={\"k\":topN})\n",
    "\n",
    "for idx in tqdm(range(len(dataset['test']))):\n",
    "    query = dataset['test'][idx]['question']\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    labels_predicted.append([doc.metadata['id'] for doc in docs])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee92c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking Metrics:\n",
      "  MRR: 0.6506\n",
      "  mAP: 0.6506\n",
      "  AvgRank: 6.8743\n",
      "  CMC@1: 0.5854\n",
      "  Recall@k (macro)@1: 0.5854\n",
      "  Precision@k (macro)@1: 0.5854\n",
      "  Accuracy@1: 0.5854\n",
      "  F1@k (macro)@1: 0.5854\n",
      "  CMC@5: 0.7268\n",
      "  Recall@k (macro)@5: 0.7268\n",
      "  Precision@k (macro)@5: 0.1454\n",
      "  Accuracy@5: 0.7268\n",
      "  F1@k (macro)@5: 0.2423\n",
      "  CMC@10: 0.7610\n",
      "  Recall@k (macro)@10: 0.7610\n",
      "  Precision@k (macro)@10: 0.0761\n",
      "  Accuracy@10: 0.7610\n",
      "  F1@k (macro)@10: 0.1384\n",
      "  CMC@20: 0.7951\n",
      "  Recall@k (macro)@20: 0.7951\n",
      "  Precision@k (macro)@20: 0.0398\n",
      "  Accuracy@20: 0.7951\n",
      "  F1@k (macro)@20: 0.0757\n",
      "  CMC@100: 0.8537\n",
      "  Recall@k (macro)@100: 0.8537\n",
      "  Precision@k (macro)@100: 0.0085\n",
      "  Accuracy@100: 0.8537\n",
      "  F1@k (macro)@100: 0.0169\n"
     ]
    }
   ],
   "source": [
    "metrics = calc_ranking_metrics(labels_predicted, labels_real, ks=[1, 5, 10, 20, 100], one_relevant_per_query=True)\n",
    "\n",
    "print(\"Ranking Metrics:\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
