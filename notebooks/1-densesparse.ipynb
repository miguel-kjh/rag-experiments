{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a129598",
   "metadata": {},
   "source": [
    "## Hybrid Retriever- Combining Dense And Sparse Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f518da96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.retrievers import BM25Retriever, TFIDFRetriever \n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f2cfdde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.schema import Document\n",
    "import faiss\n",
    "import numpy as np\n",
    "import uuid\n",
    "\n",
    "def build_faiss_with_progress(docs, embedding_model, batch_size=32, normalize=True):\n",
    "    \"\"\"\n",
    "    Construye un Ã­ndice FAISS mostrando barra de progreso\n",
    "    y conservando metadatos de los Document.\n",
    "    \"\"\"\n",
    "    texts = [d.page_content for d in docs]\n",
    "    embeddings = []\n",
    "\n",
    "    # 1) Calcular embeddings en lotes con barra\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generando embeddings\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        batch_emb = embedding_model.embed_documents(batch)\n",
    "        embeddings.extend(batch_emb)\n",
    "\n",
    "    embeddings = np.array(embeddings, dtype=\"float32\")\n",
    "\n",
    "    # 2) Normalizar si se quiere coseno\n",
    "    if normalize:\n",
    "        faiss.normalize_L2(embeddings)\n",
    "\n",
    "    # 3) Crear Ã­ndice FAISS\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(embeddings)\n",
    "\n",
    "    # 4) Guardar Document completos con sus metadatos\n",
    "    id_map = {str(uuid.uuid4()): d for d in docs}\n",
    "    docstore = InMemoryDocstore(id_map)\n",
    "\n",
    "    # 5) Devolver FAISS listo para usar\n",
    "    vectorstore = FAISS(\n",
    "        embedding_function=embedding_model,\n",
    "        index=index,\n",
    "        docstore=docstore,\n",
    "        index_to_docstore_id=list(id_map.keys())\n",
    "    )\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38f2a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"dense_vectorstore = FAISS.from_documents(\n",
    "    docs, \n",
    "    embedding_model,\n",
    "    normalize_L2=True  # Normalize vectors for cosine similarity\n",
    ")\n",
    "dense_retriever = dense_vectorstore.as_retriever()\n",
    "dense_retriever.search_kwargs['k'] = 2  # Retrieve top 2 documents\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5c86da13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Sample documents\n",
    "docs = [\n",
    "    Document(page_content=\"LangChain helps build LLM applications.\", metadata={\"id\": \"doc1\"}),\n",
    "    Document(page_content=\"Pinecone is a vector database for semantic search.\", metadata={\"id\": \"doc2\"}),\n",
    "    Document(page_content=\"The Eiffel Tower is located in Paris.\", metadata={\"id\": \"doc3\"}),\n",
    "    Document(page_content=\"Langchain can be used to develop agentic ai application.\", metadata={\"id\": \"doc4\"}),\n",
    "    Document(page_content=\"Langchain has many types of retrievers.\", metadata={\"id\": \"doc5\"}),\n",
    "    Document(page_content=\"FAISS is a library for efficient similarity search and clustering of dense vectors.\", metadata={\"id\": \"doc6\"}),\n",
    "    Document(page_content=\"HuggingFace provides a wide range of pre-trained models for NLP tasks.\", metadata={\"id\": \"doc7\"}),\n",
    "    Document(page_content=\"BM25 is a ranking function used by search engines to estimate the relevance of documents to a given search query.\", metadata={\"id\": \"doc8\"}),\n",
    "    Document(page_content=\"TF-IDF stands for Term Frequency-Inverse Document Frequency.\", metadata={\"id\": \"doc9\"}),\n",
    "    Document(page_content=\"Ensemble methods combine multiple models to improve performance.\", metadata={\"id\": \"doc10\"}),\n",
    "]\n",
    "\n",
    "# Step 2: Dense Retriever (FAISS + HuggingFace)\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"google/embeddinggemma-300m\",\n",
    "    model_kwargs={\"device\": \"cuda\"}  # <--- GPU\n",
    ")\n",
    "\n",
    "dense_vectorstore = build_faiss_with_progress(docs, embedding_model, normalize=True)\n",
    "dense_retriever = dense_vectorstore.as_retriever()\n",
    "dense_retriever.search_kwargs['k'] = 2  # Retrieve top 2 documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bf503d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16162/3548855305.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  dense_retriever.get_relevant_documents(\"What is LangChain?\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'id': 'doc1'}, page_content='LangChain helps build LLM applications.'),\n",
       " Document(metadata={'id': 'doc9'}, page_content='TF-IDF stands for Term Frequency-Inverse Document Frequency.')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_retriever.get_relevant_documents(\"What is LangChain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "60bc8ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizer=TfidfVectorizer() docs=[Document(metadata={'id': 'doc1'}, page_content='LangChain helps build LLM applications.'), Document(metadata={'id': 'doc2'}, page_content='Pinecone is a vector database for semantic search.'), Document(metadata={'id': 'doc3'}, page_content='The Eiffel Tower is located in Paris.'), Document(metadata={'id': 'doc4'}, page_content='Langchain can be used to develop agentic ai application.'), Document(metadata={'id': 'doc5'}, page_content='Langchain has many types of retrievers.'), Document(metadata={'id': 'doc6'}, page_content='FAISS is a library for efficient similarity search and clustering of dense vectors.'), Document(metadata={'id': 'doc7'}, page_content='HuggingFace provides a wide range of pre-trained models for NLP tasks.'), Document(metadata={'id': 'doc8'}, page_content='BM25 is a ranking function used by search engines to estimate the relevance of documents to a given search query.'), Document(metadata={'id': 'doc9'}, page_content='TF-IDF stands for Term Frequency-Inverse Document Frequency.'), Document(metadata={'id': 'doc10'}, page_content='Ensemble methods combine multiple models to improve performance.')] tfidf_array=<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 89 stored elements and shape (10, 71)> k=2\n"
     ]
    }
   ],
   "source": [
    "tf_idf_retriever = TFIDFRetriever.from_documents(docs)\n",
    "tf_idf_retriever.k = 2  # Retrieve top 2 documents\n",
    "print(tf_idf_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "76569a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sparse Retriever(BM25)\n",
    "sparse_retriever=BM25Retriever.from_documents(docs)\n",
    "sparse_retriever.k=4 ##top- k documents to retriever\n",
    "\n",
    "## step 4 : Combine with Ensemble Retriever\n",
    "hybrid_retriever=EnsembleRetriever(\n",
    "    retrievers=[dense_retriever,sparse_retriever],\n",
    "    weights=[0.7,0.3]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "57d59933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnsembleRetriever(retrievers=[VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x76cdcd1eaa10>, search_kwargs={'k': 2}), BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x76cd32747f50>)], weights=[0.7, 0.3])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dec3b869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Document 1:\n",
      "LangChain helps build LLM applications.\n",
      "\n",
      "ðŸ”¹ Document 2:\n",
      "Langchain can be used to develop agentic ai application.\n",
      "\n",
      "ðŸ”¹ Document 3:\n",
      "TF-IDF stands for Term Frequency-Inverse Document Frequency.\n",
      "\n",
      "ðŸ”¹ Document 4:\n",
      "Ensemble methods combine multiple models to improve performance.\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Query and get results\n",
    "query = \"How can I build an application using LLMs?\"\n",
    "results = hybrid_retriever.invoke(query)\n",
    "\n",
    "# Step 6: Print results\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\nðŸ”¹ Document {i+1}:\\n{doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11c57cb",
   "metadata": {},
   "source": [
    "### RAG Pipeline with hybrid retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0bf22afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99e17a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x76cd3e76ec90>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x76cd33e2bf90>, root_client=<openai.OpenAI object at 0x76cd3dc1e9d0>, root_async_client=<openai.AsyncOpenAI object at 0x76cd33e2b9d0>, temperature=0.2, model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Prompt Template\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "\n",
    "## step 6-llm\n",
    "llm=init_chat_model(\"openai:gpt-3.5-turbo\",temperature=0.2)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9eb55e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | EnsembleRetriever(retrievers=[VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x76cdcd0a33d0>, search_kwargs={'k': 3}), BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x76cd3daa3590>, k=3)], weights=[0.7, 0.3]), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\nAnswer the question based on the context below.\\n\\nContext:\\n{context}\\n\\nQuestion: {input}\\n')\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x76cd3e76ec90>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x76cd33e2bf90>, root_client=<openai.OpenAI object at 0x76cd3dc1e9d0>, root_async_client=<openai.AsyncOpenAI object at 0x76cd33e2b9d0>, temperature=0.2, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create stuff Docuemnt Chain\n",
    "document_chain=create_stuff_documents_chain(llm=llm,prompt=prompt)\n",
    "\n",
    "## create Full rAg chain\n",
    "rag_chain=create_retrieval_chain(retriever=hybrid_retriever,combine_docs_chain=document_chain)\n",
    "rag_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6bb2441c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Answer:\n",
      " You can build an app using LLMs by utilizing LangChain, which helps in developing LLM applications. LangChain offers various types of retrievers that can be used in building agentic AI applications. Additionally, Pinecone, a vector database for semantic search, can also be integrated into the app to enhance its functionality.\n",
      "\n",
      "ðŸ“„ Source Documents:\n",
      "\n",
      "Doc 1: LangChain helps build LLM applications.\n",
      "\n",
      "Doc 2: Langchain can be used to develop agentic ai application.\n",
      "\n",
      "Doc 3: Pinecone is a vector database for semantic search.\n",
      "\n",
      "Doc 4: Langchain has many types of retrievers.\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Ask a question\n",
    "query = {\"input\": \"How can I build an app using LLMs?\"}\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "# Step 10: Output\n",
    "print(\"âœ… Answer:\\n\", response[\"answer\"])\n",
    "\n",
    "print(\"\\nðŸ“„ Source Documents:\")\n",
    "for i, doc in enumerate(response[\"context\"]):\n",
    "    print(f\"\\nDoc {i+1}: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c4468a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a58760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb41501b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99604bae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
