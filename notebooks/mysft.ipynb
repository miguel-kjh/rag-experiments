{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69384459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miguel/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_1673506/4009208669.py:5: UserWarning: WARNING: Unsloth should be imported before trl, transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel, FastModel, UnslothTrainer, UnslothTrainingArguments\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 10-31 15:45:13 [__init__.py:216] Automatically detected platform cuda.\n",
      "WARNING 10-31 15:45:13 [interface.py:391] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from unsloth import FastLanguageModel, FastModel, UnslothTrainer, UnslothTrainingArguments\n",
    "from transformers.trainer import Accelerator\n",
    "import warnings\n",
    "from transformers import TrainerCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37a6b044",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"text\": f\"Your training data example {i}\"}\n",
    "    for i in range(1, 10)\n",
    "]\n",
    "train_ds = Dataset.from_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20b8c570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 9\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c775a411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- 3) Tu SFTTrainer personalizado con regularizador --------\n",
    "\n",
    "class KLLoggerCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and \"kl_loss\" in logs:\n",
    "            print(f\"[step {state.global_step}] CE = {logs['ce_loss']:.4f} - KL = {logs['kl_loss']:.4f} - Total = {logs['loss']:.4f}\")\n",
    "\n",
    "class SFTTrainerWithKL(SFTTrainer):\n",
    "    def __init__(self, *args, kl_lambda=0.01, temperature=1.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.kl_lambda = kl_lambda\n",
    "        self.temperature = temperature\n",
    "        self.ce_loss = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _logits_ref_without_lora(self, inputs_no_labels):\n",
    "        # Ejecuta el MISMO modelo con LoRA desactivado\n",
    "        try:\n",
    "            with self.model.disable_adapter():\n",
    "                outputs_ref = self.model(**inputs_no_labels)\n",
    "        except Exception:\n",
    "            was_training = self.model.training\n",
    "            if hasattr(self.model, \"disable_adapter\"):\n",
    "                self.model.disable_adapter()\n",
    "            outputs_ref = self.model(**inputs_no_labels)\n",
    "            if hasattr(self.model, \"enable_adapter\"):\n",
    "                self.model.enable_adapter()\n",
    "            if was_training:\n",
    "                self.model.train()\n",
    "        return outputs_ref.logits\n",
    "\n",
    "    def _kl_pt_pref(self, logits_t, logits_ref, labels):\n",
    "        # Shift para alinear con CE\n",
    "        shift_t = logits_t[:, :-1, :] / self.temperature\n",
    "        shift_ref = logits_ref[:, :-1, :] / self.temperature\n",
    "        shift_labels = labels[:, 1:]\n",
    "\n",
    "        valid = (shift_labels != -100).float()\n",
    "        denom = valid.sum().clamp_min(1.0)\n",
    "\n",
    "        log_pt = torch.log_softmax(shift_t, dim=-1)\n",
    "        log_pref = torch.log_softmax(shift_ref, dim=-1)\n",
    "        pt = log_pt.exp()\n",
    "\n",
    "        kl_tokens = (pt * (log_pt - log_pref)).sum(dim=-1)  # [B, T-1]\n",
    "        kl_mean = (kl_tokens * valid).sum() / denom\n",
    "        return kl_mean\n",
    "\n",
    "    # Acepta el arg extra de Unsloth\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None, **kwargs):\n",
    "        # 1) Separa labels y NO se los pases al modelo\n",
    "        labels = inputs.get(\"labels\")\n",
    "        inputs_no_labels = {k: v for k, v in inputs.items() if k != \"labels\"}\n",
    "\n",
    "        # 2) Forward normal (LoRA activo) -> logits_t\n",
    "        outputs_t = model(**inputs_no_labels)\n",
    "        logits_t = outputs_t.logits\n",
    "\n",
    "        # 3) CE manual (evita la fused CE de Unsloth)\n",
    "        shift_logits = logits_t[:, :-1, :].contiguous()\n",
    "        shift_labels = labels[:, 1:].contiguous()\n",
    "        loss_ce = self.ce_loss(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1),\n",
    "        )\n",
    "\n",
    "        # 4) Forward de referencia (LoRA desactivado, sin gradiente)\n",
    "        with torch.no_grad():\n",
    "            logits_ref = self._logits_ref_without_lora(inputs_no_labels)\n",
    "\n",
    "        # 5) KL promedio en posiciones v√°lidas\n",
    "        kl = self._kl_pt_pref(logits_t, logits_ref, labels)\n",
    "\n",
    "        total = loss_ce + self.kl_lambda * kl\n",
    "\n",
    "        if self.state is not None and self.state.global_step % self.args.logging_steps == 0:\n",
    "            self.log({\"kl_loss\": kl.detach().float().item(), \"ce_loss\": loss_ce.detach().float().item()})\n",
    "            \n",
    "        return (total, outputs_t) if return_outputs else total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03a3eff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.9.8: Fast Qwen3 patching. Transformers: 4.56.2. vLLM: 0.10.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.988 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.27it/s]\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 512\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name=\"Qwen/Qwen3-8B\",   # cambia a tu modelo (Qwen, Llama, etc.)\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=True,\n",
    "    load_in_16bit=False,\n",
    "    full_finetuning=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22bbb06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.0,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    max_seq_length=max_seq_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fc5e11b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The specified `eos_token` ('<EOS_TOKEN>') is not found in the vocabulary of the given `processing_class` (Qwen2TokenizerFast). Ensure that the `eos_token` exists in the vocabulary before using it as an EOS token.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ---------- 4) Configuraci√≥n y entrenamiento ----------\u001b[39;00m\n\u001b[1;32m      2\u001b[0m training_args \u001b[38;5;241m=\u001b[39m SFTConfig(\n\u001b[1;32m      3\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs-kl\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     packing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSFTTrainerWithKL\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkl_lambda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# peso del KL (aj√∫stalo)\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mKLLoggerCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m, in \u001b[0;36mSFTTrainerWithKL.__init__\u001b[0;34m(self, kl_lambda, temperature, *args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, kl_lambda\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkl_lambda \u001b[38;5;241m=\u001b[39m kl_lambda\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemperature \u001b[38;5;241m=\u001b[39m temperature\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:662\u001b[0m, in \u001b[0;36mSFTTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, formatting_func)\u001b[0m\n\u001b[1;32m    660\u001b[0m     eos_token_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(eos_token)\n\u001b[1;32m    661\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eos_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 662\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    663\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe specified `eos_token` (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00meos_token\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m) is not found in the vocabulary of the given \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    664\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`processing_class` (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprocessing_class\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). Ensure that the `eos_token` exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    665\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min the vocabulary before using it as an EOS token.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    666\u001b[0m         )\n\u001b[1;32m    667\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39meos_token_id \u001b[38;5;241m=\u001b[39m eos_token_id\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mchat_template_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: The specified `eos_token` ('<EOS_TOKEN>') is not found in the vocabulary of the given `processing_class` (Qwen2TokenizerFast). Ensure that the `eos_token` exists in the vocabulary before using it as an EOS token."
     ]
    }
   ],
   "source": [
    "# ---------- 4) Configuraci√≥n y entrenamiento ----------\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"outputs-kl\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    logging_steps=1,\n",
    "    report_to=\"none\",\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainerWithKL(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    kl_lambda=0.1,     # peso del KL (aj√∫stalo)\n",
    "    callbacks=[KLLoggerCallback()]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac62682b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 9 | Num Epochs = 5 | Total steps = 45\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 1 x 1) = 1\n",
      " \"-____-\"     Trainable parameters = 21,823,488 of 8,212,558,848 (0.27% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 0] KL = 3.7438\n",
      "[step 0] CE = 1.9908\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 00:22, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.028200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.103200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.061100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.813100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.116900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.047100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.699500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.671100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.734200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.514600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.338900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.347000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.180500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.190900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.980600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.277100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.977000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.105200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.846400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.877000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.804000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.780500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.855900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.653000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.558800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.584700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.518400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.677500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.629800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.551700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.567300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.541300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.553700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.523200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.501700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.617400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.478500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.517400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.597600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.506900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.634700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.491200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.483200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 1] KL = 3.7438\n",
      "[step 1] CE = 2.0658\n",
      "[step 2] KL = 3.7484\n",
      "[step 2] CE = 1.9905\n",
      "[step 3] KL = 3.7540\n",
      "[step 3] CE = 2.0235\n",
      "[step 4] KL = 3.7555\n",
      "[step 4] CE = 1.7755\n",
      "[step 5] KL = 3.7619\n",
      "[step 5] CE = 2.0793\n",
      "[step 6] KL = 3.7796\n",
      "[step 6] CE = 2.0093\n",
      "[step 7] KL = 3.8014\n",
      "[step 7] CE = 1.6615\n",
      "[step 8] KL = 3.8252\n",
      "[step 8] CE = 1.6329\n",
      "[step 9] KL = 3.8570\n",
      "[step 9] CE = 1.6956\n",
      "[step 10] KL = 3.8991\n",
      "[step 10] CE = 1.4756\n",
      "[step 11] KL = 3.9277\n",
      "[step 11] CE = 1.2997\n",
      "[step 12] KL = 3.9744\n",
      "[step 12] CE = 1.3073\n",
      "[step 13] KL = 4.0240\n",
      "[step 13] CE = 1.1402\n",
      "[step 14] KL = 4.0662\n",
      "[step 14] CE = 1.1502\n",
      "[step 15] KL = 4.1194\n",
      "[step 15] CE = 0.9394\n",
      "[step 16] KL = 4.1324\n",
      "[step 16] CE = 1.2358\n",
      "[step 17] KL = 4.1681\n",
      "[step 17] CE = 0.9353\n",
      "[step 18] KL = 4.2021\n",
      "[step 18] CE = 1.0632\n",
      "[step 19] KL = 4.2704\n",
      "[step 19] CE = 0.8037\n",
      "[step 20] KL = 4.3217\n",
      "[step 20] CE = 0.8337\n",
      "[step 21] KL = 4.3978\n",
      "[step 21] CE = 0.7600\n",
      "[step 22] KL = 4.4497\n",
      "[step 22] CE = 0.7360\n",
      "[step 23] KL = 4.5533\n",
      "[step 23] CE = 0.8104\n",
      "[step 24] KL = 4.6416\n",
      "[step 24] CE = 0.6066\n",
      "[step 25] KL = 4.7649\n",
      "[step 25] CE = 0.5111\n",
      "[step 26] KL = 4.8716\n",
      "[step 26] CE = 0.5360\n",
      "[step 27] KL = 4.9587\n",
      "[step 27] CE = 0.4688\n",
      "[step 28] KL = 5.0537\n",
      "[step 28] CE = 0.6369\n",
      "[step 29] KL = 5.1532\n",
      "[step 29] CE = 0.6259\n",
      "[step 30] KL = 5.2254\n",
      "[step 30] CE = 0.5775\n",
      "[step 31] KL = 5.3153\n",
      "[step 31] CE = 0.4986\n",
      "[step 32] KL = 5.3814\n",
      "[step 32] CE = 0.5135\n",
      "[step 33] KL = 5.4169\n",
      "[step 33] CE = 0.4871\n",
      "[step 34] KL = 5.4881\n",
      "[step 34] CE = 0.4988\n",
      "[step 35] KL = 5.5590\n",
      "[step 35] CE = 0.4676\n",
      "[step 36] KL = 5.6012\n",
      "[step 36] CE = 0.4457\n",
      "[step 37] KL = 5.6385\n",
      "[step 37] CE = 0.5610\n",
      "[step 38] KL = 5.6426\n",
      "[step 38] CE = 0.4221\n",
      "[step 39] KL = 5.6732\n",
      "[step 39] CE = 0.4606\n",
      "[step 40] KL = 5.6878\n",
      "[step 40] CE = 0.5407\n",
      "[step 41] KL = 5.7023\n",
      "[step 41] CE = 0.4499\n",
      "[step 42] KL = 5.6723\n",
      "[step 42] CE = 0.5780\n",
      "[step 43] KL = 5.7135\n",
      "[step 43] CE = 0.4340\n",
      "[step 44] KL = 5.7349\n",
      "[step 44] CE = 0.4259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=45, training_loss=1.0278545763757494, metrics={'train_runtime': 23.6025, 'train_samples_per_second': 1.907, 'train_steps_per_second': 1.907, 'total_flos': 262318313963520.0, 'train_loss': 1.0278545763757494, 'epoch': 5.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
