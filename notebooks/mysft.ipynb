{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69384459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miguel/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_1413833/1540875224.py:5: UserWarning: WARNING: Unsloth should be imported before trl, transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel, FastModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 10-30 16:30:48 [__init__.py:216] Automatically detected platform cuda.\n",
      "WARNING 10-30 16:30:49 [interface.py:391] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from unsloth import FastLanguageModel, FastModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37a6b044",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Di hola en una palabra\"},\n",
    "                  {\"role\": \"assistant\", \"content\": \"Hola\"}]},\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Â¿CuÃ¡nto es 2+2?\"},\n",
    "                  {\"role\": \"assistant\", \"content\": \"4\"}]},\n",
    "]\n",
    "train_ds = Dataset.from_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c775a411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- 3) Tu SFTTrainer personalizado con regularizador --------\n",
    "class MySFTTrainer(SFTTrainer):\n",
    "    def __init__(self, *args, lmbda_l2=0.0, lmbda_entropy=0.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.lmbda_l2 = lmbda_l2\n",
    "        self.lmbda_entropy = lmbda_entropy\n",
    "        self._reg_params = None\n",
    "\n",
    "    def _collect_lora_params(self):\n",
    "        if self._reg_params is None:\n",
    "            params = []\n",
    "            for n, p in self.model.named_parameters():\n",
    "                # Filtra los adaptadores LoRA (ajusta el criterio si usas otro naming)\n",
    "                if \"lora_\" in n and p.requires_grad:\n",
    "                    params.append(p)\n",
    "            self._reg_params = params\n",
    "        return self._reg_params\n",
    "\n",
    "    def _l2_regularizer(self):\n",
    "        if self.lmbda_l2 <= 0:\n",
    "            return torch.tensor(0.0, device=self.model.device)\n",
    "        reg = torch.tensor(0.0, device=self.model.device)\n",
    "        for p in self._collect_lora_params():\n",
    "            reg = reg + torch.sum(p ** 2)\n",
    "        return self.lmbda_l2 * reg\n",
    "\n",
    "    def _entropy_regularizer(self, logits, labels):\n",
    "        # EntropÃ­a media de las distribuciones de salida (ignorando etiquetas -100)\n",
    "        if self.lmbda_entropy <= 0:\n",
    "            return torch.tensor(0.0, device=logits.device)\n",
    "        with torch.no_grad():\n",
    "            mask = (labels != -100).float()\n",
    "            count = mask.sum().clamp_min(1.0)\n",
    "        # EntropÃ­a: -sum p log p\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        entropy = -(probs * torch.log(probs.clamp_min(1e-12))).sum(dim=-1)  # [B, T]\n",
    "        # Promedio en posiciones vÃ¡lidas\n",
    "        ent_mean = (entropy * mask).sum() / count\n",
    "        return self.lmbda_entropy * (-ent_mean)  # penaliza entropÃ­a alta (o cambia el signo a tu gusto)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Importante: pedimos outputs para tener 'logits' y 'labels'\n",
    "        outputs = model(**inputs)\n",
    "        # SFTTrainer ya prepara 'labels' en inputs\n",
    "        logits = outputs.logits\n",
    "        labels = inputs.get(\"labels\")\n",
    "\n",
    "        # PÃ©rdida base de SFT (cross-entropy) igual que Trainer\n",
    "        # Si no viene ya en outputs, la calculamos manualmente:\n",
    "        if outputs.loss is not None:\n",
    "            loss = outputs.loss\n",
    "        else:\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = labels[:, 1:].contiguous()\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "        # Suma de regularizadores\n",
    "        reg = self._l2_regularizer() + self._entropy_regularizer(logits, labels)\n",
    "        total = loss + reg\n",
    "\n",
    "        return (total, outputs) if return_outputs else total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a3eff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 512\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name=\"unsloth/Llama-3.2-1B-Instruct\",   # cambia a tu modelo (Qwen, Llama, etc.)\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=True,\n",
    "    load_in_16bit=False,\n",
    "    full_finetuning=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
