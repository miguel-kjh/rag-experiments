{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8e7d68c",
   "metadata": {},
   "source": [
    "### ðŸ§  What is Query Decomposition?\n",
    "Query decomposition is the process of taking a complex, multi-part question and breaking it into simpler, atomic sub-questions that can each be retrieved and answered individually.\n",
    "\n",
    "#### âœ… Why Use Query Decomposition?\n",
    "\n",
    "- Complex queries often involve multiple concepts\n",
    "\n",
    "- LLMs or retrievers may miss parts of the original question\n",
    "\n",
    "- It enables multi-hop reasoning (answering in steps)\n",
    "\n",
    "- Allows parallelism (especially in multi-agent frameworks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b23a442b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miguel/miniconda3/envs/RAG/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.runnables import RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76de0145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and embed the document\n",
    "loader = TextLoader(\"langchain_crewai_dataset.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(chunks, embedding)\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 4, \"lambda_mult\": 0.7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f43149d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7bb9a9779e10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7bb9a8f679d0>, root_client=<openai.OpenAI object at 0x7bb9a97798d0>, root_async_client=<openai.AsyncOpenAI object at 0x7bb9a8f676d0>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "llm=init_chat_model(model=\"gpt-3.5-turbo\",temperature=0)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0af1982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Query decomposition\n",
    "decomposition_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an AI assistant. Decompose the following complex question into 2 to 4 smaller sub-questions for better document retrieval.\n",
    "\n",
    "Question: \"{question}\"\n",
    "\n",
    "Sub-questions:\n",
    "\"\"\")\n",
    "decomposition_chain = decomposition_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c9797ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "decomposition_question=decomposition_chain.invoke({\"question\": query})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b4819b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. How does LangChain utilize memory in its operations?\n",
      "2. How does LangChain employ agents in its system?\n",
      "3. How does CrewAI utilize memory in its operations?\n",
      "4. How does CrewAI employ agents in its system?\n"
     ]
    }
   ],
   "source": [
    "print(decomposition_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5be04719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: QA chain per sub-question\n",
    "qa_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Use the context below to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "qa_chain = create_stuff_documents_chain(llm=llm, prompt=qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26c735b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Full RAG pipeline logic\n",
    "def full_query_decomposition_rag_pipeline(user_query):\n",
    "    # Decompose the query\n",
    "    sub_qs_text = decomposition_chain.invoke({\"question\": user_query})\n",
    "    sub_questions = [q.strip(\"-â€¢1234567890. \").strip() for q in sub_qs_text.split(\"\\n\") if q.strip()]\n",
    "    \n",
    "    results = []\n",
    "    for subq in sub_questions:\n",
    "        docs = retriever.invoke(subq)\n",
    "        result = qa_chain.invoke({\"input\": subq, \"context\": docs})\n",
    "        results.append(f\"Q: {subq}\\nA: {result}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac50f32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Final Answer:\n",
      "\n",
      "Q: How does LangChain utilize memory in its operations?\n",
      "A: LangChain utilizes memory modules like ConversationBufferMemory and ConversationSummaryMemory to allow the LLM to maintain awareness of previous conversation turns or summarize long interactions to fit within token limits.\n",
      "\n",
      "Q: How does LangChain employ agents in its system?\n",
      "A: LangChain employs agents in its system by using LLMs to reason about which tool to call, what input to provide, and how to process the output. The agents can execute multi-step tasks, integrating with tools like web search, calculators, and code execution. They operate using a planner-executor model, planning out a sequence of tool invocations to achieve a goal, including dynamic decision-making, branching logic, and context-aware memory use across steps.\n",
      "\n",
      "Q: How does CrewAI utilize memory in its operations?\n",
      "A: CrewAI utilizes memory in its operations by enabling agents to share context and dynamically communicate with one another. This allows the agents to work together in organized crews, dividing responsibilities and collaborating to complete tasks efficiently. By sharing context and communicating dynamically, CrewAI agents can leverage their collective memory to enhance their performance in multi-step workflows such as market research, legal document analysis, product development, and coding assistance.\n",
      "\n",
      "Q: How does CrewAI employ agents in its system?\n",
      "A: CrewAI employs agents in its system by defining each agent with a specific purpose, goal, and set of tools they can use. Each agent has a defined role within a crew, such as researcher, planner, or executor, and operates semi-independently within a collaborative context. The framework ensures that each agent stays on task and contributes meaningfully to the overall crew objective.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run\n",
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "final_answer = full_query_decomposition_rag_pipeline(query)\n",
    "print(\"âœ… Final Answer:\\n\")\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd01d153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Final Summary:\n",
      "\n",
      "LangChain utilizes memory modules like ConversationBufferMemory and ConversationSummaryMemory to maintain awareness of previous conversation turns and summarize interactions, while CrewAI enables agents to share context and communicate dynamically to work together efficiently in organized crews. Both systems employ agents to execute multi-step tasks and leverage collective memory for enhanced performance.\n"
     ]
    }
   ],
   "source": [
    "# step 7 uses a LLM to reformulate the final answer\n",
    "final_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an AI assistant. Given the following series of question and answer pairs, provide a concise summary answer to the original question.\n",
    "Q&A Pairs:\n",
    "{input}\n",
    "Final Answer:\n",
    "\"\"\")\n",
    "final_chain = final_prompt | llm | StrOutputParser()\n",
    "final_summary = final_chain.invoke({\"input\": final_answer})\n",
    "print(\"âœ… Final Summary:\\n\")\n",
    "print(final_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
