{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02911839",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miguel/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse, os, numpy as np, torch\n",
    "from datasets import load_dataset, ClassLabel\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoConfig, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer, DataCollatorWithPadding, EarlyStoppingCallback\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from transformers import BitsAndBytesConfig\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ee5bd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 29684.73 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 400\n",
      "Labels: ['7500_1', '7500_2', '7500_3', '7500_4', '7500_5', '7501_1', '7501_2', '7501_3', '7501_4', '7501_5', '7502_1', '7502_2', '7502_3', '7502_4', '7502_5', '7503_1', '7503_2', '7503_3', '7503_4', '7503_5', '7504_1', '7504_2', '7504_3', '7504_4', '7504_5', '7505_1', '7505_2', '7505_3', '7505_4', '7505_5', '7506_1', '7506_2', '7506_3', '7506_4', '7506_5', '7507_1', '7507_2', '7507_3', '7507_4', '7507_5', '7508_1', '7508_2', '7508_3', '7508_4', '7508_5', '7509_1', '7509_2', '7509_3', '7509_4', '7509_5', '7510_1', '7510_2', '7510_3', '7510_4', '7510_5', '7511_1', '7511_2', '7511_3', '7511_4', '7511_5', '7512_1', '7512_2', '7512_3', '7512_4', '7512_5', '7513_1', '7513_2', '7513_3', '7513_4', '7513_5', '7514_1', '7514_2', '7514_3', '7514_4', '7514_5', '7515_1', '7515_2', '7515_3', '7515_4', '7515_5', '7516_1', '7516_2', '7516_3', '7516_4', '7516_5', '7517_1', '7517_2', '7517_3', '7517_4', '7517_5', '7518_1', '7518_2', '7518_3', '7518_4', '7518_5', '7519_1', '7519_2', '7519_3', '7519_4', '7519_5', '7520_1', '7520_2', '7520_3', '7520_4', '7520_5', '7521_1', '7521_2', '7521_3', '7521_4', '7521_5', '7522_1', '7522_2', '7522_3', '7522_4', '7522_5', '7523_1', '7523_2', '7523_3', '7523_4', '7523_5', '7524_1', '7524_2', '7524_3', '7524_4', '7524_5', '7525_1', '7525_2', '7525_3', '7525_4', '7525_5', '7526_1', '7526_2', '7526_3', '7526_4', '7526_5', '7527_1', '7527_2', '7527_3', '7527_4', '7527_5', '7528_1', '7528_2', '7528_3', '7528_4', '7528_5', '7529_1', '7529_2', '7529_3', '7529_4', '7529_5', '7530_1', '7530_2', '7530_3', '7530_4', '7530_5', '7531_1', '7531_2', '7531_3', '7531_4', '7531_5', '7532_1', '7532_2', '7532_3', '7532_4', '7532_5', '7533_1', '7533_2', '7533_3', '7533_4', '7533_5', '7534_1', '7534_2', '7534_3', '7534_4', '7534_5', '7535_1', '7535_2', '7535_3', '7535_4', '7535_5', '7536_1', '7536_2', '7536_3', '7536_4', '7536_5', '7537_1', '7537_2', '7537_3', '7537_4', '7537_5', '7538_1', '7538_2', '7538_3', '7538_4', '7538_5', '7539_1', '7539_2', '7539_3', '7539_4', '7539_5', '7540_1', '7540_2', '7540_3', '7540_4', '7540_5', '7541_1', '7541_2', '7541_3', '7541_4', '7541_5', '7542_1', '7542_2', '7542_3', '7542_4', '7542_5', '7543_1', '7543_2', '7543_3', '7543_4', '7543_5', '7544_1', '7544_2', '7544_3', '7544_4', '7544_5', '7545_1', '7545_2', '7545_3', '7545_4', '7545_5', '7546_1', '7546_2', '7546_3', '7546_4', '7546_5', '7547_1', '7547_2', '7547_3', '7547_4', '7547_5', '7548_1', '7548_2', '7548_3', '7548_4', '7548_5', '7549_1', '7549_2', '7549_3', '7549_4', '7549_5', '7550_1', '7550_2', '7550_3', '7550_4', '7550_5', '7551_1', '7551_2', '7551_3', '7551_4', '7551_5', '7552_1', '7552_2', '7552_3', '7552_4', '7552_5', '7553_1', '7553_2', '7553_3', '7553_4', '7553_5', '7554_1', '7554_2', '7554_3', '7554_4', '7554_5', '7555_1', '7555_2', '7555_3', '7555_4', '7555_5', '7556_1', '7556_2', '7556_3', '7556_4', '7556_5', '7557_1', '7557_2', '7557_3', '7557_4', '7557_5', '7558_1', '7558_2', '7558_3', '7558_4', '7558_5', '7559_1', '7559_2', '7559_3', '7559_4', '7559_5', '7560_1', '7560_2', '7560_3', '7560_4', '7560_5', '7561_1', '7561_2', '7561_3', '7561_4', '7561_5', '7562_1', '7562_2', '7562_3', '7562_4', '7562_5', '7563_1', '7563_2', '7563_3', '7563_4', '7563_5', '7564_1', '7564_2', '7564_3', '7564_4', '7564_5', '7565_1', '7565_2', '7565_3', '7565_4', '7565_5', '7566_1', '7566_2', '7566_3', '7566_4', '7566_5', '7567_1', '7567_2', '7567_3', '7567_4', '7567_5', '7568_1', '7568_2', '7568_3', '7568_4', '7568_5', '7569_1', '7569_2', '7569_3', '7569_4', '7569_5', '7570_1', '7570_2', '7570_3', '7570_4', '7570_5', '7571_1', '7571_2', '7571_3', '7571_4', '7571_5', '7572_1', '7572_2', '7572_3', '7572_4', '7572_5', '7573_1', '7573_2', '7573_3', '7573_4', '7573_5', '7574_1', '7574_2', '7574_3', '7574_4', '7574_5', '7575_1', '7575_2', '7575_3', '7575_4', '7575_5', '7576_1', '7576_2', '7576_3', '7576_4', '7576_5', '7577_1', '7577_2', '7577_3', '7577_4', '7577_5', '7578_1', '7578_2', '7578_3', '7578_4', '7578_5', '7579_1', '7579_2', '7579_3', '7579_4', '7579_5']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 48656.41 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_indexing_csv = pd.read_csv(\"../notebooks/data/contacts_docs.csv\")\n",
    "\n",
    "# to dataset huggingface\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset_indexing = Dataset.from_pandas(dataset_indexing_csv)\n",
    "\n",
    "# create a new column 'text' that concatenates 'name', 'phone'\n",
    "def concatenate_columns(example):\n",
    "    return {\n",
    "        \"text\": f\"Nombre: {example['name']}\\nTeléfono: {example['phone']}\"\n",
    "    }\n",
    "dataset_indexing = dataset_indexing.map(concatenate_columns)\n",
    "# rename column 'id' to 'label'\n",
    "dataset_indexing = dataset_indexing.rename_column(\"id\", \"label\")\n",
    "\n",
    "num_labels = len(dataset_indexing['label'])\n",
    "print(f\"Number of labels: {num_labels}\")\n",
    "labels_list = dataset_indexing.unique('label')\n",
    "print(f\"Labels: {labels_list}\")\n",
    "\n",
    "# map labels to integers\n",
    "label_to_id = {label: i for i, label in enumerate(labels_list)}\n",
    "def map_labels(example):\n",
    "    return {\n",
    "        \"label\": label_to_id[example['label']]\n",
    "    }\n",
    "dataset_indexing = dataset_indexing.map(map_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df77002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_dataset_train = pd.read_csv(\"../notebooks/data/contacts_queries_train.csv\")\n",
    "query_dataset_val = pd.read_csv(\"../notebooks/data/contacts_queries_val.csv\")\n",
    "query_dataset_test = pd.read_csv(\"../notebooks/data/contacts_queries_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88962b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1400 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1400/1400 [00:00<00:00, 56029.17 examples/s]\n",
      "Map: 100%|██████████| 300/300 [00:00<00:00, 26971.86 examples/s]\n",
      "Map: 100%|██████████| 300/300 [00:00<00:00, 58746.50 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '¿Cómo puedo contactar con Antonio Alonso?', 'label': 194}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_for_queries = {\n",
    "    \"train\": Dataset.from_pandas(query_dataset_train),\n",
    "    \"validation\": Dataset.from_pandas(query_dataset_val),\n",
    "    \"test\": Dataset.from_pandas(query_dataset_test)\n",
    "}\n",
    "\n",
    "for split in dataset_for_queries:\n",
    "    dataset_for_queries[split] = dataset_for_queries[split].rename_column(\"question\", \"text\")\n",
    "    dataset_for_queries[split] = dataset_for_queries[split].rename_column(\"id\", \"label\")\n",
    "    # map labels to integers\n",
    "    dataset_for_queries[split] = dataset_for_queries[split].map(map_labels)\n",
    "\n",
    "# to dataset huggingface\n",
    "from datasets import DatasetDict\n",
    "dataset_for_queries = DatasetDict(dataset_for_queries)\n",
    "print(dataset_for_queries[\"train\"][1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4136616c",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c076355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents created for indexing: 400\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.retrievers import BM25Retriever\n",
    "\n",
    "def create_documents_from_datasets(datasets):\n",
    "    documents = []\n",
    "    for dataset in datasets:\n",
    "        for item in dataset:\n",
    "            doc = Document(\n",
    "                page_content=item[\"text\"],\n",
    "                metadata={\"label\": item[\"label\"]}\n",
    "            )\n",
    "            documents.append(doc)\n",
    "    return documents\n",
    "documents_indexing = create_documents_from_datasets([dataset_indexing])\n",
    "print(f\"Number of documents created for indexing: {len(documents_indexing)}\")\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents_indexing, embedding_model)\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 100})\n",
    "\n",
    "retriever_sparse = BM25Retriever.from_documents(documents_indexing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e80e801f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]/tmp/ipykernel_769363/1615760136.py:13: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(query)\n",
      "100%|██████████| 300/300 [00:09<00:00, 31.33it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "list_of_labels_embeddings = []\n",
    "list_of_labels_sparse = []\n",
    "list_of_real_labels_embeddings = []\n",
    "for index in tqdm(range(len(dataset_for_queries[\"test\"]))):\n",
    "    query = dataset_for_queries[\"test\"][index][\"text\"]\n",
    "    real_label_id = dataset_for_queries[\"test\"][index][\"label\"]\n",
    "    id_to_label = {v: k for k, v in label_to_id.items()}\n",
    "    real_label = id_to_label[real_label_id]\n",
    "\n",
    "    # retrieve documents\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    # get labels from retrieved documents\n",
    "    retrieved_labels = [doc.metadata[\"label\"] for doc in docs]\n",
    "    # to names\n",
    "    retrieved_labels = [id_to_label[label_id] for label_id in retrieved_labels]\n",
    "    list_of_labels_embeddings.append(retrieved_labels)\n",
    "    list_of_real_labels_embeddings.append([real_label])\n",
    "\n",
    "    # retrieve documents (sparse)\n",
    "    docs_sparse = retriever_sparse.get_relevant_documents(query)\n",
    "    # get labels from retrieved documents\n",
    "    retrieved_labels_sparse = [doc.metadata[\"label\"] for doc in docs_sparse]\n",
    "    # to names\n",
    "    retrieved_labels_sparse = [id_to_label[label_id] for label_id in retrieved_labels_sparse]\n",
    "    list_of_labels_sparse.append(retrieved_labels_sparse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8dd8d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking Metrics:\n",
      "  MRR: 0.7261\n",
      "  mAP: 0.7261\n",
      "  AvgRank: 7.6983\n",
      "  CMC@1: 0.6800\n",
      "  Recall@k (macro)@1: 0.6800\n",
      "  Precision@k (macro)@1: 0.6800\n",
      "  Accuracy@1: 0.6800\n",
      "  F1@k (macro)@1: 0.6800\n",
      "  CMC@5: 0.7600\n",
      "  Recall@k (macro)@5: 0.7600\n",
      "  Precision@k (macro)@5: 0.1520\n",
      "  Accuracy@5: 0.7600\n",
      "  F1@k (macro)@5: 0.2533\n",
      "  CMC@10: 0.8300\n",
      "  Recall@k (macro)@10: 0.8300\n",
      "  Precision@k (macro)@10: 0.0830\n",
      "  Accuracy@10: 0.8300\n",
      "  F1@k (macro)@10: 0.1509\n",
      "  CMC@20: 0.8833\n",
      "  Recall@k (macro)@20: 0.8833\n",
      "  Precision@k (macro)@20: 0.0442\n",
      "  Accuracy@20: 0.8833\n",
      "  F1@k (macro)@20: 0.0841\n",
      "  CMC@100: 0.9833\n",
      "  Recall@k (macro)@100: 0.9833\n",
      "  Precision@k (macro)@100: 0.0098\n",
      "  Accuracy@100: 0.9833\n",
      "  F1@k (macro)@100: 0.0195\n"
     ]
    }
   ],
   "source": [
    "from ranking_metrics import calc_ranking_metrics\n",
    "\n",
    "metrics = calc_ranking_metrics(list_of_labels_embeddings, list_of_real_labels_embeddings, ks=[1, 5, 10, 20, 100], one_relevant_per_query=True)\n",
    "\n",
    "print(\"Ranking Metrics:\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4709eca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse Ranking Metrics:\n",
      "  MRR: 0.5539\n",
      "  mAP: 0.5539\n",
      "  AvgRank: 1.4158\n",
      "  CMC@1: 0.5133\n",
      "  Recall@k (macro)@1: 0.5133\n",
      "  Precision@k (macro)@1: 0.5133\n",
      "  Accuracy@1: 0.5133\n",
      "  F1@k (macro)@1: 0.5133\n",
      "  CMC@5: 0.6333\n",
      "  Recall@k (macro)@5: 0.6333\n",
      "  Precision@k (macro)@5: 0.1267\n",
      "  Accuracy@5: 0.6333\n",
      "  F1@k (macro)@5: 0.2111\n",
      "  CMC@10: 0.6333\n",
      "  Recall@k (macro)@10: 0.6333\n",
      "  Precision@k (macro)@10: 0.0633\n",
      "  Accuracy@10: 0.6333\n",
      "  F1@k (macro)@10: 0.1152\n",
      "  CMC@20: 0.6333\n",
      "  Recall@k (macro)@20: 0.6333\n",
      "  Precision@k (macro)@20: 0.0317\n",
      "  Accuracy@20: 0.6333\n",
      "  F1@k (macro)@20: 0.0603\n",
      "  CMC@100: 0.6333\n",
      "  Recall@k (macro)@100: 0.6333\n",
      "  Precision@k (macro)@100: 0.0063\n",
      "  Accuracy@100: 0.6333\n",
      "  F1@k (macro)@100: 0.0125\n"
     ]
    }
   ],
   "source": [
    "metrics_sparse = calc_ranking_metrics(list_of_labels_sparse, list_of_real_labels_embeddings, ks=[1, 5, 10, 20, 100], one_relevant_per_query=True)\n",
    "\n",
    "print(\"Sparse Ranking Metrics:\")\n",
    "for k, v in metrics_sparse.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1107a838",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19767e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4f1a9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-0.6B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b372807",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_r = 256\n",
    "lora_alpha = lora_r * 2\n",
    "lora_dropout = 0.0\n",
    "lora_bias = \"none\"\n",
    "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"down_proj\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f23ba430",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    inference_mode=False,\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=lora_bias,\n",
    "    target_modules=target_modules\n",
    ")\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c43bc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.base_model.model.score.parameters():\n",
    "    p.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "579c201d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 50917.20 examples/s]\n",
      "Map: 100%|██████████| 1400/1400 [00:00<00:00, 80521.43 examples/s]\n",
      "Map: 100%|██████████| 300/300 [00:00<00:00, 50877.05 examples/s]\n",
      "Map: 100%|██████████| 300/300 [00:00<00:00, 52767.39 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=MAX_LENGTH, padding=False)\n",
    "\n",
    "# tokenize test from dataset\n",
    "tokenized_datasets_indexing = dataset_indexing.map(preprocess, batched=True)\n",
    "tokenized_datasets_query = dataset_for_queries.map(preprocess, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c2db441",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8)\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    # Restar el máximo para evitar overflow\n",
    "    x_shift = x - np.max(x, axis=axis, keepdims=True)\n",
    "    exps = np.exp(x_shift)\n",
    "    return exps / np.sum(exps, axis=axis, keepdims=True)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = (preds == labels).mean()\n",
    "    # softmax to get probabilities using numpy\n",
    "    probs = softmax(logits, axis=-1)\n",
    "    # get list of labels sorted by probability\n",
    "    sorted_indices = np.argsort(probs, axis=-1)\n",
    "    # reverse to have the highest probability first\n",
    "    sorted_indices = sorted_indices[:, ::-1]\n",
    "    list_of_labels = [\n",
    "        [idx] for idx in labels  \n",
    "    ]\n",
    "\n",
    "    ranking_metrics = calc_ranking_metrics(\n",
    "        sorted_indices, \n",
    "        list_of_labels, \n",
    "        ks=[1, 5, 10, 20, 100], \n",
    "        one_relevant_per_query=True\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,  \n",
    "        \"mAP\": ranking_metrics[\"mAP\"], \n",
    "        \"Hint@1\": ranking_metrics[\"Accuracy@1\"], \n",
    "        \"Hint@10\": ranking_metrics[\"Accuracy@10\"],\n",
    "        \"Hint@100\": ranking_metrics[\"Accuracy@100\"],\n",
    "        # recall@k\n",
    "        \"recall@1\": ranking_metrics[\"Recall@k (macro)@1\"],\n",
    "        \"recall@10\": ranking_metrics[\"Recall@k (macro)@10\"],\n",
    "        \"recall@100\": ranking_metrics[\"Recall@k (macro)@100\"],\n",
    "        # precision@k\n",
    "        \"precision@1\": ranking_metrics[\"Precision@k (macro)@1\"],\n",
    "        \"precision@10\": ranking_metrics[\"Precision@k (macro)@10\"],\n",
    "        \"precision@100\": ranking_metrics[\"Precision@k (macro)@100\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcd6998e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test compute_metrics: {'accuracy': np.float64(0.0), 'mAP': 0.5, 'Hint@1': 0.0, 'Hint@10': 1.0, 'Hint@100': 1.0, 'recall@1': 0.0, 'recall@10': 1.0, 'recall@100': 1.0, 'precision@1': 0.0, 'precision@10': 0.1, 'precision@100': 0.01}\n"
     ]
    }
   ],
   "source": [
    "# testing compute_metrics\n",
    "test_logits = torch.tensor([[0.1, 0.69, 0.99], [0.8, 0.1, 0.91]])\n",
    "test_labels = torch.tensor([1, 0])\n",
    "test_eval_pred = (test_logits.detach().numpy(), test_labels.detach().numpy())\n",
    "test_metrics = compute_metrics(test_eval_pred)\n",
    "print(f\"Test compute_metrics: {test_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddac5382",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "CYCLES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6601a52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmiguel_kjh\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/miguel/projects/rag-experiments/notebooks/wandb/run-20251027_161633-sl41lu3t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/miguel_kjh/agenda_multilabel_classification_retriever/runs/sl41lu3t' target=\"_blank\">entrenamiento_indexing_y_query</a></strong> to <a href='https://wandb.ai/miguel_kjh/agenda_multilabel_classification_retriever' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/miguel_kjh/agenda_multilabel_classification_retriever' target=\"_blank\">https://wandb.ai/miguel_kjh/agenda_multilabel_classification_retriever</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/miguel_kjh/agenda_multilabel_classification_retriever/runs/sl41lu3t' target=\"_blank\">https://wandb.ai/miguel_kjh/agenda_multilabel_classification_retriever/runs/sl41lu3t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [langchain] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/miguel_kjh/agenda_multilabel_classification_retriever/runs/sl41lu3t?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7abdba0ae260>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 10 that is less than the current step 11. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 20 that is less than the current step 21. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 30 that is less than the current step 31. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 40 that is less than the current step 41. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 50 that is less than the current step 51. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 60 that is less than the current step 61. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 70 that is less than the current step 71. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 80 that is less than the current step 81. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 90 that is less than the current step 91. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 100 that is less than the current step 101. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 110 that is less than the current step 111. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 120 that is less than the current step 121. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 130 that is less than the current step 131. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 130 that is less than the current step 132. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 130 that is less than the current step 133. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 10 that is less than the current step 134. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 10 that is less than the current step 135. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 20 that is less than the current step 136. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 20 that is less than the current step 137. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 30 that is less than the current step 138. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 30 that is less than the current step 139. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 40 that is less than the current step 140. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 40 that is less than the current step 141. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 50 that is less than the current step 142. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 50 that is less than the current step 143. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 60 that is less than the current step 144. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 60 that is less than the current step 145. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 70 that is less than the current step 146. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 70 that is less than the current step 147. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 80 that is less than the current step 148. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 80 that is less than the current step 149. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 90 that is less than the current step 150. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 90 that is less than the current step 151. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 100 that is less than the current step 152. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 100 that is less than the current step 153. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 110 that is less than the current step 154. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 110 that is less than the current step 155. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 120 that is less than the current step 156. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 120 that is less than the current step 157. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 130 that is less than the current step 158. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 130 that is less than the current step 159. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 140 that is less than the current step 160. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 140 that is less than the current step 161. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 150 that is less than the current step 162. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 150 that is less than the current step 163. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 160 that is less than the current step 164. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 160 that is less than the current step 165. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 170 that is less than the current step 171. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 180 that is less than the current step 181. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 190 that is less than the current step 191. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 200 that is less than the current step 201. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 210 that is less than the current step 211. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 220 that is less than the current step 221. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 230 that is less than the current step 231. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 240 that is less than the current step 241. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 250 that is less than the current step 251. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 260 that is less than the current step 261. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 270 that is less than the current step 271. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 280 that is less than the current step 281. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 290 that is less than the current step 291. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 300 that is less than the current step 301. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 310 that is less than the current step 311. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 320 that is less than the current step 321. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 330 that is less than the current step 331. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 340 that is less than the current step 341. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 350 that is less than the current step 351. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 360 that is less than the current step 361. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 370 that is less than the current step 371. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 380 that is less than the current step 381. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 390 that is less than the current step 391. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 400 that is less than the current step 401. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 410 that is less than the current step 411. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 420 that is less than the current step 421. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 430 that is less than the current step 431. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 440 that is less than the current step 441. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 440 that is less than the current step 442. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 440 that is less than the current step 443. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 10 that is less than the current step 444. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 10 that is less than the current step 445. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 20 that is less than the current step 446. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 20 that is less than the current step 447. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 30 that is less than the current step 448. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 30 that is less than the current step 449. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 40 that is less than the current step 450. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 40 that is less than the current step 451. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 50 that is less than the current step 452. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 50 that is less than the current step 453. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 60 that is less than the current step 454. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 60 that is less than the current step 455. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 70 that is less than the current step 456. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 70 that is less than the current step 457. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 80 that is less than the current step 458. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 80 that is less than the current step 459. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 90 that is less than the current step 460. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 90 that is less than the current step 461. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 100 that is less than the current step 462. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 100 that is less than the current step 463. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 110 that is less than the current step 464. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 110 that is less than the current step 465. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 120 that is less than the current step 466. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 120 that is less than the current step 467. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 130 that is less than the current step 468. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 130 that is less than the current step 469. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 130 that is less than the current step 470. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 130 that is less than the current step 471. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 10 that is less than the current step 472. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 10 that is less than the current step 473. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 20 that is less than the current step 474. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 20 that is less than the current step 475. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 30 that is less than the current step 476. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 30 that is less than the current step 477. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 40 that is less than the current step 478. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 40 that is less than the current step 479. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 50 that is less than the current step 480. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 50 that is less than the current step 481. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 60 that is less than the current step 482. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 60 that is less than the current step 483. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 70 that is less than the current step 484. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 70 that is less than the current step 485. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 80 that is less than the current step 486. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 80 that is less than the current step 487. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 90 that is less than the current step 488. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 90 that is less than the current step 489. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 100 that is less than the current step 490. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 100 that is less than the current step 491. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 110 that is less than the current step 492. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 110 that is less than the current step 493. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 120 that is less than the current step 494. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 120 that is less than the current step 495. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 130 that is less than the current step 496. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 130 that is less than the current step 497. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 140 that is less than the current step 498. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 140 that is less than the current step 499. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 150 that is less than the current step 500. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 150 that is less than the current step 501. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 160 that is less than the current step 502. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 160 that is less than the current step 503. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 170 that is less than the current step 504. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 170 that is less than the current step 505. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 180 that is less than the current step 506. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 180 that is less than the current step 507. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 190 that is less than the current step 508. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 190 that is less than the current step 509. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 200 that is less than the current step 510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 200 that is less than the current step 511. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 210 that is less than the current step 512. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 210 that is less than the current step 513. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 220 that is less than the current step 514. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 220 that is less than the current step 515. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 230 that is less than the current step 516. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 230 that is less than the current step 517. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 240 that is less than the current step 518. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 240 that is less than the current step 519. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 250 that is less than the current step 520. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 250 that is less than the current step 521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 260 that is less than the current step 522. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 260 that is less than the current step 523. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 270 that is less than the current step 524. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 270 that is less than the current step 525. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 280 that is less than the current step 526. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 280 that is less than the current step 527. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 290 that is less than the current step 528. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 290 that is less than the current step 529. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 300 that is less than the current step 530. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 300 that is less than the current step 531. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 310 that is less than the current step 532. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 310 that is less than the current step 533. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 320 that is less than the current step 534. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 320 that is less than the current step 535. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 330 that is less than the current step 536. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 330 that is less than the current step 537. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 340 that is less than the current step 538. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 340 that is less than the current step 539. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 350 that is less than the current step 540. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 350 that is less than the current step 541. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 360 that is less than the current step 542. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 360 that is less than the current step 543. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 370 that is less than the current step 544. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 370 that is less than the current step 545. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 380 that is less than the current step 546. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 380 that is less than the current step 547. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 390 that is less than the current step 548. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 390 that is less than the current step 549. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 400 that is less than the current step 550. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 400 that is less than the current step 551. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 410 that is less than the current step 552. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 410 that is less than the current step 553. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 420 that is less than the current step 554. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 420 that is less than the current step 555. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 430 that is less than the current step 556. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 430 that is less than the current step 557. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 440 that is less than the current step 558. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 440 that is less than the current step 559. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 440 that is less than the current step 560. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 440 that is less than the current step 561. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    project=\"agenda_multilabel_classification_retriever\",\n",
    "    name=\"entrenamiento_indexing_y_query\",\n",
    "    config={\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"model\": model_name,\n",
    "        \"cycles\": CYCLES\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7d1c013",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class PrefixedWandbCallback(TrainerCallback):\n",
    "    def __init__(self, phase=\"\"):\n",
    "        self.phase = phase\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            prefixed_logs = {f\"{self.phase}/{k}\": v for k, v in logs.items()}\n",
    "            wandb.log(prefixed_logs, step=state.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3585bfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento\n",
    "\n",
    "training_args_indexing = TrainingArguments(\n",
    "    output_dir=f\"models/contacts_clf_{model_name.replace('/', '_')}_indexing\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.05,\n",
    "    warmup_ratio=0.2,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    eval_strategy=\"steps\",     # o \"no\" si no vas a evaluar\n",
    "    save_strategy=\"no\",        # <-- no guarda checkpoints ni el modelo final\n",
    "    eval_steps=10,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=False,  # <-- desactivado porque no hay checkpoints\n",
    "    fp16=True,\n",
    "    report_to=\"wandb\",\n",
    "    seed=SEED,\n",
    "    label_smoothing_factor=0.1,\n",
    ")\n",
    "\n",
    "training_args_query = TrainingArguments(\n",
    "    output_dir=f\"models/contacts_clf_{model_name.replace('/', '_')}_query\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.05,\n",
    "    warmup_ratio=0.2,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"no\",        # <-- no guarda checkpoints ni el modelo final\n",
    "    eval_steps=10,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=False,  # <-- desactivado porque no hay checkpoints\n",
    "    fp16=True,\n",
    "    report_to=\"wandb\",\n",
    "    seed=SEED,\n",
    "    label_smoothing_factor=0.1,\n",
    ")\n",
    "\n",
    "# IDs de tokens\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.bos_token_id = tokenizer.bos_token_id\n",
    "\n",
    "trainer_indexing = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_indexing,\n",
    "    train_dataset=tokenized_datasets_indexing,\n",
    "    eval_dataset=tokenized_datasets_query[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[\n",
    "        PrefixedWandbCallback(phase=\"indexing\"),\n",
    "        #EarlyStoppingCallback(early_stopping_patience=3),\n",
    "    ],\n",
    ")\n",
    "\n",
    "trainer_query = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_query,\n",
    "    train_dataset=tokenized_datasets_query[\"train\"],\n",
    "    eval_dataset=tokenized_datasets_query[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[\n",
    "        PrefixedWandbCallback(phase=\"query\"),\n",
    "        #EarlyStoppingCallback(early_stopping_patience=3),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bc5c22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 132,939,776 || all params: 728,989,696 || trainable%: 18.2362\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()  # Verificar parámetros entrenables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "044ea9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########Starting training cycle 1##########\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='130' max='130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [130/130 00:27, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Map</th>\n",
       "      <th>Hint@1</th>\n",
       "      <th>Hint@10</th>\n",
       "      <th>Hint@100</th>\n",
       "      <th>Recall@1</th>\n",
       "      <th>Recall@10</th>\n",
       "      <th>Recall@100</th>\n",
       "      <th>Precision@1</th>\n",
       "      <th>Precision@10</th>\n",
       "      <th>Precision@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>7.823100</td>\n",
       "      <td>7.826665</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.024571</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.002700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>6.260700</td>\n",
       "      <td>7.136801</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.017873</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.002667</td>\n",
       "      <td>0.002700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>6.130700</td>\n",
       "      <td>6.715888</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.019513</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.023333</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.023333</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.002333</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>6.040800</td>\n",
       "      <td>6.679628</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.017478</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>0.263333</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>0.263333</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.002667</td>\n",
       "      <td>0.002633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.832200</td>\n",
       "      <td>6.738959</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.019016</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.002667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>5.340300</td>\n",
       "      <td>6.989319</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.023146</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.276667</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.276667</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.002767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>4.706000</td>\n",
       "      <td>7.199372</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.029392</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.046667</td>\n",
       "      <td>0.276667</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.046667</td>\n",
       "      <td>0.276667</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.004667</td>\n",
       "      <td>0.002767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.501000</td>\n",
       "      <td>7.152828</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.033152</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.303333</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.303333</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.003033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.227300</td>\n",
       "      <td>7.415062</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.038590</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.306667</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.306667</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.003067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.489000</td>\n",
       "      <td>7.436361</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.034661</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.076667</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.076667</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.007667</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.263800</td>\n",
       "      <td>7.431804</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.036251</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.073333</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.073333</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.007333</td>\n",
       "      <td>0.002933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.168100</td>\n",
       "      <td>7.438257</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.038137</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.073333</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.073333</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.007333</td>\n",
       "      <td>0.002933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.150600</td>\n",
       "      <td>7.440542</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.037936</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.073333</td>\n",
       "      <td>0.296667</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.073333</td>\n",
       "      <td>0.296667</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.007333</td>\n",
       "      <td>0.002967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 01:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing Dataset Test Metrics: {\n",
      "    \"eval_loss\": 7.465508460998535,\n",
      "    \"eval_accuracy\": 0.0033333333333333335,\n",
      "    \"eval_mAP\": 0.02296182578005492,\n",
      "    \"eval_Hint@1\": 0.0033333333333333335,\n",
      "    \"eval_Hint@10\": 0.043333333333333335,\n",
      "    \"eval_Hint@100\": 0.35333333333333333,\n",
      "    \"eval_recall@1\": 0.0033333333333333335,\n",
      "    \"eval_recall@10\": 0.043333333333333335,\n",
      "    \"eval_recall@100\": 0.35333333333333333,\n",
      "    \"eval_precision@1\": 0.0033333333333333335,\n",
      "    \"eval_precision@10\": 0.004333333333333333,\n",
      "    \"eval_precision@100\": 0.003533333333333333,\n",
      "    \"eval_runtime\": 0.5539,\n",
      "    \"eval_samples_per_second\": 541.59,\n",
      "    \"eval_steps_per_second\": 18.053,\n",
      "    \"epoch\": 10.0\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='440' max='440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [440/440 01:29, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Map</th>\n",
       "      <th>Hint@1</th>\n",
       "      <th>Hint@10</th>\n",
       "      <th>Hint@100</th>\n",
       "      <th>Recall@1</th>\n",
       "      <th>Recall@10</th>\n",
       "      <th>Recall@100</th>\n",
       "      <th>Precision@1</th>\n",
       "      <th>Precision@10</th>\n",
       "      <th>Precision@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>7.035400</td>\n",
       "      <td>6.460120</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.041989</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.346667</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.346667</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.003467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5.941900</td>\n",
       "      <td>5.792597</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.085633</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>0.004900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>5.511800</td>\n",
       "      <td>5.381407</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.141382</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.256667</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.256667</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.025667</td>\n",
       "      <td>0.005800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>5.113500</td>\n",
       "      <td>4.964217</td>\n",
       "      <td>0.156667</td>\n",
       "      <td>0.216049</td>\n",
       "      <td>0.156667</td>\n",
       "      <td>0.356667</td>\n",
       "      <td>0.643333</td>\n",
       "      <td>0.156667</td>\n",
       "      <td>0.356667</td>\n",
       "      <td>0.643333</td>\n",
       "      <td>0.156667</td>\n",
       "      <td>0.035667</td>\n",
       "      <td>0.006433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.497000</td>\n",
       "      <td>4.798158</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.253580</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>0.663333</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>0.663333</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.038333</td>\n",
       "      <td>0.006633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.186900</td>\n",
       "      <td>4.669966</td>\n",
       "      <td>0.216667</td>\n",
       "      <td>0.289008</td>\n",
       "      <td>0.216667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.673333</td>\n",
       "      <td>0.216667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.673333</td>\n",
       "      <td>0.216667</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.006733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>4.169000</td>\n",
       "      <td>4.513351</td>\n",
       "      <td>0.256667</td>\n",
       "      <td>0.319008</td>\n",
       "      <td>0.256667</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.723333</td>\n",
       "      <td>0.256667</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.723333</td>\n",
       "      <td>0.256667</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>0.007233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>4.304300</td>\n",
       "      <td>4.408713</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.335853</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.473333</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.473333</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.047333</td>\n",
       "      <td>0.007333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.847600</td>\n",
       "      <td>4.097679</td>\n",
       "      <td>0.363333</td>\n",
       "      <td>0.424435</td>\n",
       "      <td>0.363333</td>\n",
       "      <td>0.526667</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.363333</td>\n",
       "      <td>0.526667</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.363333</td>\n",
       "      <td>0.052667</td>\n",
       "      <td>0.007667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.306100</td>\n",
       "      <td>4.185694</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.414009</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.553333</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.553333</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.055333</td>\n",
       "      <td>0.007800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>3.191300</td>\n",
       "      <td>3.948184</td>\n",
       "      <td>0.376667</td>\n",
       "      <td>0.453866</td>\n",
       "      <td>0.376667</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.376667</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.376667</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.008233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.881900</td>\n",
       "      <td>3.862306</td>\n",
       "      <td>0.426667</td>\n",
       "      <td>0.487116</td>\n",
       "      <td>0.426667</td>\n",
       "      <td>0.606667</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.426667</td>\n",
       "      <td>0.606667</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.426667</td>\n",
       "      <td>0.060667</td>\n",
       "      <td>0.008500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.934600</td>\n",
       "      <td>3.567106</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.537262</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.873333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.873333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.066000</td>\n",
       "      <td>0.008733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.121000</td>\n",
       "      <td>3.410472</td>\n",
       "      <td>0.546667</td>\n",
       "      <td>0.606277</td>\n",
       "      <td>0.546667</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>0.856667</td>\n",
       "      <td>0.546667</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>0.856667</td>\n",
       "      <td>0.546667</td>\n",
       "      <td>0.070667</td>\n",
       "      <td>0.008567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.903800</td>\n",
       "      <td>3.351478</td>\n",
       "      <td>0.546667</td>\n",
       "      <td>0.612644</td>\n",
       "      <td>0.546667</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.546667</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.546667</td>\n",
       "      <td>0.072000</td>\n",
       "      <td>0.008700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.915900</td>\n",
       "      <td>3.156824</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.662613</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.756667</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.756667</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.075667</td>\n",
       "      <td>0.008900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.881200</td>\n",
       "      <td>2.883879</td>\n",
       "      <td>0.656667</td>\n",
       "      <td>0.705385</td>\n",
       "      <td>0.656667</td>\n",
       "      <td>0.793333</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>0.656667</td>\n",
       "      <td>0.793333</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>0.656667</td>\n",
       "      <td>0.079333</td>\n",
       "      <td>0.008867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.644000</td>\n",
       "      <td>2.790023</td>\n",
       "      <td>0.676667</td>\n",
       "      <td>0.731760</td>\n",
       "      <td>0.676667</td>\n",
       "      <td>0.813333</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.676667</td>\n",
       "      <td>0.813333</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.676667</td>\n",
       "      <td>0.081333</td>\n",
       "      <td>0.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.423400</td>\n",
       "      <td>2.712394</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.730553</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.083000</td>\n",
       "      <td>0.009200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.394000</td>\n",
       "      <td>2.658722</td>\n",
       "      <td>0.703333</td>\n",
       "      <td>0.748761</td>\n",
       "      <td>0.703333</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.703333</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.703333</td>\n",
       "      <td>0.083000</td>\n",
       "      <td>0.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.393200</td>\n",
       "      <td>2.571423</td>\n",
       "      <td>0.713333</td>\n",
       "      <td>0.761738</td>\n",
       "      <td>0.713333</td>\n",
       "      <td>0.836667</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.713333</td>\n",
       "      <td>0.836667</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.713333</td>\n",
       "      <td>0.083667</td>\n",
       "      <td>0.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.301200</td>\n",
       "      <td>2.428069</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.798045</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.846667</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.846667</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.084667</td>\n",
       "      <td>0.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.250200</td>\n",
       "      <td>2.338925</td>\n",
       "      <td>0.796667</td>\n",
       "      <td>0.822357</td>\n",
       "      <td>0.793333</td>\n",
       "      <td>0.873333</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.793333</td>\n",
       "      <td>0.873333</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.793333</td>\n",
       "      <td>0.087333</td>\n",
       "      <td>0.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.210400</td>\n",
       "      <td>2.301188</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.826042</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.087000</td>\n",
       "      <td>0.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.218100</td>\n",
       "      <td>2.289827</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.827228</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.913333</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.913333</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.087000</td>\n",
       "      <td>0.009133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.207300</td>\n",
       "      <td>2.221137</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.832807</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.923333</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.923333</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.088000</td>\n",
       "      <td>0.009233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.158500</td>\n",
       "      <td>2.309638</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.828790</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.923333</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.923333</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.088000</td>\n",
       "      <td>0.009233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.119700</td>\n",
       "      <td>2.213659</td>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.841992</td>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.087667</td>\n",
       "      <td>0.009200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.128000</td>\n",
       "      <td>2.176015</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.847602</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.087667</td>\n",
       "      <td>0.009167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.119700</td>\n",
       "      <td>2.092677</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.848293</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.873333</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.873333</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.087333</td>\n",
       "      <td>0.009167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.087600</td>\n",
       "      <td>2.118739</td>\n",
       "      <td>0.843333</td>\n",
       "      <td>0.852965</td>\n",
       "      <td>0.843333</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.913333</td>\n",
       "      <td>0.843333</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.913333</td>\n",
       "      <td>0.843333</td>\n",
       "      <td>0.086667</td>\n",
       "      <td>0.009133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.062400</td>\n",
       "      <td>2.073494</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.857244</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.923333</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.923333</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.087000</td>\n",
       "      <td>0.009233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.056800</td>\n",
       "      <td>2.073429</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>0.858597</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>0.087000</td>\n",
       "      <td>0.009267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.058400</td>\n",
       "      <td>2.027529</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.858221</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.087000</td>\n",
       "      <td>0.009200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.048200</td>\n",
       "      <td>2.050610</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>0.860090</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>0.923333</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>0.923333</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>0.087667</td>\n",
       "      <td>0.009233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.027000</td>\n",
       "      <td>2.057936</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.860674</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>0.923333</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>0.923333</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.087667</td>\n",
       "      <td>0.009233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.017200</td>\n",
       "      <td>2.032630</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.861428</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.088000</td>\n",
       "      <td>0.009200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.017000</td>\n",
       "      <td>2.019130</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>0.862216</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>0.088000</td>\n",
       "      <td>0.009200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.014300</td>\n",
       "      <td>2.023190</td>\n",
       "      <td>0.856667</td>\n",
       "      <td>0.864010</td>\n",
       "      <td>0.856667</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.856667</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.856667</td>\n",
       "      <td>0.087667</td>\n",
       "      <td>0.009200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.007700</td>\n",
       "      <td>2.020602</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.866014</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.087667</td>\n",
       "      <td>0.009167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>1.002000</td>\n",
       "      <td>2.026547</td>\n",
       "      <td>0.856667</td>\n",
       "      <td>0.864364</td>\n",
       "      <td>0.856667</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.856667</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.856667</td>\n",
       "      <td>0.087667</td>\n",
       "      <td>0.009167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.999600</td>\n",
       "      <td>2.027508</td>\n",
       "      <td>0.856667</td>\n",
       "      <td>0.864372</td>\n",
       "      <td>0.856667</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.856667</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.856667</td>\n",
       "      <td>0.087667</td>\n",
       "      <td>0.009167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.027338</td>\n",
       "      <td>0.856667</td>\n",
       "      <td>0.864373</td>\n",
       "      <td>0.856667</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.856667</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.856667</td>\n",
       "      <td>0.087667</td>\n",
       "      <td>0.009167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.999600</td>\n",
       "      <td>2.027075</td>\n",
       "      <td>0.856667</td>\n",
       "      <td>0.864370</td>\n",
       "      <td>0.856667</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.856667</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.856667</td>\n",
       "      <td>0.087667</td>\n",
       "      <td>0.009167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Dataset Test Metrics: {\n",
      "    \"eval_loss\": 2.063112735748291,\n",
      "    \"eval_accuracy\": 0.8233333333333334,\n",
      "    \"eval_mAP\": 0.837499539564592,\n",
      "    \"eval_Hint@1\": 0.8233333333333334,\n",
      "    \"eval_Hint@10\": 0.8633333333333333,\n",
      "    \"eval_Hint@100\": 0.9266666666666666,\n",
      "    \"eval_recall@1\": 0.8233333333333334,\n",
      "    \"eval_recall@10\": 0.8633333333333333,\n",
      "    \"eval_recall@100\": 0.9266666666666666,\n",
      "    \"eval_precision@1\": 0.8233333333333334,\n",
      "    \"eval_precision@10\": 0.08633333333333333,\n",
      "    \"eval_precision@100\": 0.009266666666666666,\n",
      "    \"eval_runtime\": 0.5656,\n",
      "    \"eval_samples_per_second\": 530.44,\n",
      "    \"eval_steps_per_second\": 17.681,\n",
      "    \"epoch\": 10.0\n",
      "}\n",
      "##########Starting training cycle 2##########\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='130' max='130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [130/130 00:26, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Map</th>\n",
       "      <th>Hint@1</th>\n",
       "      <th>Hint@10</th>\n",
       "      <th>Hint@100</th>\n",
       "      <th>Recall@1</th>\n",
       "      <th>Recall@10</th>\n",
       "      <th>Recall@100</th>\n",
       "      <th>Precision@1</th>\n",
       "      <th>Precision@10</th>\n",
       "      <th>Precision@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.180100</td>\n",
       "      <td>1.990600</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.872000</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.088667</td>\n",
       "      <td>0.009267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.084800</td>\n",
       "      <td>1.994292</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.898441</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.943333</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.943333</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.009433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.111700</td>\n",
       "      <td>2.272829</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.884513</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.903333</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.903333</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.090333</td>\n",
       "      <td>0.009533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.249200</td>\n",
       "      <td>2.091163</td>\n",
       "      <td>0.843333</td>\n",
       "      <td>0.865665</td>\n",
       "      <td>0.843333</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.963333</td>\n",
       "      <td>0.843333</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.963333</td>\n",
       "      <td>0.843333</td>\n",
       "      <td>0.090667</td>\n",
       "      <td>0.009633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.137400</td>\n",
       "      <td>2.273551</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.860117</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.091000</td>\n",
       "      <td>0.009700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.129100</td>\n",
       "      <td>2.323157</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.877203</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.903333</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.903333</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.090333</td>\n",
       "      <td>0.009667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.088300</td>\n",
       "      <td>2.438304</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.880418</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.009700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.049100</td>\n",
       "      <td>2.304219</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.883896</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.090667</td>\n",
       "      <td>0.009700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.036600</td>\n",
       "      <td>2.269526</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.892813</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.091000</td>\n",
       "      <td>0.009733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.003300</td>\n",
       "      <td>2.360610</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.895071</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.090667</td>\n",
       "      <td>0.009733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.993700</td>\n",
       "      <td>2.304598</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>0.897279</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.976667</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.976667</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>0.090667</td>\n",
       "      <td>0.009767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.978500</td>\n",
       "      <td>2.327823</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>0.897363</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.976667</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.976667</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>0.091000</td>\n",
       "      <td>0.009767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.974400</td>\n",
       "      <td>2.323342</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>0.897361</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.976667</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.976667</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>0.091000</td>\n",
       "      <td>0.009767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing Dataset Test Metrics: {\n",
      "    \"eval_loss\": 2.2778730392456055,\n",
      "    \"eval_accuracy\": 0.9,\n",
      "    \"eval_mAP\": 0.9092368525752316,\n",
      "    \"eval_Hint@1\": 0.9,\n",
      "    \"eval_Hint@10\": 0.92,\n",
      "    \"eval_Hint@100\": 0.9733333333333334,\n",
      "    \"eval_recall@1\": 0.9,\n",
      "    \"eval_recall@10\": 0.92,\n",
      "    \"eval_recall@100\": 0.9733333333333334,\n",
      "    \"eval_precision@1\": 0.9,\n",
      "    \"eval_precision@10\": 0.092,\n",
      "    \"eval_precision@100\": 0.009733333333333333,\n",
      "    \"eval_runtime\": 0.57,\n",
      "    \"eval_samples_per_second\": 526.35,\n",
      "    \"eval_steps_per_second\": 17.545,\n",
      "    \"epoch\": 10.0\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='440' max='440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [440/440 01:30, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Map</th>\n",
       "      <th>Hint@1</th>\n",
       "      <th>Hint@10</th>\n",
       "      <th>Hint@100</th>\n",
       "      <th>Recall@1</th>\n",
       "      <th>Recall@10</th>\n",
       "      <th>Recall@100</th>\n",
       "      <th>Precision@1</th>\n",
       "      <th>Precision@10</th>\n",
       "      <th>Precision@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.345400</td>\n",
       "      <td>2.052304</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.899928</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.090667</td>\n",
       "      <td>0.009800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.082800</td>\n",
       "      <td>1.664072</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.901990</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.913333</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.913333</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.091333</td>\n",
       "      <td>0.009700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.048300</td>\n",
       "      <td>1.713210</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.902539</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.913333</td>\n",
       "      <td>0.976667</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.913333</td>\n",
       "      <td>0.976667</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.091333</td>\n",
       "      <td>0.009767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.035500</td>\n",
       "      <td>1.674879</td>\n",
       "      <td>0.903333</td>\n",
       "      <td>0.908031</td>\n",
       "      <td>0.903333</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.976667</td>\n",
       "      <td>0.903333</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.976667</td>\n",
       "      <td>0.903333</td>\n",
       "      <td>0.091667</td>\n",
       "      <td>0.009767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.042200</td>\n",
       "      <td>1.766106</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.899664</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.091667</td>\n",
       "      <td>0.009733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.046700</td>\n",
       "      <td>1.753112</td>\n",
       "      <td>0.896667</td>\n",
       "      <td>0.901920</td>\n",
       "      <td>0.896667</td>\n",
       "      <td>0.913333</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.896667</td>\n",
       "      <td>0.913333</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.896667</td>\n",
       "      <td>0.091333</td>\n",
       "      <td>0.009667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.049000</td>\n",
       "      <td>1.791327</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.896892</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.091000</td>\n",
       "      <td>0.009700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.072000</td>\n",
       "      <td>1.761824</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.900078</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.913333</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.913333</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.091333</td>\n",
       "      <td>0.009733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.079600</td>\n",
       "      <td>1.818500</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.892073</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.009700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.117500</td>\n",
       "      <td>1.823001</td>\n",
       "      <td>0.873333</td>\n",
       "      <td>0.887455</td>\n",
       "      <td>0.873333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.873333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.873333</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.009733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.143300</td>\n",
       "      <td>1.838456</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>0.888006</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.963333</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.963333</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>0.090667</td>\n",
       "      <td>0.009633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.130300</td>\n",
       "      <td>1.832847</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.891521</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.963333</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.963333</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.090667</td>\n",
       "      <td>0.009633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.152900</td>\n",
       "      <td>1.858312</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.883423</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.009600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.120700</td>\n",
       "      <td>1.946193</td>\n",
       "      <td>0.843333</td>\n",
       "      <td>0.868581</td>\n",
       "      <td>0.843333</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.843333</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.843333</td>\n",
       "      <td>0.090667</td>\n",
       "      <td>0.009600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.142800</td>\n",
       "      <td>1.819707</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.883036</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.903333</td>\n",
       "      <td>0.963333</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.903333</td>\n",
       "      <td>0.963333</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.090333</td>\n",
       "      <td>0.009633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.103400</td>\n",
       "      <td>1.889261</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.881590</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.009400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.094500</td>\n",
       "      <td>1.880822</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.883066</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.903333</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.903333</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.090333</td>\n",
       "      <td>0.009467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.086700</td>\n",
       "      <td>1.870833</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.892690</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.009467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.081900</td>\n",
       "      <td>1.828146</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.885263</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.009467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.104700</td>\n",
       "      <td>1.846970</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.878883</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.009533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.083500</td>\n",
       "      <td>1.831897</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.881750</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.896667</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.896667</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.089667</td>\n",
       "      <td>0.009533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.069000</td>\n",
       "      <td>1.790875</td>\n",
       "      <td>0.873333</td>\n",
       "      <td>0.887243</td>\n",
       "      <td>0.873333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.963333</td>\n",
       "      <td>0.873333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.963333</td>\n",
       "      <td>0.873333</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.009633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.057600</td>\n",
       "      <td>1.830344</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>0.888361</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>0.896667</td>\n",
       "      <td>0.963333</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>0.896667</td>\n",
       "      <td>0.963333</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>0.089667</td>\n",
       "      <td>0.009633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.053800</td>\n",
       "      <td>1.759528</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>0.887469</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>0.903333</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>0.903333</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>0.090333</td>\n",
       "      <td>0.009733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.038000</td>\n",
       "      <td>1.767298</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.891348</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.903333</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.903333</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.090333</td>\n",
       "      <td>0.009800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.036900</td>\n",
       "      <td>1.740147</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>0.894917</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>0.903333</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>0.903333</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>0.090333</td>\n",
       "      <td>0.009667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.024400</td>\n",
       "      <td>1.780342</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>0.894344</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>0.903333</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>0.903333</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>0.090333</td>\n",
       "      <td>0.009667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.013700</td>\n",
       "      <td>1.702657</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.890707</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.903333</td>\n",
       "      <td>0.976667</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.903333</td>\n",
       "      <td>0.976667</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.090333</td>\n",
       "      <td>0.009767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.018800</td>\n",
       "      <td>1.735764</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.891799</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.009733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.018500</td>\n",
       "      <td>1.666069</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.895202</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.009700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.003700</td>\n",
       "      <td>1.710119</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.897428</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.009667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.992400</td>\n",
       "      <td>1.685600</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.897502</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.009667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.990200</td>\n",
       "      <td>1.697871</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.897034</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.009667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.987600</td>\n",
       "      <td>1.689545</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.897044</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.009667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.988300</td>\n",
       "      <td>1.667292</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.896428</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.009667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.975800</td>\n",
       "      <td>1.700587</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.895902</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.009667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.970500</td>\n",
       "      <td>1.686174</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.895627</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.009667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.970400</td>\n",
       "      <td>1.680286</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.897552</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.009667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.969200</td>\n",
       "      <td>1.686450</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.898121</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.009667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.967100</td>\n",
       "      <td>1.685852</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.898126</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.009667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.964600</td>\n",
       "      <td>1.684616</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.898127</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.009667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.964200</td>\n",
       "      <td>1.685421</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.898126</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.009667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.965100</td>\n",
       "      <td>1.685549</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.898126</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.009667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.964800</td>\n",
       "      <td>1.685542</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.898129</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.009667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Dataset Test Metrics: {\n",
      "    \"eval_loss\": 1.679082989692688,\n",
      "    \"eval_accuracy\": 0.89,\n",
      "    \"eval_mAP\": 0.9002551528796817,\n",
      "    \"eval_Hint@1\": 0.89,\n",
      "    \"eval_Hint@10\": 0.9166666666666666,\n",
      "    \"eval_Hint@100\": 0.98,\n",
      "    \"eval_recall@1\": 0.89,\n",
      "    \"eval_recall@10\": 0.9166666666666666,\n",
      "    \"eval_recall@100\": 0.98,\n",
      "    \"eval_precision@1\": 0.89,\n",
      "    \"eval_precision@10\": 0.09166666666666666,\n",
      "    \"eval_precision@100\": 0.0098,\n",
      "    \"eval_runtime\": 0.5437,\n",
      "    \"eval_samples_per_second\": 551.732,\n",
      "    \"eval_steps_per_second\": 18.391,\n",
      "    \"epoch\": 10.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from json import dumps\n",
    "sep = \"#\" * 10\n",
    "for ci in range(CYCLES):\n",
    "    print(f\"{sep}Starting training cycle {ci + 1}{sep}\")\n",
    "    trainer_indexing.train()\n",
    "    metrics_indexing = trainer_indexing.evaluate(eval_dataset=tokenized_datasets_query[\"test\"])\n",
    "    print(f\"Indexing Dataset Test Metrics: {dumps(metrics_indexing, indent=4)}\")\n",
    "    trainer_query.train()\n",
    "    metrics_query = trainer_query.evaluate(eval_dataset=tokenized_datasets_query[\"test\"])\n",
    "    print(f\"Query Dataset Test Metrics: {dumps(metrics_query, indent=4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf27958a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on test set for query dataset...\n",
      "Query Dataset Test Metrics: {\n",
      "    \"eval_loss\": 1.3098227977752686,\n",
      "    \"eval_accuracy\": 0.8266666666666667,\n",
      "    \"eval_mAP\": 0.8520377084375034,\n",
      "    \"eval_Hint@1\": 0.8266666666666667,\n",
      "    \"eval_Hint@10\": 0.89,\n",
      "    \"eval_Hint@100\": 0.9166666666666666,\n",
      "    \"eval_recall@1\": 0.8266666666666667,\n",
      "    \"eval_recall@10\": 0.89,\n",
      "    \"eval_recall@100\": 0.9166666666666666,\n",
      "    \"eval_precision@1\": 0.8266666666666667,\n",
      "    \"eval_precision@10\": 0.089,\n",
      "    \"eval_precision@100\": 0.009166666666666667,\n",
      "    \"eval_runtime\": 0.5565,\n",
      "    \"eval_samples_per_second\": 539.063,\n",
      "    \"eval_steps_per_second\": 17.969,\n",
      "    \"epoch\": 10.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# test final\n",
    "from json import dumps\n",
    "print(\"Evaluating on test set for query dataset...\")\n",
    "metrics_query = trainer_query.evaluate(eval_dataset=tokenized_datasets_query[\"test\"])\n",
    "print(f\"Query Dataset Test Metrics: {dumps(metrics_query, indent=4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3988ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on test set for query dataset...\n",
      "Sample query: ¿Cómo puedo contactar con Hugo Castro?\n",
      "Real label for the first test query: 7557_4\n",
      "Predicted label for query '¿Cómo puedo contactar con Hugo Castro?': 7557_4\n",
      "Top 20 predictions:\n",
      "Label: 7557_4, Probability: 0.9956\n",
      "Label: 7542_3, Probability: 0.0013\n",
      "Label: 7579_1, Probability: 0.0008\n",
      "Label: 7517_5, Probability: 0.0002\n",
      "Label: 7513_3, Probability: 0.0002\n",
      "Label: 7502_4, Probability: 0.0001\n",
      "Label: 7550_2, Probability: 0.0001\n",
      "Label: 7568_1, Probability: 0.0001\n",
      "Label: 7552_5, Probability: 0.0001\n",
      "Label: 7511_5, Probability: 0.0001\n",
      "Label: 7503_5, Probability: 0.0001\n",
      "Label: 7540_2, Probability: 0.0001\n",
      "Label: 7534_3, Probability: 0.0001\n",
      "Label: 7504_4, Probability: 0.0001\n",
      "Label: 7515_5, Probability: 0.0001\n",
      "Label: 7539_4, Probability: 0.0000\n",
      "Label: 7519_3, Probability: 0.0000\n",
      "Label: 7546_5, Probability: 0.0000\n",
      "Label: 7552_4, Probability: 0.0000\n",
      "Label: 7538_5, Probability: 0.0000\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# testea la salida de una pregunta, me refiero a obtener la predicción\n",
    "\n",
    "print(\"Evaluating on test set for query dataset...\")\n",
    "index = 100\n",
    "query = tokenized_datasets_query[\"test\"][index][\"text\"]\n",
    "real_label_id = tokenized_datasets_query[\"test\"][index][\"label\"]\n",
    "id_to_label = {v: k for k, v in label_to_id.items()}\n",
    "real_label = id_to_label[real_label_id]\n",
    "print(f\"Sample query: {query}\")\n",
    "print(f\"Real label for the first test query: {real_label}\")\n",
    "inputs = tokenizer(query, return_tensors=\"pt\", truncation=True, max_length=MAX_LENGTH)\n",
    "inputs = {k: v.to(trainer_query.model.device) for k, v in inputs.items()}\n",
    "with torch.no_grad():\n",
    "    outputs = trainer_query.model(**inputs)\n",
    "logits = outputs.logits\n",
    "predicted_class_id = logits.argmax().item()\n",
    "id_to_label = {v: k for k, v in label_to_id.items()}\n",
    "predicted_label = id_to_label[predicted_class_id]\n",
    "print(f\"Predicted label for query '{query}': {predicted_label}\")\n",
    "# extra los top k prediciones con mayor probabilidad\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "top_k = 20\n",
    "top_k_probs, top_k_indices = torch.topk(probs, top_k)\n",
    "print(f\"Top {top_k} predictions:\")\n",
    "for prob, idx in zip(top_k_probs[0], top_k_indices[0]):\n",
    "    label = id_to_label[idx.item()]\n",
    "    print(f\"Label: {label}, Probability: {prob.item():.4f}\")\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea4cb616",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:21<00:00, 14.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# me gustaria que me hicieras un bucle con la logica de la celda anterior para ver si no acierta alguna muestra me diga que posicion esta en la lista de logits\n",
    "from tqdm import tqdm\n",
    "\n",
    "labels_predicted = []\n",
    "labels_real = []\n",
    "\n",
    "for index in tqdm(range(len(tokenized_datasets_query[\"test\"]))):\n",
    "    query = tokenized_datasets_query[\"test\"][index][\"text\"]\n",
    "    real_label_id = tokenized_datasets_query[\"test\"][index][\"label\"]\n",
    "    id_to_label = {v: k for k, v in label_to_id.items()}\n",
    "    real_label = id_to_label[real_label_id]\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\", truncation=True, max_length=MAX_LENGTH)\n",
    "    inputs = {k: v.to(trainer_query.model.device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = trainer_query.model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    # get list of labels sorted by probability\n",
    "    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "    sorted_labels = [id_to_label[idx.item()] for idx in sorted_indices[0]]\n",
    "\n",
    "    labels_predicted.append(sorted_labels)\n",
    "    labels_real.append([real_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6baf6c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking Metrics:\n",
      "  MRR: 0.9003\n",
      "  mAP: 0.9003\n",
      "  AvgRank: 9.2100\n",
      "  CMC@1: 0.8900\n",
      "  Recall@k (macro)@1: 0.8900\n",
      "  Precision@k (macro)@1: 0.8900\n",
      "  Accuracy@1: 0.8900\n",
      "  F1@k (macro)@1: 0.8900\n",
      "  CMC@5: 0.9100\n",
      "  Recall@k (macro)@5: 0.9100\n",
      "  Precision@k (macro)@5: 0.1820\n",
      "  Accuracy@5: 0.9100\n",
      "  F1@k (macro)@5: 0.3033\n",
      "  CMC@10: 0.9167\n",
      "  Recall@k (macro)@10: 0.9167\n",
      "  Precision@k (macro)@10: 0.0917\n",
      "  Accuracy@10: 0.9167\n",
      "  F1@k (macro)@10: 0.1667\n",
      "  CMC@20: 0.9167\n",
      "  Recall@k (macro)@20: 0.9167\n",
      "  Precision@k (macro)@20: 0.0458\n",
      "  Accuracy@20: 0.9167\n",
      "  F1@k (macro)@20: 0.0873\n",
      "  CMC@100: 0.9800\n",
      "  Recall@k (macro)@100: 0.9800\n",
      "  Precision@k (macro)@100: 0.0098\n",
      "  Accuracy@100: 0.9800\n",
      "  F1@k (macro)@100: 0.0194\n"
     ]
    }
   ],
   "source": [
    "from ranking_metrics import calc_ranking_metrics\n",
    "\n",
    "metrics = calc_ranking_metrics(labels_predicted, labels_real, ks=[1, 5, 10, 20, 100], one_relevant_per_query=True)\n",
    "\n",
    "print(\"Ranking Metrics:\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d03f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
