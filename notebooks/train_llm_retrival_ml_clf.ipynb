{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02911839",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miguel/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse, os, numpy as np, torch\n",
    "from datasets import load_dataset, ClassLabel\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoConfig, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer, DataCollatorWithPadding\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from transformers import BitsAndBytesConfig\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee5bd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 45092.77 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 400\n",
      "Labels: ['7500_1', '7500_2', '7500_3', '7500_4', '7500_5', '7501_1', '7501_2', '7501_3', '7501_4', '7501_5', '7502_1', '7502_2', '7502_3', '7502_4', '7502_5', '7503_1', '7503_2', '7503_3', '7503_4', '7503_5', '7504_1', '7504_2', '7504_3', '7504_4', '7504_5', '7505_1', '7505_2', '7505_3', '7505_4', '7505_5', '7506_1', '7506_2', '7506_3', '7506_4', '7506_5', '7507_1', '7507_2', '7507_3', '7507_4', '7507_5', '7508_1', '7508_2', '7508_3', '7508_4', '7508_5', '7509_1', '7509_2', '7509_3', '7509_4', '7509_5', '7510_1', '7510_2', '7510_3', '7510_4', '7510_5', '7511_1', '7511_2', '7511_3', '7511_4', '7511_5', '7512_1', '7512_2', '7512_3', '7512_4', '7512_5', '7513_1', '7513_2', '7513_3', '7513_4', '7513_5', '7514_1', '7514_2', '7514_3', '7514_4', '7514_5', '7515_1', '7515_2', '7515_3', '7515_4', '7515_5', '7516_1', '7516_2', '7516_3', '7516_4', '7516_5', '7517_1', '7517_2', '7517_3', '7517_4', '7517_5', '7518_1', '7518_2', '7518_3', '7518_4', '7518_5', '7519_1', '7519_2', '7519_3', '7519_4', '7519_5', '7520_1', '7520_2', '7520_3', '7520_4', '7520_5', '7521_1', '7521_2', '7521_3', '7521_4', '7521_5', '7522_1', '7522_2', '7522_3', '7522_4', '7522_5', '7523_1', '7523_2', '7523_3', '7523_4', '7523_5', '7524_1', '7524_2', '7524_3', '7524_4', '7524_5', '7525_1', '7525_2', '7525_3', '7525_4', '7525_5', '7526_1', '7526_2', '7526_3', '7526_4', '7526_5', '7527_1', '7527_2', '7527_3', '7527_4', '7527_5', '7528_1', '7528_2', '7528_3', '7528_4', '7528_5', '7529_1', '7529_2', '7529_3', '7529_4', '7529_5', '7530_1', '7530_2', '7530_3', '7530_4', '7530_5', '7531_1', '7531_2', '7531_3', '7531_4', '7531_5', '7532_1', '7532_2', '7532_3', '7532_4', '7532_5', '7533_1', '7533_2', '7533_3', '7533_4', '7533_5', '7534_1', '7534_2', '7534_3', '7534_4', '7534_5', '7535_1', '7535_2', '7535_3', '7535_4', '7535_5', '7536_1', '7536_2', '7536_3', '7536_4', '7536_5', '7537_1', '7537_2', '7537_3', '7537_4', '7537_5', '7538_1', '7538_2', '7538_3', '7538_4', '7538_5', '7539_1', '7539_2', '7539_3', '7539_4', '7539_5', '7540_1', '7540_2', '7540_3', '7540_4', '7540_5', '7541_1', '7541_2', '7541_3', '7541_4', '7541_5', '7542_1', '7542_2', '7542_3', '7542_4', '7542_5', '7543_1', '7543_2', '7543_3', '7543_4', '7543_5', '7544_1', '7544_2', '7544_3', '7544_4', '7544_5', '7545_1', '7545_2', '7545_3', '7545_4', '7545_5', '7546_1', '7546_2', '7546_3', '7546_4', '7546_5', '7547_1', '7547_2', '7547_3', '7547_4', '7547_5', '7548_1', '7548_2', '7548_3', '7548_4', '7548_5', '7549_1', '7549_2', '7549_3', '7549_4', '7549_5', '7550_1', '7550_2', '7550_3', '7550_4', '7550_5', '7551_1', '7551_2', '7551_3', '7551_4', '7551_5', '7552_1', '7552_2', '7552_3', '7552_4', '7552_5', '7553_1', '7553_2', '7553_3', '7553_4', '7553_5', '7554_1', '7554_2', '7554_3', '7554_4', '7554_5', '7555_1', '7555_2', '7555_3', '7555_4', '7555_5', '7556_1', '7556_2', '7556_3', '7556_4', '7556_5', '7557_1', '7557_2', '7557_3', '7557_4', '7557_5', '7558_1', '7558_2', '7558_3', '7558_4', '7558_5', '7559_1', '7559_2', '7559_3', '7559_4', '7559_5', '7560_1', '7560_2', '7560_3', '7560_4', '7560_5', '7561_1', '7561_2', '7561_3', '7561_4', '7561_5', '7562_1', '7562_2', '7562_3', '7562_4', '7562_5', '7563_1', '7563_2', '7563_3', '7563_4', '7563_5', '7564_1', '7564_2', '7564_3', '7564_4', '7564_5', '7565_1', '7565_2', '7565_3', '7565_4', '7565_5', '7566_1', '7566_2', '7566_3', '7566_4', '7566_5', '7567_1', '7567_2', '7567_3', '7567_4', '7567_5', '7568_1', '7568_2', '7568_3', '7568_4', '7568_5', '7569_1', '7569_2', '7569_3', '7569_4', '7569_5', '7570_1', '7570_2', '7570_3', '7570_4', '7570_5', '7571_1', '7571_2', '7571_3', '7571_4', '7571_5', '7572_1', '7572_2', '7572_3', '7572_4', '7572_5', '7573_1', '7573_2', '7573_3', '7573_4', '7573_5', '7574_1', '7574_2', '7574_3', '7574_4', '7574_5', '7575_1', '7575_2', '7575_3', '7575_4', '7575_5', '7576_1', '7576_2', '7576_3', '7576_4', '7576_5', '7577_1', '7577_2', '7577_3', '7577_4', '7577_5', '7578_1', '7578_2', '7578_3', '7578_4', '7578_5', '7579_1', '7579_2', '7579_3', '7579_4', '7579_5']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 43054.93 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_indexing_csv = pd.read_csv(\"../notebooks/data/contacts_docs.csv\")\n",
    "\n",
    "# to dataset huggingface\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset_indexing = Dataset.from_pandas(dataset_indexing_csv)\n",
    "\n",
    "# create a new column 'text' that concatenates 'name', 'phone'\n",
    "def concatenate_columns(example):\n",
    "    return {\n",
    "        \"text\": f\"Nombre: {example['name']}\\nTeléfono: {example['phone']}\"\n",
    "    }\n",
    "dataset_indexing = dataset_indexing.map(concatenate_columns)\n",
    "# rename column 'id' to 'label'\n",
    "dataset_indexing = dataset_indexing.rename_column(\"id\", \"label\")\n",
    "\n",
    "num_labels = len(dataset_indexing['label'])\n",
    "print(f\"Number of labels: {num_labels}\")\n",
    "labels_list = dataset_indexing.unique('label')\n",
    "print(f\"Labels: {labels_list}\")\n",
    "\n",
    "# map labels to integers\n",
    "label_to_id = {label: i for i, label in enumerate(labels_list)}\n",
    "def map_labels(example):\n",
    "    return {\n",
    "        \"label\": label_to_id[example['label']]\n",
    "    }\n",
    "dataset_indexing = dataset_indexing.map(map_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df77002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_dataset_train = pd.read_csv(\"../notebooks/data/contacts_queries_train.csv\")\n",
    "query_dataset_val = pd.read_csv(\"../notebooks/data/contacts_queries_val.csv\")\n",
    "query_dataset_test = pd.read_csv(\"../notebooks/data/contacts_queries_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88962b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1400/1400 [00:00<00:00, 59512.37 examples/s]\n",
      "Map: 100%|██████████| 300/300 [00:00<00:00, 25925.97 examples/s]\n",
      "Map: 100%|██████████| 300/300 [00:00<00:00, 53094.70 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '¿Cómo puedo contactar con Antonio Alonso?', 'label': 194}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_for_queries = {\n",
    "    \"train\": Dataset.from_pandas(query_dataset_train),\n",
    "    \"validation\": Dataset.from_pandas(query_dataset_val),\n",
    "    \"test\": Dataset.from_pandas(query_dataset_test)\n",
    "}\n",
    "\n",
    "for split in dataset_for_queries:\n",
    "    dataset_for_queries[split] = dataset_for_queries[split].rename_column(\"question\", \"text\")\n",
    "    dataset_for_queries[split] = dataset_for_queries[split].rename_column(\"id\", \"label\")\n",
    "    # map labels to integers\n",
    "    dataset_for_queries[split] = dataset_for_queries[split].map(map_labels)\n",
    "\n",
    "# to dataset huggingface\n",
    "from datasets import DatasetDict\n",
    "dataset_for_queries = DatasetDict(dataset_for_queries)\n",
    "print(dataset_for_queries[\"train\"][1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1107a838",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19767e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f1a9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-0.6B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b372807",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_r = 256\n",
    "lora_alpha = lora_r * 2\n",
    "lora_dropout = 0.0\n",
    "lora_bias = \"none\"\n",
    "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"down_proj\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23ba430",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    inference_mode=False,\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=lora_bias,\n",
    "    target_modules=target_modules\n",
    ")\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c43bc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.base_model.model.score.parameters():\n",
    "    p.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579c201d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 43406.94 examples/s]\n",
      "Map: 100%|██████████| 1400/1400 [00:00<00:00, 71951.40 examples/s]\n",
      "Map: 100%|██████████| 300/300 [00:00<00:00, 48215.93 examples/s]\n",
      "Map: 100%|██████████| 300/300 [00:00<00:00, 54344.44 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=MAX_LENGTH, padding=False)\n",
    "\n",
    "# tokenize test from dataset\n",
    "tokenized_datasets_indexing = dataset_indexing.map(preprocess, batched=True)\n",
    "tokenized_datasets_query = dataset_for_queries.map(preprocess, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2db441",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = (preds == labels).mean()\n",
    "    return {\"accuracy\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3585bfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento\n",
    "SEED = 42\n",
    "training_args_indexing = TrainingArguments(\n",
    "    output_dir=f\"models/contacts_clf_{model_name.replace('/', '_')}_indexing\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",  \n",
    "    eval_steps=10,\n",
    "    save_steps=10,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "training_args_query = TrainingArguments(\n",
    "    output_dir=f\"models/contacts_clf_{model_name.replace('/', '_')}_query\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",  \n",
    "    eval_steps=10,\n",
    "    save_steps=10,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "# IDs de tokens\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.bos_token_id = tokenizer.bos_token_id\n",
    "\n",
    "trainer_indexing = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_indexing,\n",
    "    train_dataset=tokenized_datasets_indexing,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer_query = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_query,\n",
    "    train_dataset=tokenized_datasets_query[\"train\"],\n",
    "    eval_dataset=tokenized_datasets_query[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044ea9a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='260' max='260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [260/260 01:18, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>7.142000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>6.264900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>6.139100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>6.065100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.947300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>5.554800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>4.853900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.566000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.242400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.257400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.071000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.016000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.006400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='311' max='440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [311/440 01:53 < 00:47, 2.71 it/s, Epoch 7.05/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>6.429800</td>\n",
       "      <td>5.777628</td>\n",
       "      <td>0.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5.027400</td>\n",
       "      <td>4.590471</td>\n",
       "      <td>0.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>4.040200</td>\n",
       "      <td>4.295148</td>\n",
       "      <td>0.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.032700</td>\n",
       "      <td>3.883223</td>\n",
       "      <td>0.323333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.083800</td>\n",
       "      <td>3.697811</td>\n",
       "      <td>0.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.761400</td>\n",
       "      <td>3.644974</td>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.763100</td>\n",
       "      <td>3.176554</td>\n",
       "      <td>0.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.741400</td>\n",
       "      <td>2.926815</td>\n",
       "      <td>0.483333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.998100</td>\n",
       "      <td>2.770937</td>\n",
       "      <td>0.503333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.080000</td>\n",
       "      <td>2.669616</td>\n",
       "      <td>0.553333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.047100</td>\n",
       "      <td>2.425822</td>\n",
       "      <td>0.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.795200</td>\n",
       "      <td>2.246543</td>\n",
       "      <td>0.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.780500</td>\n",
       "      <td>2.020119</td>\n",
       "      <td>0.673333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.438300</td>\n",
       "      <td>1.911532</td>\n",
       "      <td>0.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.200300</td>\n",
       "      <td>1.762261</td>\n",
       "      <td>0.693333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.229100</td>\n",
       "      <td>1.781374</td>\n",
       "      <td>0.706667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.246300</td>\n",
       "      <td>1.680032</td>\n",
       "      <td>0.723333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.143500</td>\n",
       "      <td>1.653084</td>\n",
       "      <td>0.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.064800</td>\n",
       "      <td>1.584628</td>\n",
       "      <td>0.746667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.041600</td>\n",
       "      <td>1.524203</td>\n",
       "      <td>0.783333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.068500</td>\n",
       "      <td>1.465057</td>\n",
       "      <td>0.813333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.055100</td>\n",
       "      <td>1.472940</td>\n",
       "      <td>0.816667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>1.459963</td>\n",
       "      <td>0.823333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>1.398092</td>\n",
       "      <td>0.826667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>1.374674</td>\n",
       "      <td>0.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>1.357482</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.330648</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>1.320909</td>\n",
       "      <td>0.843333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>1.317185</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>1.301732</td>\n",
       "      <td>0.843333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.298919</td>\n",
       "      <td>0.843333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:664] . unexpected pos 680704576 vs 680704464",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:967\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 967\u001b[0m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:1268\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[0;32m-> 1268\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:858] . PytorchStreamWriter failed writing file data/650: file write failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(CICLES):\n\u001b[1;32m      3\u001b[0m     trainer_indexing\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mtrainer_query\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2328\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2326\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2754\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2753\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2754\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2755\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2756\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2757\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2758\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2762\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2763\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2764\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2765\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:3234\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[1;32m   3231\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save \u001b[38;5;241m=\u001b[39m is_new_best_metric\n\u001b[1;32m   3233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 3234\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3235\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:3342\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial)\u001b[0m\n\u001b[1;32m   3338\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbest_model_checkpoint \u001b[38;5;241m=\u001b[39m best_checkpoint_dir\n\u001b[1;32m   3340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_only_model:\n\u001b[1;32m   3341\u001b[0m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[0;32m-> 3342\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_optimizer_and_scheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3343\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_scaler(output_dir)\n\u001b[1;32m   3344\u001b[0m     \u001b[38;5;66;03m# Save RNG state\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:3469\u001b[0m, in \u001b[0;36mTrainer._save_optimizer_and_scheduler\u001b[0;34m(self, output_dir)\u001b[0m\n\u001b[1;32m   3464\u001b[0m     save_fsdp_optimizer(\n\u001b[1;32m   3465\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfsdp_plugin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, output_dir\n\u001b[1;32m   3466\u001b[0m     )\n\u001b[1;32m   3467\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m   3468\u001b[0m     \u001b[38;5;66;03m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[39;00m\n\u001b[0;32m-> 3469\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOPTIMIZER_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3471\u001b[0m \u001b[38;5;66;03m# Save SCHEDULER & SCALER\u001b[39;00m\n\u001b[1;32m   3472\u001b[0m is_deepspeed_custom_scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_deepspeed_enabled \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   3473\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler, DeepSpeedSchedulerWrapper\n\u001b[1;32m   3474\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:966\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    963\u001b[0m     f \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(f)\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 966\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    967\u001b[0m         _save(\n\u001b[1;32m    968\u001b[0m             obj,\n\u001b[1;32m    969\u001b[0m             opened_zipfile,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    972\u001b[0m             _disable_byteorder_record,\n\u001b[1;32m    973\u001b[0m         )\n\u001b[1;32m    974\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:798\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 798\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:664] . unexpected pos 680704576 vs 680704464"
     ]
    }
   ],
   "source": [
    "CICLES = 1\n",
    "for _ in range(CICLES):\n",
    "    trainer_indexing.train()\n",
    "    trainer_query.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf27958a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
