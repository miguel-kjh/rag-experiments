{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "473b050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_from_disk\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import torch\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c9d8b356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.9.8: Fast Qwen3 patching. Transformers: 4.56.2. vLLM: 0.10.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.988 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = 8192,\n",
    "    load_in_4bit = False,\n",
    "    load_in_8bit = False,\n",
    ")\n",
    "RANK = 128\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = RANK,           # Choose any number > 0! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = RANK*2,  # Best to choose alpha = rank or rank*2\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = SEED,\n",
    "    use_rslora = False,   # We support rank stabilized LoRA\n",
    "    loftq_config = None,  # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5b956a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    ")\n",
    "\n",
    "db = FAISS.load_local(\n",
    "    \"../data/db/parliament_db/parliament_all_docs_embeddings_sentence-transformers_paraphrase-multilingual-mpnet-base-v2\",\n",
    "    embedding_model,\n",
    "    allow_dangerous_deserialization=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "08fbfd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 11162\n"
     ]
    }
   ],
   "source": [
    "#quiero la lista de documentos\n",
    "docs = db.docstore._dict.values()\n",
    "documents = list(docs)\n",
    "print(f\"Number of documents: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cbda5e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_AUTORE = \"../data/processed/parliament_qa\"\n",
    "dataset = load_from_disk(FOLDER_AUTORE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1efe551",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5126e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompt_for_indexing(documents: list):\n",
    "    prompt = \"\"\"\n",
    "    Este documento tiene el DOCID:{doc_id}.\n",
    "    Contenido del documento:\n",
    "    {doc}\n",
    "    \"\"\"\n",
    "    for doc in documents:\n",
    "        document = doc.page_content\n",
    "        doc_id = doc.metadata.get(\"id\", \"unknown\")\n",
    "        yield prompt.format(doc=document, doc_id=doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e1f36a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_it(tokenizer, system_prompt: str, prompt: str, response: str) -> str:\n",
    "    \"\"\"Builds the chat prompt for a single example using the tokenizer chat template.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\",   \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": response}\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3d4d4258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompts_for_retrieval(dataset, tokenizer):\n",
    "    system_prompt = \"\"\"Eres un módulo de recuperación. Tu única tarea es devolver el identificador del documento correspondiente a la consulta dada.\n",
    "Sigue estrictamente estas reglas:\n",
    "1) Devuelve EXACTAMENTE una línea con el formato: DOCID:{<id>}.\n",
    "2) No incluyas palabras, explicaciones o puntuación extra antes o después de las llaves.\n",
    "3) Si múltiples documentos son plausibles, elige el mejor ID.\n",
    "4) Nunca inventes un ID fuera del espacio permitido. Mantente dentro de los prefijos válidos.\n",
    "5) No respondas a la pregunta; solo devuelve el docid.\"\n",
    "\"\"\"\n",
    "    prompts = []\n",
    "    for item in dataset:\n",
    "        prompt = \"\"\"\n",
    "        Dada la siguiente consulta, recupera los identificadores de los documentos relevantes. \n",
    "        Consulta: {QUERY}\n",
    "        \"\"\"\n",
    "        response = \"DOCID:{docid}\"\n",
    "        question = item[\"question\"]\n",
    "        prompt = prompt.format(QUERY=question)\n",
    "        prompts.append(build_prompt_it(tokenizer, system_prompt, prompt, response.format(docid=item[\"id\"])))\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b744a024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prompts: 11162\n"
     ]
    }
   ],
   "source": [
    "prompts = list(prepare_prompt_for_indexing(documents))\n",
    "print(f\"Number of prompts: {len(prompts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a7b9ed38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Este documento tiene el DOCID:6596_4.\\n    Contenido del documento:\\n    Esta sesión del parlamento se realizó el 2024-05-07. 11L/PO/P-0750 PREGUNTA DEL SEÑOR DIPUTADO DON NICASIO JESÚS GALVÁN SASIA, DEL GRUPO PARLAMENTARIO VOX, SOBRE MEDIDAS QUE SE VAN A LLEVAR A CABO PARA DEMOCRATIZAR Y REDISTRIBUIR LA RIQUEZA DEL SECTOR TURÍSTICO, DIRIGIDA A LA PRESIDENCIA DEL GOBIERNO La señora PRESIDENTA: Siguiente pregunta, del señor diputado don Nicasio Galván Sasia, del Grupo Parlamentario VOX, sobre medidas que se van a llevar a cabo para democratizar y redistribuir la riqueza del sector turístico, dirigida al señor presidente del Gobierno. Cuando quiera. El señor GALVÁN SASIA (desde su escaño): Buenos días, señor Clavijo, buenos días. Escuchándole en la rueda de prensa posterior a la Conferencia de Presidentes nos han surgido varias preguntas, y nos consta que no solo a nosotros. Se le oía escuchar hablar de la democratización y la redistribución de la riqueza del sector turístico y culpabilizó al turismo -sí, a esas 500 000 personas que han venido a vivir a Canarias en los últimos veinte años- de la presión de los servicios públicos y de la demanda de vivienda. No le escuchamos hablar de esas 200 000 viviendas que hay vacías en nuestras islas. Habló también del reto demográfico, pero desde un punto de vista neomalthusiano, de escasez y agotamiento de los recursos por causa del incremento de la población. Esas tesis, señor Clavijo, se han demostrado erróneas, porque no han tenido en cuenta la capacidad de inventiva ni innovación del ser humano. Desde nuestro punto de vista, se demuestra se demuestra que no hay agenda clara de gobierno, que parece que ustedes pretenden gobernar a base de titular de prensa y como reacción a las manifestaciones. Les recuerdo, también, que desde la tribuna de este Parlamento en el pasado pleno su grupo parlamentario faltó al respeto a las universidades canarias y señaló directamente a una empresa privada, yo creo que atacando al sector económico más importante de Canarias, a nuestras universidades y a las empresas privadas, no vamos a lograr incrementar el producto interior bruto canario ni a lograr subir el salario medio ni tampoco mejorar la investigación, desarrollo e innovación tecnológica, tampoco enviaremos un mensaje sobre seguridad jurídica y, por supuesto, no atraeremos inversión nacional e internacional. Recordarle también que el año pasado se incrementó la burocracia en Canarias cerca de un catorce por ciento. Señor Clavijo, así no. Nosotros creemos que así no se van a revertir esos datos de pobreza y de desempleo que tenemos en Canarias, no se van a generar ese ahorro y esa inversión que necesitamos para alcanzar la tan ansiada diversificación económica y productividad en nuestras islas. Señor Clavijo, comentarle que el lenguaje es muy importante a la hora de trasladar mensajes al mercado, sobre todo de un presidente de comunidad autónoma. Y créame si le digo, por favor, créame que hablar de democratizar y redistribuir la riqueza del sector turístico no ayuda, no ayuda, sino causa -recordemos que el sector turístico es iniciativa privada-, no ayuda, sino causa el efecto contrario de lo que se pretende, que espero que en este caso sea el crecimiento de Canarias y la riqueza de nuestras islas, por supuesto, respetando el medio ambiente. Muchas gracias. La señora PRESIDENTA: Señor presidente, tiene la palabra. El señor PRESIDENTE DE CANARIAS (Clavijo Batlle) (desde su escaño): Gracias, señora presidenta. Señor Galván, créame, créame que, desde luego, las tesis que salieron de la Conferencia de Presidentes desde luego no tiene nada que ver con el ser neomalthusianos o..., porque todo lo contrario, lo que se habla es de regular nuestros recursos naturales para que no se agoten y para que puedan disfrutarlos, ¿no?, porque el paisaje, la naturaleza, si la protegemos y la cuidamos, seguirán viniendo. Ahora, si hacemos un uso desmedido y agotamos esos recursos o los antropizamos, es decir, lo consumimos y lo transformamos por la acción del hombre, la realidad es que al final dejaremos de ser unas islas afortunadas y la gente se irá a otro sitio. Pero, mire, este Gobierno ha heredado una Canarias más injusta, más insolidaria, nunca antes los ricos en Canarias, después del pacto de las flores, han sido tan ricos y los pobres tan pobres. Y cuando hablamos de democratizar, cuando hablamos de distribuir la riqueza lo que este Gobierno quiere decir, y si lo ha llevado a confusión le pido disculpas, yo no soy un gran orador, pero sí lo que le puedo decir es que lo que buscamos es que de los 22 000 millones de euros de facturación del sector turístico se quede la mayor cantidad de dinero aquí. Hablamos de que las familias puedan llegar a final de mes, hablamos de mejorar la productividad, por supuesto que sí, porque en la medida que mejoremos la productividad seremos más competitivos, pero eso tiene que ir de la mano con las mejoras salariales, porque no nos resignamos a que en Canarias tengamos los salarios más bajos de toda España, no nos resignamos a que tengamos las cotas de exclusión social. Y para eso, y para eso hemos establecido esta Conferencia de Presidentes, para eso vamos a hacer los grupos de trabajo, para eso van a poder participar los sindicatos y los empresarios en las soluciones. Las soluciones, no tenemos una varita mágica, es más, somos muy respetuosos con la concertación social para que patronal y sindicatos se pongan de acuerdo en las retribuciones y en los convenios colectivos; ya se están dando pasos, ya, previsiblemente, antes de que acabe el año se va a sacar adelante un convenio en el sector turístico donde hay mejoras salariales. Eso es lo que perseguimos, ni somos un Gobierno intervencionista, porque ya se ha visto lo que ocurrió con el Gobierno anterior y las consecuencias, ni, desde luego, somos responsables de que la ley... porque votamos en contra de que la ley de vivienda aprobada en Madrid lo que haya hecho es sacar vivienda del mercado porque el propietario tiene miedo de ponerla a alquilar porque no tiene las garantías jurídicas. Créame que ese es el camino, no se confunda. Y, desde luego, en esto espero que nos ayude. La señora PRESIDENTA: Gracias, señor presidente. Tiempo de réplica, señor Galván, cuando quiera. El señor GALVÁN SASIA (desde su escaño): Señor Clavijo, pues le pido que nos invite a nosotros, a este grupo parlamentario, a participar en esas conversaciones de esos grupos, porque creo que se evitarían muchas malas interpretaciones, si es como usted dice, y, de todas maneras, controlaríamos mejor esa acción de gobierno. Muchas gracias. La señora PRESIDENTA: Gracias, señor Galván.\\n    '"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ed65b3b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 11162\n",
       "})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataset from prompts\n",
    "from datasets import Dataset\n",
    "indexing_dataset = Dataset.from_dict({\"text\": prompts})\n",
    "indexing_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cda96468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Este documento tiene el DOCID:6596_4.\\n    Contenido del documento:\\n    Esta sesión del parlamento se realizó el 2024-05-07. 11L/PO/P-0750 PREGUNTA DEL SEÑOR DIPUTADO DON NICASIO JESÚS GALVÁN SASIA, DEL GRUPO PARLAMENTARIO VOX, SOBRE MEDIDAS QUE SE VAN A LLEVAR A CABO PARA DEMOCRATIZAR Y REDISTRIBUIR LA RIQUEZA DEL SECTOR TURÍSTICO, DIRIGIDA A LA PRESIDENCIA DEL GOBIERNO La señora PRESIDENTA: Siguiente pregunta, del señor diputado don Nicasio Galván Sasia, del Grupo Parlamentario VOX, sobre medidas que se van a llevar a cabo para democratizar y redistribuir la riqueza del sector turístico, dirigida al señor presidente del Gobierno. Cuando quiera. El señor GALVÁN SASIA (desde su escaño): Buenos días, señor Clavijo, buenos días. Escuchándole en la rueda de prensa posterior a la Conferencia de Presidentes nos han surgido varias preguntas, y nos consta que no solo a nosotros. Se le oía escuchar hablar de la democratización y la redistribución de la riqueza del sector turístico y culpabilizó al turismo -sí, a esas 500 000 personas que han venido a vivir a Canarias en los últimos veinte años- de la presión de los servicios públicos y de la demanda de vivienda. No le escuchamos hablar de esas 200 000 viviendas que hay vacías en nuestras islas. Habló también del reto demográfico, pero desde un punto de vista neomalthusiano, de escasez y agotamiento de los recursos por causa del incremento de la población. Esas tesis, señor Clavijo, se han demostrado erróneas, porque no han tenido en cuenta la capacidad de inventiva ni innovación del ser humano. Desde nuestro punto de vista, se demuestra se demuestra que no hay agenda clara de gobierno, que parece que ustedes pretenden gobernar a base de titular de prensa y como reacción a las manifestaciones. Les recuerdo, también, que desde la tribuna de este Parlamento en el pasado pleno su grupo parlamentario faltó al respeto a las universidades canarias y señaló directamente a una empresa privada, yo creo que atacando al sector económico más importante de Canarias, a nuestras universidades y a las empresas privadas, no vamos a lograr incrementar el producto interior bruto canario ni a lograr subir el salario medio ni tampoco mejorar la investigación, desarrollo e innovación tecnológica, tampoco enviaremos un mensaje sobre seguridad jurídica y, por supuesto, no atraeremos inversión nacional e internacional. Recordarle también que el año pasado se incrementó la burocracia en Canarias cerca de un catorce por ciento. Señor Clavijo, así no. Nosotros creemos que así no se van a revertir esos datos de pobreza y de desempleo que tenemos en Canarias, no se van a generar ese ahorro y esa inversión que necesitamos para alcanzar la tan ansiada diversificación económica y productividad en nuestras islas. Señor Clavijo, comentarle que el lenguaje es muy importante a la hora de trasladar mensajes al mercado, sobre todo de un presidente de comunidad autónoma. Y créame si le digo, por favor, créame que hablar de democratizar y redistribuir la riqueza del sector turístico no ayuda, no ayuda, sino causa -recordemos que el sector turístico es iniciativa privada-, no ayuda, sino causa el efecto contrario de lo que se pretende, que espero que en este caso sea el crecimiento de Canarias y la riqueza de nuestras islas, por supuesto, respetando el medio ambiente. Muchas gracias. La señora PRESIDENTA: Señor presidente, tiene la palabra. El señor PRESIDENTE DE CANARIAS (Clavijo Batlle) (desde su escaño): Gracias, señora presidenta. Señor Galván, créame, créame que, desde luego, las tesis que salieron de la Conferencia de Presidentes desde luego no tiene nada que ver con el ser neomalthusianos o..., porque todo lo contrario, lo que se habla es de regular nuestros recursos naturales para que no se agoten y para que puedan disfrutarlos, ¿no?, porque el paisaje, la naturaleza, si la protegemos y la cuidamos, seguirán viniendo. Ahora, si hacemos un uso desmedido y agotamos esos recursos o los antropizamos, es decir, lo consumimos y lo transformamos por la acción del hombre, la realidad es que al final dejaremos de ser unas islas afortunadas y la gente se irá a otro sitio. Pero, mire, este Gobierno ha heredado una Canarias más injusta, más insolidaria, nunca antes los ricos en Canarias, después del pacto de las flores, han sido tan ricos y los pobres tan pobres. Y cuando hablamos de democratizar, cuando hablamos de distribuir la riqueza lo que este Gobierno quiere decir, y si lo ha llevado a confusión le pido disculpas, yo no soy un gran orador, pero sí lo que le puedo decir es que lo que buscamos es que de los 22 000 millones de euros de facturación del sector turístico se quede la mayor cantidad de dinero aquí. Hablamos de que las familias puedan llegar a final de mes, hablamos de mejorar la productividad, por supuesto que sí, porque en la medida que mejoremos la productividad seremos más competitivos, pero eso tiene que ir de la mano con las mejoras salariales, porque no nos resignamos a que en Canarias tengamos los salarios más bajos de toda España, no nos resignamos a que tengamos las cotas de exclusión social. Y para eso, y para eso hemos establecido esta Conferencia de Presidentes, para eso vamos a hacer los grupos de trabajo, para eso van a poder participar los sindicatos y los empresarios en las soluciones. Las soluciones, no tenemos una varita mágica, es más, somos muy respetuosos con la concertación social para que patronal y sindicatos se pongan de acuerdo en las retribuciones y en los convenios colectivos; ya se están dando pasos, ya, previsiblemente, antes de que acabe el año se va a sacar adelante un convenio en el sector turístico donde hay mejoras salariales. Eso es lo que perseguimos, ni somos un Gobierno intervencionista, porque ya se ha visto lo que ocurrió con el Gobierno anterior y las consecuencias, ni, desde luego, somos responsables de que la ley... porque votamos en contra de que la ley de vivienda aprobada en Madrid lo que haya hecho es sacar vivienda del mercado porque el propietario tiene miedo de ponerla a alquilar porque no tiene las garantías jurídicas. Créame que ese es el camino, no se confunda. Y, desde luego, en esto espero que nos ayude. La señora PRESIDENTA: Gracias, señor presidente. Tiempo de réplica, señor Galván, cuando quiera. El señor GALVÁN SASIA (desde su escaño): Señor Clavijo, pues le pido que nos invite a nosotros, a este grupo parlamentario, a participar en esas conversaciones de esos grupos, porque creo que se evitarían muchas malas interpretaciones, si es como usted dice, y, de todas maneras, controlaríamos mejor esa acción de gobierno. Muchas gracias. La señora PRESIDENTA: Gracias, señor Galván.\\n    '"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexing_dataset[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9ee44269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of retrieval prompts: 614\n",
      "Number of retrieval prompts: 161\n"
     ]
    }
   ],
   "source": [
    "prompts_retrieval_train = prepare_prompts_for_retrieval(dataset[\"train\"], tokenizer)\n",
    "prompts_retrieval_val = prepare_prompts_for_retrieval(dataset[\"validation\"], tokenizer)\n",
    "\n",
    "print(f\"Number of retrieval prompts: {len(prompts_retrieval_train)}\")\n",
    "print(f\"Number of retrieval prompts: {len(prompts_retrieval_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6ab6c0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "Eres un módulo de recuperación. Tu única tarea es devolver el identificador del documento correspondiente a la consulta dada.\n",
      "Sigue estrictamente estas reglas:\n",
      "1) Devuelve EXACTAMENTE una línea con el formato: DOCID:{<id>}.\n",
      "2) No incluyas palabras, explicaciones o puntuación extra antes o después de las llaves.\n",
      "3) Si múltiples documentos son plausibles, elige el mejor ID.\n",
      "4) Nunca inventes un ID fuera del espacio permitido. Mantente dentro de los prefijos válidos.\n",
      "5) No respondas a la pregunta; solo devuelve el docid.\"\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "\n",
      "        Dada la siguiente consulta, recupera los identificadores de los documentos relevantes. \n",
      "        Consulta: ¿Qué argumentos presentó el grupo parlamentario que intervino en la sesión del 22 de octubre de 2024, en relación con la propuesta de alteración del orden del día y su impacto en el desarrollo de las comparecencias del Gobierno?\n",
      "        <|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "DOCID:6592_1<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompts_retrieval_train[0], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "97de98e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset from prompts train, val, test\n",
    "retrieval_train_dataset = Dataset.from_dict({\"text\": prompts_retrieval_train})\n",
    "retrieval_val_dataset = Dataset.from_dict({\"text\": prompts_retrieval_val})\n",
    "\n",
    "retrieval_dataset = {\n",
    "    \"train\": retrieval_train_dataset,\n",
    "    \"validation\": retrieval_val_dataset,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "60314e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function_autoregressive(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1a0505aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 11162/11162 [01:05<00:00, 171.05 examples/s]\n"
     ]
    }
   ],
   "source": [
    "indexing_dataset_tokenizer = indexing_dataset.map(tokenize_function_autoregressive, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "898f2e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 614/614 [00:00<00:00, 1249.00 examples/s]\n",
      "Map: 100%|██████████| 161/161 [00:00<00:00, 1092.74 examples/s]\n"
     ]
    }
   ],
   "source": [
    "retrieval_train_dataset_tokenizer = retrieval_dataset[\"train\"].map(tokenize_function_autoregressive, batched=True)\n",
    "retrieval_val_dataset_tokenizer = retrieval_dataset[\"validation\"].map(tokenize_function_autoregressive, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa230b46",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "01d9a429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sft training\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "auto_config = SFTConfig(\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
    "    save_steps=100,\n",
    "    warmup_steps = 5,\n",
    "    num_train_epochs = 1, # Set this for 1 full training run.\n",
    "    #max_steps = 60,\n",
    "    learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
    "    logging_steps = 100,\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    seed = SEED,\n",
    "    report_to = \"none\", # Use this for WandB etc\n",
    "    output_dir=\"../models/qwen3-0.6b-rag-indexer\",\n",
    ")\n",
    "\n",
    "it_config = SFTConfig(\n",
    "    dataset_text_field=\"text\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,         # <-- añade eval batch size\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=5,\n",
    "    save_steps=5,\n",
    "    eval_steps=5,\n",
    "    eval_strategy=\"steps\",         # <-- activa evaluación periódica\n",
    "    num_train_epochs=1,             # <-- opcional: usa epochs en lugar de max_steps\n",
    "    #max_steps=60,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=1,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=SEED,\n",
    "    report_to=\"none\",\n",
    "    output_dir=\"../models/qwen3-0.6b-rag-retriever\",\n",
    "    load_best_model_at_end=True,          # <-- opcional\n",
    "    metric_for_best_model=\"eval_loss\",    # <-- opcional\n",
    "    greater_is_better=False,              # <-- opcional\n",
    ")\n",
    "\n",
    "trainer_auto = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=indexing_dataset_tokenizer,\n",
    "    tokenizer=tokenizer,\n",
    "    args=auto_config,\n",
    ")\n",
    "\n",
    "trainer_it = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=retrieval_train_dataset_tokenizer,\n",
    "    eval_dataset=retrieval_val_dataset_tokenizer,\n",
    "    tokenizer=tokenizer,\n",
    "    args=it_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "176683a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4090. Max memory = 23.988 GB.\n",
      "15.178 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b15ec644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 80,740,352 || all params: 676,790,272 || trainable%: 11.9299\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f708de6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 11,162 | Num Epochs = 1 | Total steps = 1,396\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 80,740,352 of 676,790,272 (11.93% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1396' max='1396' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1396/1396 23:51, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.883600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.570300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.528000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.486100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.466400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.447500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.394300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.439900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.406600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.412700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.390200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.397600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.389800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 614 | Num Epochs = 1 | Total steps = 77\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 80,740,352 of 676,790,272 (11.93% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='77' max='77' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [77/77 04:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.153900</td>\n",
       "      <td>0.104053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.053000</td>\n",
       "      <td>0.054196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.059800</td>\n",
       "      <td>0.050812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.050700</td>\n",
       "      <td>0.049696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.045800</td>\n",
       "      <td>0.048039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.043300</td>\n",
       "      <td>0.046564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.046600</td>\n",
       "      <td>0.045545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.040200</td>\n",
       "      <td>0.044809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.047300</td>\n",
       "      <td>0.044132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.041200</td>\n",
       "      <td>0.043629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.042200</td>\n",
       "      <td>0.043081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.046800</td>\n",
       "      <td>0.042708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.045800</td>\n",
       "      <td>0.042425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>0.042125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.041950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 11,162 | Num Epochs = 1 | Total steps = 1,396\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 80,740,352 of 676,790,272 (11.93% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1396' max='1396' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1396/1396 23:06, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.391000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.377600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.362200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.341100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.334400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.327900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.288200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.341600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.320900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.335200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.323100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.340400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.343000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 614 | Num Epochs = 1 | Total steps = 77\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 80,740,352 of 676,790,272 (11.93% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='77' max='77' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [77/77 04:09, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.054800</td>\n",
       "      <td>0.060458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.052700</td>\n",
       "      <td>0.058876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.051800</td>\n",
       "      <td>0.045965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.043700</td>\n",
       "      <td>0.044662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.039400</td>\n",
       "      <td>0.043547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.037100</td>\n",
       "      <td>0.042395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.040200</td>\n",
       "      <td>0.041889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.036200</td>\n",
       "      <td>0.041253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.042300</td>\n",
       "      <td>0.040736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.036200</td>\n",
       "      <td>0.040478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.036900</td>\n",
       "      <td>0.039892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.040100</td>\n",
       "      <td>0.039555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.038200</td>\n",
       "      <td>0.039382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.032900</td>\n",
       "      <td>0.039158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.032600</td>\n",
       "      <td>0.038999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 11,162 | Num Epochs = 1 | Total steps = 1,396\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 80,740,352 of 676,790,272 (11.93% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1396' max='1396' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1396/1396 25:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.239000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.242100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.236900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.228100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.230700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.233900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.206200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.268200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.259900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.283800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.282200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.322700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 614 | Num Epochs = 1 | Total steps = 77\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 80,740,352 of 676,790,272 (11.93% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='77' max='77' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [77/77 04:15, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.061100</td>\n",
       "      <td>0.068314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.038800</td>\n",
       "      <td>0.053920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.041800</td>\n",
       "      <td>0.045058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.037800</td>\n",
       "      <td>0.044377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.032900</td>\n",
       "      <td>0.043305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.033200</td>\n",
       "      <td>0.042847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.035200</td>\n",
       "      <td>0.042104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.033400</td>\n",
       "      <td>0.041183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.038600</td>\n",
       "      <td>0.040400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.032500</td>\n",
       "      <td>0.040001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.035600</td>\n",
       "      <td>0.039411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.039100</td>\n",
       "      <td>0.039093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.034400</td>\n",
       "      <td>0.038874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.029100</td>\n",
       "      <td>0.038594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.028000</td>\n",
       "      <td>0.038432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 11,162 | Num Epochs = 1 | Total steps = 1,396\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 80,740,352 of 676,790,272 (11.93% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1396' max='1396' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1396/1396 25:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.106000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.127800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.133000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.132000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.140100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.151700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.135700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.204600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.209200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.243400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.251700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.290100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.312400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 614 | Num Epochs = 1 | Total steps = 77\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 80,740,352 of 676,790,272 (11.93% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='77' max='77' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [77/77 04:14, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.036100</td>\n",
       "      <td>0.050206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.021900</td>\n",
       "      <td>0.059765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.033400</td>\n",
       "      <td>0.051433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.033200</td>\n",
       "      <td>0.046313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.031500</td>\n",
       "      <td>0.045301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.031500</td>\n",
       "      <td>0.043690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.034500</td>\n",
       "      <td>0.043414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.031800</td>\n",
       "      <td>0.042228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.036300</td>\n",
       "      <td>0.041628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.032000</td>\n",
       "      <td>0.040689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.032200</td>\n",
       "      <td>0.040030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.035900</td>\n",
       "      <td>0.039500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.033100</td>\n",
       "      <td>0.039288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.026400</td>\n",
       "      <td>0.039044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>0.038940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPOCHS = 4\n",
    "for _ in range(EPOCHS):\n",
    "    trainer_sft_stats = trainer_auto.train() # (context, id)\n",
    "    trainer_it_stats = trainer_it.train() # (query, id)\n",
    "    # GUARDAR MODELOS CADA SUPER EPOCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d7333bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 80,740,352 || all params: 676,790,272 || trainable%: 11.9299\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0d6735bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak reserved memory = 15.178 GB.\n",
      "Peak reserved memory for training = 0.0 GB.\n",
      "Peak reserved memory % of max memory = 63.273 %.\n",
      "Peak reserved memory for training % of max memory = 0.0 %.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d51434cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompts_for_testing(dataset, tokenizer):\n",
    "    system_prompt = \"\"\"Eres un módulo de recuperación. Tu única tarea es devolver el identificador del documento correspondiente a la consulta dada.\n",
    "Sigue estrictamente estas reglas:\n",
    "1) Devuelve EXACTAMENTE una línea con el formato: DOCID:{<id>}.\n",
    "2) No incluyas palabras, explicaciones o puntuación extra antes o después de las llaves.\n",
    "3) Si múltiples documentos son plausibles, elige el mejor ID.\n",
    "4) Nunca inventes un ID fuera del espacio permitido. Mantente dentro de los prefijos válidos.\n",
    "5) No respondas a la pregunta; solo devuelve el docid.\"\n",
    "\"\"\"\n",
    "    def build_prompt_it(tokenizer, system_prompt: str, prompt: str) -> str:\n",
    "        \"\"\"Builds the chat prompt for a single example using the tokenizer chat template.\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\",   \"content\": prompt},\n",
    "        ]\n",
    "        return tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False,\n",
    "        )\n",
    "    prompts = []\n",
    "    for item in dataset:\n",
    "        prompt = \"\"\"\n",
    "        Dada la siguiente consulta, recupera los identificadores de los documentos relevantes. \n",
    "        Consulta: {QUERY}\n",
    "        \"\"\"\n",
    "        question = item[\"question\"]\n",
    "        prompt = prompt.format(QUERY=question)\n",
    "        prompts.append(\n",
    "            ( \n",
    "                build_prompt_it(tokenizer, system_prompt, prompt),\n",
    "                item[\"id\"],\n",
    "            )\n",
    "        )\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c4f12665",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_retrieval_test = prepare_prompts_for_testing(dataset[\"train\"], tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4ba62589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6405_27\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "text = prompts_retrieval_test[i][0]\n",
    "doc_id_targets = prompts_retrieval_test[i][1]\n",
    "print(doc_id_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "43cbeadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "Eres un módulo de recuperación. Tu única tarea es devolver el identificador del documento correspondiente a la consulta dada.\n",
      "Sigue estrictamente estas reglas:\n",
      "1) Devuelve EXACTAMENTE una línea con el formato: DOCID:{<id>}.\n",
      "2) No incluyas palabras, explicaciones o puntuación extra antes o después de las llaves.\n",
      "3) Si múltiples documentos son plausibles, elige el mejor ID.\n",
      "4) Nunca inventes un ID fuera del espacio permitido. Mantente dentro de los prefijos válidos.\n",
      "5) No respondas a la pregunta; solo devuelve el docid.\"\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "\n",
      "        Dada la siguiente consulta, recupera los identificadores de los documentos relevantes. \n",
      "        Consulta: ¿Qué argumentos presentó el grupo parlamentario Mixto en relación con la falta de remisión de documentación sobre las inversiones en medios de comunicación durante el año 2020, en el contexto de la pregunta formulada por el diputado don Ricardo Fernández de la Puente Armas en la sesión del 12 de julio de 2022?\n",
      "        <|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "47228fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "DOCID:6405_1<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# test the model in streaming mode\n",
    "from transformers import TextStreamer\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=False)\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 64, # Increase for longer outputs!\n",
    "    temperature = 0.00000001,\n",
    "    streamer = streamer,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
