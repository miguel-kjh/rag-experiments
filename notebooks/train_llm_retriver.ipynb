{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "473b050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_from_disk\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import torch\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c9d8b356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.9.6: Fast Qwen3 patching. Transformers: 4.55.4. vLLM: 0.10.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070 SUPER. Num GPUs = 1. Max memory: 11.994 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = 8192,\n",
    "    load_in_4bit = False,\n",
    "    load_in_8bit = False,\n",
    ")\n",
    "RANK = 32\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = RANK,           # Choose any number > 0! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = RANK*2,  # Best to choose alpha = rank or rank*2\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,   # We support rank stabilized LoRA\n",
    "    loftq_config = None,  # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5b956a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    ")\n",
    "\n",
    "db = FAISS.load_local(\n",
    "    \"../data/db/ragbench-covidqa/ragbench-covidqa_embeddings_sentence-transformers_paraphrase-multilingual-mpnet-base-v2\",\n",
    "    embedding_model,\n",
    "    allow_dangerous_deserialization=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "08fbfd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 4944\n"
     ]
    }
   ],
   "source": [
    "#quiero la lista de documentos\n",
    "docs = db.docstore._dict.values()\n",
    "documents = list(docs)\n",
    "print(f\"Number of documents: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbda5e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_AUTORE = \"../data/processed/ragbench-covidqa\"\n",
    "dataset = load_from_disk(FOLDER_AUTORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57135256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Title: Emergent severe acute respiratory distress syndrome caused by adenovirus type 55 in immunocompetent adults in 2013: a prospective observational study\\nPassage: Recent studies have shown that the immune system plays a crucial role in the clearance of HAdV viremia and survival of the host . Chen et al. reported that, in the acute phase of HAdV-55 infection, patients with severe disease may have high levels of dendritic cells and Th17 cells . In our study, the only patient who recovered from severe infection had higher T-cell counts. Three of the five patients had relatively low T-cell counts when admitted. Our results suggest that these three patients may have been relatively immunocompromised and that a lower T-cell count may be a risk'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0][\"documents\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1efe551",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5126e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompt_for_indexing(documents: list):\n",
    "    prompt = \"\"\"\n",
    "    DOCID:{doc_id}\n",
    "    Document:\n",
    "    {doc}\n",
    "    \"\"\"\n",
    "    for doc in documents:\n",
    "        document = doc.page_content\n",
    "        doc_id = doc.metadata.get(\"id\", \"unknown\")\n",
    "        yield prompt.format(doc=document, doc_id=doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1f36a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_it(tokenizer, system_prompt: str, prompt: str) -> str:\n",
    "    \"\"\"Builds the chat prompt for a single example using the tokenizer chat template.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\",   \"content\": prompt},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d4d4258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompts_for_retrieval(dataset, tokenizer):\n",
    "    system_prompt = \"\"\"You are a retrieval module. Your sole task is to output the document identifier for the given query.\n",
    "Follow these rules strictly:\n",
    "1) Output EXACTLY one line in the format: DOCID:{<id>}.\n",
    "2) Do NOT include any extra words, explanations, or punctuation before or after the braces.\n",
    "3) If multiple documents are plausible, choose the single best ID.\n",
    "4) Never invent an ID outside the allowed space. Stay within valid prefixes.\n",
    "5) Do NOT answer the question; only output the docid.\"\n",
    "\"\"\"\n",
    "    prompts = []\n",
    "    for item in dataset:\n",
    "        prompt = \"\"\"\n",
    "        [QUERY]\n",
    "        {QUERY}\n",
    "        [OUTPUT]\n",
    "        \"\"\"\n",
    "        docid_prompt = \"DOCID:{docid}\"\n",
    "        question = item[\"question\"]\n",
    "        prompt = prompt.format(QUERY=question)\n",
    "        for docid in item[\"document_ids\"]:\n",
    "            prompt += docid_prompt.format(docid=docid) + \"\\n\"\n",
    "        prompts.append(build_prompt_it(tokenizer, system_prompt, prompt))\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b744a024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prompts: 4944\n"
     ]
    }
   ],
   "source": [
    "prompts = list(prepare_prompt_for_indexing(documents))\n",
    "print(f\"Number of prompts: {len(prompts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed65b3b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 4944\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataset from prompts\n",
    "from datasets import Dataset\n",
    "indexing_dataset = Dataset.from_dict({\"text\": prompts})\n",
    "indexing_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ee44269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of retrieval prompts: 1252\n",
      "Number of retrieval prompts: 267\n",
      "Number of retrieval prompts: 246\n"
     ]
    }
   ],
   "source": [
    "prompts_retrieval_train = prepare_prompts_for_retrieval(dataset[\"train\"], tokenizer)\n",
    "prompts_retrieval_val = prepare_prompts_for_retrieval(dataset[\"validation\"], tokenizer)\n",
    "prompts_retrieval_test = prepare_prompts_for_retrieval(dataset[\"test\"], tokenizer)\n",
    "\n",
    "print(f\"Number of retrieval prompts: {len(prompts_retrieval_train)}\")\n",
    "print(f\"Number of retrieval prompts: {len(prompts_retrieval_val)}\")\n",
    "print(f\"Number of retrieval prompts: {len(prompts_retrieval_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97de98e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset from prompts train, val, test\n",
    "retrieval_train_dataset = Dataset.from_dict({\"text\": prompts_retrieval_train})\n",
    "retrieval_val_dataset = Dataset.from_dict({\"text\": prompts_retrieval_val})\n",
    "retrieval_test_dataset = Dataset.from_dict({\"text\": prompts_retrieval_test})\n",
    "\n",
    "retrieval_dataset = {\n",
    "    \"train\": retrieval_train_dataset,\n",
    "    \"validation\": retrieval_val_dataset,\n",
    "    \"test\": retrieval_test_dataset,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60314e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function_autoregressive(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a0505aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4944/4944 [00:02<00:00, 1865.16 examples/s]\n"
     ]
    }
   ],
   "source": [
    "indexing_dataset_tokenizer = indexing_dataset.map(tokenize_function_autoregressive, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa230b46",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d9a429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=32):   0%|          | 0/1252 [00:00<?, ? examples/s]"
     ]
    }
   ],
   "source": [
    "# sft training\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "auto_config = SFTConfig(\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
    "    save_steps=10,\n",
    "    warmup_steps = 5,\n",
    "    num_train_epochs = 1, # Set this for 1 full training run.\n",
    "    # max_steps = 30,\n",
    "    learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
    "    logging_steps = 1,\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    seed = SEED,\n",
    "    report_to = \"none\", # Use this for WandB etc\n",
    "    output_dir=\"../models/qwen3-0.6b-rag-indexer\",\n",
    ")\n",
    "\n",
    "it_config = SFTConfig(\n",
    "    dataset_text_field=\"text\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,         # <-- añade eval batch size\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=5,\n",
    "    save_steps=5,\n",
    "    eval_steps=5,\n",
    "    eval_strategy=\"steps\",         # <-- activa evaluación periódica\n",
    "    num_train_epochs=1,             # <-- opcional: usa epochs en lugar de max_steps\n",
    "    #max_steps=30,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=1,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    seed=SEED,\n",
    "    report_to=\"none\",\n",
    "    output_dir=\"../models/qwen3-0.6b-rag-retriever\",\n",
    "    load_best_model_at_end=True,          # <-- opcional\n",
    "    metric_for_best_model=\"eval_loss\",    # <-- opcional\n",
    "    greater_is_better=False,              # <-- opcional\n",
    ")\n",
    "\n",
    "trainer_auto = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=indexing_dataset_tokenizer,\n",
    "    tokenizer=tokenizer,\n",
    "    args=auto_config,\n",
    ")\n",
    "\n",
    "trainer_it = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=retrieval_dataset[\"train\"],\n",
    "    eval_dataset=retrieval_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    args=it_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "176683a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4070 SUPER. Max memory = 11.994 GB.\n",
      "7.77 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f708de6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 4,944 | Num Epochs = 1 | Total steps = 30\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 20,185,088 of 616,235,008 (3.28% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:50, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>13.546800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>13.605700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>12.674500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>11.510800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>10.027800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>9.126200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>8.680100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8.160500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>7.854700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>7.681500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>7.547200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>7.643400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>7.543700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>7.416000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>7.391100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>7.351000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>7.401000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>7.421800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>7.350800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>7.321400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>7.377800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>7.320400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>7.306800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>7.281300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>7.357600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>7.351800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>7.295800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>7.280500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>7.360600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>7.376600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,252 | Num Epochs = 1 | Total steps = 30\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 20,185,088 of 616,235,008 (3.28% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 01:12, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.293900</td>\n",
       "      <td>1.829447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.543700</td>\n",
       "      <td>0.531312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.436000</td>\n",
       "      <td>0.438055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.359100</td>\n",
       "      <td>0.426214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.478000</td>\n",
       "      <td>0.423520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.329700</td>\n",
       "      <td>0.422409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _ in range(4):\n",
    "    trainer_sft_stats = trainer_auto.train()\n",
    "    trainer_it_stats = trainer_it.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0d6735bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak reserved memory = 7.77 GB.\n",
      "Peak reserved memory for training = 0.0 GB.\n",
      "Peak reserved memory % of max memory = 64.782 %.\n",
      "Peak reserved memory for training % of max memory = 0.0 %.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c4f12665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a retrieval module. Your sole task is to output the document identifier for the given query.\n",
      "Follow these rules strictly:\n",
      "1) Output EXACTLY one line in the format: DOCID:{<id>}.\n",
      "2) Do NOT include any extra words, explanations, or punctuation before or after the braces.\n",
      "3) If multiple documents are plausible, choose the single best ID.\n",
      "4) Never invent an ID outside the allowed space. Stay within valid prefixes.\n",
      "5) Do NOT answer the question; only output the docid.\"\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "\n",
      "        [QUERY]\n",
      "        What role does T-cell count play in severe human adenovirus type 55 (HAdV-55) infection?\n",
      "[OUTPUT]\n",
      "\n",
      "DOCID:1395\n",
      "DOCID:1395\n",
      "DOCID:2160\n",
      "DOCID:2158\n",
      "<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "text_completed = retrieval_dataset['train'][0][\"text\"]\n",
    "text = text_completed.split(\"[OUTPUT]\")[0].strip() + \"\\n[OUTPUT]\\n\"\n",
    "doc_id_targets = text_completed.split(\"[OUTPUT]\")[1].strip()\n",
    "print(text)\n",
    "print(doc_id_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "47228fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCID:1197\n",
      "<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# test the model in streaming mode\n",
    "from transformers import TextStreamer\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 64, # Increase for longer outputs!\n",
    "    temperature = 0.000001,\n",
    "    streamer = streamer,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
