{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473b050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import torch\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d8b356",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B-instruct\"\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = MAX_LENGTH,\n",
    "    full_finetuning=False,\n",
    "    load_in_4bit = False,\n",
    "    load_in_8bit = False,\n",
    ")\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.eos_token_id\n",
    "RANK = 512\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = RANK,           # Choose any number > 0! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = RANK*2,  # Best to choose alpha = rank or rank*2\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = SEED,\n",
    "    use_rslora = False,   # We support rank stabilized LoRA\n",
    "    loftq_config = None,  # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb336663",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76840c6e",
   "metadata": {},
   "source": [
    "### tel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14661458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset_qa = load_from_disk(\"../notebooks/data/dataset_instruct_train/\")\n",
    "# create validation dataset from dataset_qa\n",
    "dataset_qa = dataset_qa.train_test_split(test_size=0.1, seed=SEED)\n",
    "dataset_knowledge = load_from_disk(\"../notebooks/data/dataset_telephones\")\n",
    "\n",
    "documents = list(dataset_knowledge[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e47c08f",
   "metadata": {},
   "source": [
    "### Simple dataset loading example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bb16e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.schema import Document\n",
    "dataset_knowledge = pd.read_csv(\"../notebooks/data/contacts_docs.csv\")\n",
    "documents = []\n",
    "for index, row in dataset_knowledge.iterrows():\n",
    "    doc = f\"Nombre: {row['name']}\\nTeléfono: {row['phone']}\"\n",
    "    documents.append(Document(page_content=doc, metadata={\"id\": f\"{row['id']}\" } ))\n",
    "print(f\"Loaded {len(documents)} documents.\")\n",
    "print(f\"First document: {documents[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d5204e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_dataset_train = pd.read_csv(\"../notebooks/data/contacts_queries_train.csv\")\n",
    "query_dataset_val = pd.read_csv(\"../notebooks/data/contacts_queries_val.csv\")\n",
    "query_dataset_test = pd.read_csv(\"../notebooks/data/contacts_queries_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f936fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = {\n",
    "    \"train\": query_dataset_train,\n",
    "    \"validation\": query_dataset_val,\n",
    "    \"test\": query_dataset_test,\n",
    "}\n",
    "\n",
    "#to hugginface dataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "dataset_qa = {}\n",
    "for split in all_data:\n",
    "    dataset_qa[split] = Dataset.from_pandas(all_data[split])\n",
    "dataset_qa = DatasetDict(dataset_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7f1812",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef7d329",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7952bc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JOIN TRAIN AND VAL DATASETS\n",
    "from datasets import concatenate_datasets\n",
    "dataset_qa[\"train\"] = concatenate_datasets([dataset_qa[\"train\"], dataset_qa[\"validation\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950c17d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename \"respuesta\" column to \"answer\"\n",
    "dataset_qa = dataset_qa.rename_column(\"respuesta\", \"answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12e213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_qa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1efe551",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680e0009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_it(tokenizer, system_prompt: str, prompt: str, response: str) -> str:\n",
    "    \"\"\"Builds the chat prompt for a single example using the tokenizer chat template.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\",   \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": response}\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5126e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_knowledge_injection_prompts(documents: list):\n",
    "    prompt = \"\"\"{doc}\"\"\"\n",
    "    for doc in documents:\n",
    "        yield prompt.format(doc=doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4d4258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_prompts(dataset, tokenizer):\n",
    "    system_prompt = \"\"\"\n",
    "    Eres un modelo de lenguaje entrenado para responder preguntas.\n",
    "    \"\"\"\n",
    "    prompts = []\n",
    "    for item in dataset:\n",
    "        prompt = \"\"\"{QUERY}\"\"\"\n",
    "        response = \"{response}\"\n",
    "        question = item[\"question\"]\n",
    "        prompt = prompt.format(QUERY=question)\n",
    "        prompts.append(build_prompt_it(tokenizer, system_prompt, prompt, response.format(response=item[\"answer\"])))\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b744a024",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = list(generate_knowledge_injection_prompts(documents))\n",
    "print(f\"Number of prompts: {len(prompts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b9ed38",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86e8e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUIERO VER LOS TOKENS\n",
    "def print_tokens(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    print(\"Number of tokens:\", len(tokens), \"\\n\")\n",
    "    print(\"Tokens:\", tokens)\n",
    "\n",
    "print_tokens(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed65b3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset from prompts\n",
    "from datasets import Dataset\n",
    "knowledge_dataset = Dataset.from_dict({\"text\": prompts})\n",
    "knowledge_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda96468",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_dataset[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee44269",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_qa_train = generate_qa_prompts(dataset_qa[\"train\"], tokenizer)\n",
    "prompts_qa_val = generate_qa_prompts(dataset_qa[\"test\"], tokenizer)\n",
    "\n",
    "print(f\"Number of retrieval prompts: {len(prompts_qa_train)}\")\n",
    "print(f\"Number of retrieval prompts: {len(prompts_qa_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab6c0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompts_qa_train[0], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e1cfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tokens(prompts_qa_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97de98e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset from prompts train, val, test\n",
    "qa_train_dataset = Dataset.from_dict({\"text\": prompts_qa_train})\n",
    "qa_val_dataset = Dataset.from_dict({\"text\": prompts_qa_val})\n",
    "\n",
    "qa_dataset = {\n",
    "    \"train\": qa_train_dataset,\n",
    "    \"validation\": qa_val_dataset,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60314e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function_autoregressive(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0505aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_dataset_tokenizer = knowledge_dataset.map(tokenize_function_autoregressive, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898f2e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_train_dataset_tokenizer = qa_dataset[\"train\"].map(tokenize_function_autoregressive, batched=True)\n",
    "qa_val_dataset_tokenizer = qa_dataset[\"validation\"].map(tokenize_function_autoregressive, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa230b46",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d9a429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sft training\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "auto_config = UnslothTrainingArguments(\n",
    "    per_device_train_batch_size = 4,\n",
    "    gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
    "    save_strategy=\"no\",\n",
    "    save_total_limit=0,\n",
    "    warmup_steps = 5,\n",
    "    num_train_epochs = 1, # Set this for 1 full training run.\n",
    "    #max_steps = 60,\n",
    "    learning_rate = 1e-4, # Reduce to 2e-5 for long training runs\n",
    "    logging_steps = 1,\n",
    "    # 32 bits\n",
    "    optim = \"paged_adamw_32bit\",\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    seed = SEED,\n",
    "    report_to = \"none\", # Use this for WandB etc\n",
    "    output_dir=\"../models/qwen3-0.6b-rag-indexer\",\n",
    ")\n",
    "\n",
    "it_config = SFTConfig(\n",
    "    dataset_text_field=\"text\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,         # <-- añade eval batch size\n",
    "    gradient_accumulation_steps=2,\n",
    "    warmup_steps=25,\n",
    "    save_strategy=\"no\",\n",
    "    save_total_limit=0,\n",
    "    eval_steps=1,\n",
    "    eval_strategy=\"steps\",         # <-- activa evaluación periódica\n",
    "    num_train_epochs=1,             # <-- opcional: usa epochs en lugar de max_steps\n",
    "    #max_steps=30,\n",
    "    learning_rate=1e-4,\n",
    "    logging_steps=1,\n",
    "    optim = \"paged_adamw_32bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    seed=SEED,\n",
    "    report_to=\"none\",\n",
    "    output_dir=\"../models/qwen3-0.6b-rag-retriever\",\n",
    "    load_best_model_at_end=False,          # <-- opcional\n",
    "    metric_for_best_model=\"eval_loss\",    # <-- opcional\n",
    "    greater_is_better=False,              # <-- opcional\n",
    ")\n",
    "\n",
    "trainer_auto = UnslothTrainer(\n",
    "    model=model,\n",
    "    train_dataset=knowledge_dataset_tokenizer,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    args=auto_config,\n",
    ")\n",
    "\n",
    "trainer_it = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=qa_train_dataset_tokenizer,\n",
    "    eval_dataset=qa_val_dataset_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    args=it_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176683a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15ec644",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f708de6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "for _ in range(EPOCHS):\n",
    "    print(f\"--- SUPER EPOCH {_+1} / {EPOCHS} ---\")\n",
    "    trainer_sft_stats = trainer_auto.train() \n",
    "    trainer_it_stats = trainer_it.train()\n",
    "    # GUARDAR MODELOS CADA SUPER EPOCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6735bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51434cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_prompts_testing(dataset, tokenizer):\n",
    "    def build_prompt_it_generation(tokenizer, system_prompt: str, prompt: str) -> str:\n",
    "        \"\"\"Builds the chat prompt for a single example using the tokenizer chat template.\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\",   \"content\": prompt},\n",
    "        ]\n",
    "        return tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False,\n",
    "        )\n",
    "    system_prompt = \"\"\"\n",
    "    Eres un modelo de lenguaje entrenado para responder preguntas.\n",
    "    \"\"\"\n",
    "    prompts = []\n",
    "    for item in dataset:\n",
    "        prompt = \"\"\"{QUERY}\"\"\"\n",
    "        question = item[\"question\"]\n",
    "        answer = item[\"answer\"]\n",
    "        prompt = prompt.format(QUERY=question)\n",
    "        prompts.append(\n",
    "            (\n",
    "                build_prompt_it_generation(tokenizer, system_prompt, prompt),\n",
    "                answer,\n",
    "            )\n",
    "        )\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f12665",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_retrieval_test = generate_qa_prompts_testing(dataset_qa[\"test\"], tokenizer)\n",
    "prompts_retrieval_train = generate_qa_prompts_testing(dataset_qa[\"train\"], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba62589",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "print(prompts_retrieval_train[idx][0], sep=\"\\n\")\n",
    "print(prompts_retrieval_train[idx][1], sep=\"\\n\")\n",
    "text = prompts_retrieval_train[idx][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47228fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model in streaming mode\n",
    "from transformers import TextStreamer\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=False)\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 64, # Increase for longer outputs!\n",
    "    do_sample = False,\n",
    "    top_p = 0.1,\n",
    "    temperature = 0.,\n",
    "    streamer = streamer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495dc274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model in non-streaming mode\n",
    "import re\n",
    "import tqdm\n",
    "\n",
    "acc = 0\n",
    "total = 0\n",
    "\n",
    "for text, answer in tqdm.tqdm(prompts_retrieval_test, desc=\"Testing\"):\n",
    "    print(\"\\n---\\n\")\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=64,  # Increase for longer outputs!\n",
    "        do_sample=False, temperature=0.0, top_p=1.0\n",
    "    )\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True, skip_prompt=True)\n",
    "    response = generated_text.split(\"assistant\")[-1].strip()\n",
    "    print(\"Correct:\", answer, \"==\", \"Predicted:\", response)\n",
    "    if response == answer:\n",
    "        print(\"✅ Correct\")\n",
    "        acc += 1\n",
    "    else:\n",
    "        print(\"❌ Incorrect\")\n",
    "    total += 1\n",
    "print(f\"Accuracy: {acc}/{total} = {acc/total*100:.2f} %\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
