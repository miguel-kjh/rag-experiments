{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "473b050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import torch\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9d8b356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.9.8: Fast Llama patching. Transformers: 4.56.2. vLLM: 0.10.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.988 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "trainable params: 360,710,144 || all params: 1,596,524,544 || trainable%: 22.5935\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B-instruct\"\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = MAX_LENGTH,\n",
    "    full_finetuning=False,\n",
    "    load_in_4bit = True,\n",
    "    load_in_8bit = False,\n",
    ")\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.eos_token_id\n",
    "RANK = 512\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = RANK,           # Choose any number > 0! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = RANK*2,  # Best to choose alpha = rank or rank*2\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = SEED,\n",
    "    use_rslora = False,   # We support rank stabilized LoRA\n",
    "    loftq_config = None,  # And LoftQ\n",
    ")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb336663",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e47c08f",
   "metadata": {},
   "source": [
    "### Simple dataset loading example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a6bb16e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400 documents.\n",
      "First document: page_content='Nombre: Alba Alonso\n",
      "Teléfono: 632 322 183' metadata={'id': '7500_1'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain.schema import Document\n",
    "dataset_knowledge = pd.read_csv(\"../notebooks/data/contacts_docs.csv\")\n",
    "documents = []\n",
    "for index, row in dataset_knowledge.iterrows():\n",
    "    doc = f\"Nombre: {row['name']}\\nTeléfono: {row['phone']}\"\n",
    "    documents.append(Document(page_content=doc, metadata={\"id\": f\"{row['id']}\" } ))\n",
    "print(f\"Loaded {len(documents)} documents.\")\n",
    "print(f\"First document: {documents[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d3d5204e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_dataset_train = pd.read_csv(\"../notebooks/data/contacts_queries_train.csv\")\n",
    "query_dataset_val = pd.read_csv(\"../notebooks/data/contacts_queries_val.csv\")\n",
    "query_dataset_test = pd.read_csv(\"../notebooks/data/contacts_queries_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "50f936fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = {\n",
    "    \"train\": query_dataset_train,\n",
    "    \"validation\": query_dataset_val,\n",
    "    \"test\": query_dataset_test,\n",
    "}\n",
    "\n",
    "#to hugginface dataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "dataset_qa = {}\n",
    "for split in all_data:\n",
    "    dataset_qa[split] = Dataset.from_pandas(all_data[split])\n",
    "dataset_qa = DatasetDict(dataset_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1e7f1812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'id', 'respuesta'],\n",
       "        num_rows: 1400\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'id', 'respuesta'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'id', 'respuesta'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7952bc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JOIN TRAIN AND VAL DATASETS\n",
    "from datasets import concatenate_datasets\n",
    "dataset_qa[\"train\"] = concatenate_datasets([dataset_qa[\"train\"], dataset_qa[\"validation\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "950c17d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename \"respuesta\" column to \"answer\"\n",
    "dataset_qa = dataset_qa.rename_column(\"respuesta\", \"answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c12e213c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'id', 'answer'],\n",
       "        num_rows: 1700\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'id', 'answer'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'id', 'answer'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_qa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1efe551",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "680e0009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_it(tokenizer, system_prompt: str, prompt: str, response: str) -> str:\n",
    "    \"\"\"Builds the chat prompt for a single example using the tokenizer chat template.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\",   \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": response}\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5126e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_knowledge_injection_prompts(documents: list):\n",
    "    prompt = \"\"\"{doc}\"\"\"\n",
    "    for doc in documents:\n",
    "        yield prompt.format(doc=doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3d4d4258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_prompts(dataset, tokenizer):\n",
    "    system_prompt = \"\"\"\n",
    "    Eres un modelo de lenguaje entrenado para responder preguntas.\n",
    "    \"\"\"\n",
    "    prompts = []\n",
    "    for item in dataset:\n",
    "        prompt = \"\"\"{QUERY}\"\"\"\n",
    "        response = \"{response}\"\n",
    "        question = item[\"question\"]\n",
    "        prompt = prompt.format(QUERY=question)\n",
    "        prompts.append(build_prompt_it(tokenizer, system_prompt, prompt, response.format(response=item[\"answer\"])))\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b744a024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prompts: 400\n"
     ]
    }
   ],
   "source": [
    "prompts = list(generate_knowledge_injection_prompts(documents))\n",
    "print(f\"Number of prompts: {len(prompts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a7b9ed38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nombre: Alba Alonso\\nTeléfono: 632 322 183'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d86e8e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 15 \n",
      "\n",
      "Tokens: ['Nombre', ':', 'ĠAl', 'ba', 'ĠAlonso', 'Ċ', 'Tel', 'Ã©fono', ':', 'Ġ', '632', 'Ġ', '322', 'Ġ', '183']\n"
     ]
    }
   ],
   "source": [
    "# QUIERO VER LOS TOKENS\n",
    "def print_tokens(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    print(\"Number of tokens:\", len(tokens), \"\\n\")\n",
    "    print(\"Tokens:\", tokens)\n",
    "\n",
    "print_tokens(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ed65b3b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 400\n",
       "})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataset from prompts\n",
    "from datasets import Dataset\n",
    "knowledge_dataset = Dataset.from_dict({\"text\": prompts})\n",
    "knowledge_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cda96468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nombre: Alba Alonso\\nTeléfono: 632 322 183'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knowledge_dataset[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9ee44269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of retrieval prompts: 1700\n",
      "Number of retrieval prompts: 300\n"
     ]
    }
   ],
   "source": [
    "prompts_qa_train = generate_qa_prompts(dataset_qa[\"train\"], tokenizer)\n",
    "prompts_qa_val = generate_qa_prompts(dataset_qa[\"test\"], tokenizer)\n",
    "\n",
    "print(f\"Number of retrieval prompts: {len(prompts_qa_train)}\")\n",
    "print(f\"Number of retrieval prompts: {len(prompts_qa_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6ab6c0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 07 Nov 2025\n",
      "\n",
      "Eres un modelo de lenguaje entrenado para responder preguntas.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Necesito el contacto asociado al 620 152 344. —consulta interna—<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "El número 620 152 344 pertenece a Alejandro Vega.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(prompts_qa_train[0], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "87e1cfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 87 \n",
      "\n",
      "Tokens: ['<|begin_of_text|>', '<|start_header_id|>', 'system', '<|end_header_id|>', 'ĊĊ', 'Cut', 'ting', 'ĠKnowledge', 'ĠDate', ':', 'ĠDecember', 'Ġ', '202', '3', 'Ċ', 'Today', 'ĠDate', ':', 'Ġ', '07', 'ĠNov', 'Ġ', '202', '5', 'ĊĊ', 'E', 'res', 'Ġun', 'Ġmodelo', 'Ġde', 'Ġl', 'engu', 'aje', 'Ġentren', 'ado', 'Ġpara', 'Ġresponder', 'Ġpreg', 'untas', '.', '<|eot_id|>', '<|start_header_id|>', 'user', '<|end_header_id|>', 'ĊĊ', 'N', 'ec', 'es', 'ito', 'Ġel', 'Ġcontacto', 'Ġasoci', 'ado', 'Ġal', 'Ġ', '620', 'Ġ', '152', 'Ġ', '344', '.', 'ĠâĢĶ', 'consulta', 'Ġintern', 'a', 'âĢĶ', '<|eot_id|>', '<|start_header_id|>', 'assistant', '<|end_header_id|>', 'ĊĊ', 'El', 'ĠnÃºmero', 'Ġ', '620', 'Ġ', '152', 'Ġ', '344', 'Ġpert', 'ene', 'ce', 'Ġa', 'ĠAlejandro', 'ĠVega', '.', '<|eot_id|>']\n"
     ]
    }
   ],
   "source": [
    "print_tokens(prompts_qa_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "97de98e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset from prompts train, val, test\n",
    "qa_train_dataset = Dataset.from_dict({\"text\": prompts_qa_train})\n",
    "qa_val_dataset = Dataset.from_dict({\"text\": prompts_qa_val})\n",
    "\n",
    "qa_dataset = {\n",
    "    \"train\": qa_train_dataset,\n",
    "    \"validation\": qa_val_dataset,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "60314e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function_autoregressive(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1a0505aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 12954.68 examples/s]\n"
     ]
    }
   ],
   "source": [
    "knowledge_dataset_tokenizer = knowledge_dataset.map(tokenize_function_autoregressive, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "898f2e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1700/1700 [00:00<00:00, 17855.79 examples/s]\n",
      "Map: 100%|██████████| 300/300 [00:00<00:00, 15787.84 examples/s]\n"
     ]
    }
   ],
   "source": [
    "qa_train_dataset_tokenizer = qa_dataset[\"train\"].map(tokenize_function_autoregressive, batched=True)\n",
    "qa_val_dataset_tokenizer = qa_dataset[\"validation\"].map(tokenize_function_autoregressive, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7136411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_prompts_testing(dataset, tokenizer):\n",
    "    def build_prompt_it_generation(tokenizer, system_prompt: str, prompt: str) -> str:\n",
    "        \"\"\"Builds the chat prompt for a single example using the tokenizer chat template.\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\",   \"content\": prompt},\n",
    "        ]\n",
    "        return tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False,\n",
    "        )\n",
    "    system_prompt = \"\"\"\n",
    "    Eres un modelo de lenguaje entrenado para responder preguntas.\n",
    "    \"\"\"\n",
    "    prompts = []\n",
    "    for item in dataset:\n",
    "        prompt = \"\"\"{QUERY}\"\"\"\n",
    "        question = item[\"question\"]\n",
    "        answer = item[\"answer\"]\n",
    "        prompt = prompt.format(QUERY=question)\n",
    "        prompts.append(\n",
    "            (\n",
    "                build_prompt_it_generation(tokenizer, system_prompt, prompt),\n",
    "                answer,\n",
    "            )\n",
    "        )\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b8379bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 07 Nov 2025\n",
      "\n",
      "Eres un modelo de lenguaje entrenado para responder preguntas.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "¿A quién pertenece el teléfono 736 615 948?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts_retrieval_test = generate_qa_prompts_testing(dataset_qa[\"test\"], tokenizer)\n",
    "text_for_testing = prompts_retrieval_test[0][0]\n",
    "print(text_for_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4632b63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tqdm\n",
    "import torch\n",
    "\n",
    "def levenshtein(s: str, t: str) -> int:\n",
    "    \"\"\"\n",
    "    Distancia de Levenshtein con memoria O(min(len(s), len(t))).\n",
    "    \"\"\"\n",
    "    # Asegura que t es la más corta para usar menos memoria\n",
    "    if len(s) < len(t):\n",
    "        s, t = t, s\n",
    "    previous = list(range(len(t) + 1))\n",
    "\n",
    "    for i, cs in enumerate(s, start=1):\n",
    "        current = [i]\n",
    "        for j, ct in enumerate(t, start=1):\n",
    "            costo = 0 if cs == ct else 1\n",
    "            current.append(min(\n",
    "                current[-1] + 1,          # inserción\n",
    "                previous[j] + 1,          # borrado\n",
    "                previous[j-1] + costo     # sustitución\n",
    "            ))\n",
    "        previous = current\n",
    "    return previous[-1]\n",
    "\n",
    "\n",
    "def test_model_accuracy(model, tokenizer, prompts_retrieval_test, device=\"cuda\", \n",
    "                        max_new_tokens=64, temperature=0.0, top_p=1.0):\n",
    "    \n",
    "    acc = 0\n",
    "    lev = 0\n",
    "    total = 0\n",
    "\n",
    "    progress_bar = tqdm.tqdm(prompts_retrieval_test, desc=\"Testing\")\n",
    "\n",
    "    for text, answer in progress_bar:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p\n",
    "            )\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = generated_text.split(\"assistant\")[-1].strip()\n",
    "\n",
    "        if response == answer:\n",
    "            acc += 1\n",
    "        lev += levenshtein(response, answer)\n",
    "        total += 1\n",
    "\n",
    "        progress_bar.set_postfix({\n",
    "            \"acc\": f\"{acc/total*100:.2f} %\",\n",
    "            \"lev\": f\"{lev/total:.2f}\"\n",
    "        })\n",
    "\n",
    "    final_acc = acc / total * 100 if total > 0 else 0.0\n",
    "    print(f\"Accuracy final: {acc}/{total} = {final_acc:.2f} %\")\n",
    "    final_lev = lev / total if total > 0 else 0.0\n",
    "    print(f\"Levenshtein final: {final_lev:.2f}\")\n",
    "    return final_acc, final_lev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "18dd53aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Genera gráficos de la evolución de la accuracy y las pérdidas (SFT e IT) durante el entrenamiento.\n",
    "\n",
    "    Parámetros:\n",
    "        history (list[dict]): lista de diccionarios, donde cada elemento debe contener:\n",
    "            - \"super_epoch\": número de super-época.\n",
    "            - \"accuracy\": precisión alcanzada.\n",
    "            - \"trainer_sft_stats\".training_loss: pérdida SFT.\n",
    "            - \"trainer_it_stats\".training_loss: pérdida IT.\n",
    "    \"\"\"\n",
    "    # Extraer métricas del historial\n",
    "    super_epochs = [h[\"super_epoch\"] for h in history]\n",
    "    accuracies = [h[\"accuracy\"] for h in history]\n",
    "    losses_sft = [h[\"trainer_sft_stats\"].training_loss for h in history]\n",
    "    losses_it = [h[\"trainer_it_stats\"].training_loss for h in history]\n",
    "\n",
    "    # --- Gráfico de Accuracy ---\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(super_epochs, accuracies, marker='o', color='tab:blue')\n",
    "    plt.xlabel(\"Super Epoch\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.title(\"Accuracy History\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Gráfico de Losses ---\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(super_epochs, losses_sft, marker='o', label=\"SFT Loss\", color='tab:orange')\n",
    "    plt.plot(super_epochs, losses_it, marker='s', label=\"IT Loss\", color='tab:green')\n",
    "    plt.xlabel(\"Super Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss History\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa230b46",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ace130b",
   "metadata": {},
   "source": [
    "### Iterative training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "01d9a429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sft training\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "auto_config = UnslothTrainingArguments(\n",
    "    per_device_train_batch_size = 8,\n",
    "    gradient_accumulation_steps = 8, # Use GA to mimic batch size!\n",
    "    save_strategy=\"no\",\n",
    "    save_total_limit=0,\n",
    "    warmup_steps = 5,\n",
    "    num_train_epochs = 1, # Set this for 1 full training run.\n",
    "    #max_steps = 60,\n",
    "    learning_rate = 1e-4, # Reduce to 2e-5 for long training runs\n",
    "    logging_steps = 1,\n",
    "    # 32 bits\n",
    "    optim = \"paged_adamw_32bit\",\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    seed = SEED,\n",
    "    report_to = \"none\", # Use this for WandB etc\n",
    "    output_dir=\"../models/qwen3-0.6b-rag-indexer\",\n",
    ")\n",
    "\n",
    "it_config = SFTConfig(\n",
    "    dataset_text_field=\"text\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,        # <-- añade eval batch size\n",
    "    gradient_accumulation_steps=8,\n",
    "    warmup_steps=25,\n",
    "    save_strategy=\"no\",\n",
    "    save_total_limit=0,\n",
    "    eval_steps=1,\n",
    "    eval_strategy=\"steps\",         # <-- activa evaluación periódica\n",
    "    num_train_epochs=1,             # <-- opcional: usa epochs en lugar de max_steps\n",
    "    #max_steps=30,\n",
    "    learning_rate=1e-4,\n",
    "    logging_steps=1,\n",
    "    optim = \"paged_adamw_32bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    seed=SEED,\n",
    "    report_to=\"none\",\n",
    "    output_dir=\"../models/qwen3-0.6b-rag-retriever\",\n",
    "    load_best_model_at_end=False,          # <-- opcional\n",
    "    metric_for_best_model=\"eval_loss\",    # <-- opcional\n",
    "    greater_is_better=False,              # <-- opcional\n",
    ")\n",
    "\n",
    "trainer_auto = UnslothTrainer(\n",
    "    model=model,\n",
    "    train_dataset=knowledge_dataset_tokenizer,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    args=auto_config,\n",
    ")\n",
    "\n",
    "trainer_it = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=qa_train_dataset_tokenizer,\n",
    "    eval_dataset=qa_val_dataset_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    args=it_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b15ec644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 360,710,144 || all params: 1,596,524,544 || trainable%: 22.5935\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3c9a8816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmiguel_kjh\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/miguel/projects/rag-experiments/notebooks/wandb/run-20251107_153358-lkjo9fvq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/miguel_kjh/qa_iterativo_agenda/runs/lkjo9fvq' target=\"_blank\">meta-llama_Llama-3.2-1B-instruct_r512_qa_iterativo_agenda</a></strong> to <a href='https://wandb.ai/miguel_kjh/qa_iterativo_agenda' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/miguel_kjh/qa_iterativo_agenda' target=\"_blank\">https://wandb.ai/miguel_kjh/qa_iterativo_agenda</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/miguel_kjh/qa_iterativo_agenda/runs/lkjo9fvq' target=\"_blank\">https://wandb.ai/miguel_kjh/qa_iterativo_agenda/runs/lkjo9fvq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/miguel_kjh/qa_iterativo_agenda/runs/lkjo9fvq?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7a8f0e0b9b70>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "name = f\"{model_name.replace('/', '_')}_r{RANK}_qa_iterativo_agenda\"\n",
    "\n",
    "# Inicia la sesión de wandb\n",
    "wandb.init(\n",
    "    project=\"qa_iterativo_agenda\",\n",
    "    name=name,  # opcional\n",
    "    config={\n",
    "        \"super_epochs\": 10,\n",
    "        \"model\": model_name,\n",
    "        \"r\": RANK,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f708de6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SUPER EPOCH 1 / 10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 400 | Num Epochs = 1 | Total steps = 7\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 8 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 360,710,144 of 1,596,524,544 (22.59% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:12, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.456800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.408200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.169100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.797400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.203600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.899700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.934600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,700 | Num Epochs = 1 | Total steps = 27\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 8 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 360,710,144 of 1,596,524,544 (22.59% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 02:19, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.493000</td>\n",
       "      <td>6.481918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.432200</td>\n",
       "      <td>5.049185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.053000</td>\n",
       "      <td>3.767687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.729300</td>\n",
       "      <td>2.774565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.753400</td>\n",
       "      <td>2.141159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.138400</td>\n",
       "      <td>1.656620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.621900</td>\n",
       "      <td>1.270533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.258000</td>\n",
       "      <td>0.923814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.961500</td>\n",
       "      <td>0.790050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.813000</td>\n",
       "      <td>0.770230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.772600</td>\n",
       "      <td>0.742359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.728300</td>\n",
       "      <td>0.686664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.699000</td>\n",
       "      <td>0.658338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.654000</td>\n",
       "      <td>0.643647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.646400</td>\n",
       "      <td>0.624242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.618100</td>\n",
       "      <td>0.602094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.598389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.611700</td>\n",
       "      <td>0.568133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.571900</td>\n",
       "      <td>0.545663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.529200</td>\n",
       "      <td>0.500099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.499900</td>\n",
       "      <td>0.420605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.427000</td>\n",
       "      <td>0.423137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.426400</td>\n",
       "      <td>0.412115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.414700</td>\n",
       "      <td>0.387747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.385100</td>\n",
       "      <td>0.396508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.396000</td>\n",
       "      <td>0.401424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.396000</td>\n",
       "      <td>0.398552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 300/300 [01:45<00:00,  2.84it/s, acc=0.00 %, lev=8.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy final: 0/300 = 0.00 %\n",
      "Levenshtein final: 8.94\n",
      "--- SUPER EPOCH 2 / 10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 400 | Num Epochs = 1 | Total steps = 7\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 8 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 360,710,144 of 1,596,524,544 (22.59% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:11, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.906400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.874200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.318200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.182300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.062200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.893700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.633200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,700 | Num Epochs = 1 | Total steps = 27\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 8 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 360,710,144 of 1,596,524,544 (22.59% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 02:20, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.652900</td>\n",
       "      <td>0.652781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.649000</td>\n",
       "      <td>0.535476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.539400</td>\n",
       "      <td>0.504961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.497900</td>\n",
       "      <td>0.493852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.486631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.484400</td>\n",
       "      <td>0.465859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.465000</td>\n",
       "      <td>0.452868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.449800</td>\n",
       "      <td>0.440305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.442700</td>\n",
       "      <td>0.421363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.426600</td>\n",
       "      <td>0.398265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.396200</td>\n",
       "      <td>0.381019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.387300</td>\n",
       "      <td>0.376989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.379400</td>\n",
       "      <td>0.372565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.371800</td>\n",
       "      <td>0.370958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.369900</td>\n",
       "      <td>0.372236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.364500</td>\n",
       "      <td>0.375818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.362500</td>\n",
       "      <td>0.376920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.374000</td>\n",
       "      <td>0.373394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.364400</td>\n",
       "      <td>0.374361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.351400</td>\n",
       "      <td>0.377271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.373900</td>\n",
       "      <td>0.375736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.376000</td>\n",
       "      <td>0.368532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.367300</td>\n",
       "      <td>0.367782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.363200</td>\n",
       "      <td>0.371311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.361500</td>\n",
       "      <td>0.372073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.369000</td>\n",
       "      <td>0.368982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.359000</td>\n",
       "      <td>0.365806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 300/300 [01:45<00:00,  2.84it/s, acc=0.00 %, lev=8.74]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy final: 0/300 = 0.00 %\n",
      "Levenshtein final: 8.74\n",
      "--- SUPER EPOCH 3 / 10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 400 | Num Epochs = 1 | Total steps = 7\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 8 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 360,710,144 of 1,596,524,544 (22.59% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:11, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.033200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.999700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.847800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.632800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.364500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.133100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.658300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,700 | Num Epochs = 1 | Total steps = 27\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 8 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 360,710,144 of 1,596,524,544 (22.59% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 02:19, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.378700</td>\n",
       "      <td>0.382535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.383000</td>\n",
       "      <td>0.380432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.384500</td>\n",
       "      <td>0.375038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.364100</td>\n",
       "      <td>0.367480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.364900</td>\n",
       "      <td>0.359639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.367300</td>\n",
       "      <td>0.354091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.350400</td>\n",
       "      <td>0.348205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.340600</td>\n",
       "      <td>0.339748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.343200</td>\n",
       "      <td>0.330422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.338200</td>\n",
       "      <td>0.323480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.321500</td>\n",
       "      <td>0.320244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.318200</td>\n",
       "      <td>0.314696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.314500</td>\n",
       "      <td>0.307559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.295100</td>\n",
       "      <td>0.304016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.293500</td>\n",
       "      <td>0.302015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.277600</td>\n",
       "      <td>0.297558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.278800</td>\n",
       "      <td>0.290660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>0.285795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.280200</td>\n",
       "      <td>0.283582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.249600</td>\n",
       "      <td>0.283261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.268900</td>\n",
       "      <td>0.280109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.259000</td>\n",
       "      <td>0.272220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.266200</td>\n",
       "      <td>0.269252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.266400</td>\n",
       "      <td>0.289023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.271100</td>\n",
       "      <td>0.267934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.259000</td>\n",
       "      <td>0.276264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.268100</td>\n",
       "      <td>0.261847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 300/300 [01:54<00:00,  2.62it/s, acc=0.00 %, lev=8.72]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy final: 0/300 = 0.00 %\n",
      "Levenshtein final: 8.72\n",
      "--- SUPER EPOCH 4 / 10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 400 | Num Epochs = 1 | Total steps = 7\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 8 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 360,710,144 of 1,596,524,544 (22.59% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:11, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.935400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.946100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.812700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.563700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.313600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.044300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.778200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,700 | Num Epochs = 1 | Total steps = 27\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 8 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 360,710,144 of 1,596,524,544 (22.59% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 02:18, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.335300</td>\n",
       "      <td>0.343067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.337300</td>\n",
       "      <td>0.341490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.336700</td>\n",
       "      <td>0.331347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.324600</td>\n",
       "      <td>0.320288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.313400</td>\n",
       "      <td>0.304715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.292200</td>\n",
       "      <td>0.285247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.280200</td>\n",
       "      <td>0.264224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.254900</td>\n",
       "      <td>0.247784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.243200</td>\n",
       "      <td>0.238979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.234900</td>\n",
       "      <td>0.236358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.230800</td>\n",
       "      <td>0.235356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.231800</td>\n",
       "      <td>0.234909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.230800</td>\n",
       "      <td>0.236309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.233900</td>\n",
       "      <td>0.233130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.235900</td>\n",
       "      <td>0.229804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.220900</td>\n",
       "      <td>0.229023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.218200</td>\n",
       "      <td>0.231743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.228300</td>\n",
       "      <td>0.236013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.231200</td>\n",
       "      <td>0.237009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.215400</td>\n",
       "      <td>0.237869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.228300</td>\n",
       "      <td>0.240083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.221500</td>\n",
       "      <td>0.241849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.231000</td>\n",
       "      <td>0.241494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.228300</td>\n",
       "      <td>0.242333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.233100</td>\n",
       "      <td>0.241542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.232700</td>\n",
       "      <td>0.242069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.235300</td>\n",
       "      <td>0.240290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 300/300 [01:55<00:00,  2.59it/s, acc=0.33 %, lev=8.57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy final: 1/300 = 0.33 %\n",
      "Levenshtein final: 8.57\n",
      "--- SUPER EPOCH 5 / 10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 400 | Num Epochs = 1 | Total steps = 7\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 8 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 360,710,144 of 1,596,524,544 (22.59% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:11, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.366300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.374800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.288300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.116600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.965000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.740900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.239000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,700 | Num Epochs = 1 | Total steps = 27\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 8 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 360,710,144 of 1,596,524,544 (22.59% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 02:19, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.230400</td>\n",
       "      <td>0.234299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.229400</td>\n",
       "      <td>0.233964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.226000</td>\n",
       "      <td>0.231851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.224800</td>\n",
       "      <td>0.230348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.226900</td>\n",
       "      <td>0.227813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.219000</td>\n",
       "      <td>0.225956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.221400</td>\n",
       "      <td>0.224794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.214700</td>\n",
       "      <td>0.223081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.216100</td>\n",
       "      <td>0.222205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.218000</td>\n",
       "      <td>0.221702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.212900</td>\n",
       "      <td>0.220159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.209600</td>\n",
       "      <td>0.219304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.205600</td>\n",
       "      <td>0.220170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.211400</td>\n",
       "      <td>0.218383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.207300</td>\n",
       "      <td>0.217785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.198900</td>\n",
       "      <td>0.221026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.224461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.202600</td>\n",
       "      <td>0.226692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.214700</td>\n",
       "      <td>0.226366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.196900</td>\n",
       "      <td>0.226752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.206400</td>\n",
       "      <td>0.224764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.222836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.206500</td>\n",
       "      <td>0.221675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.208500</td>\n",
       "      <td>0.221511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.204000</td>\n",
       "      <td>0.221651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.205900</td>\n",
       "      <td>0.220416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.211600</td>\n",
       "      <td>0.219522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 300/300 [01:43<00:00,  2.91it/s, acc=9.33 %, lev=8.02] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy final: 28/300 = 9.33 %\n",
      "Levenshtein final: 8.02\n",
      "--- SUPER EPOCH 6 / 10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 400 | Num Epochs = 1 | Total steps = 7\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 8 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 360,710,144 of 1,596,524,544 (22.59% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:11, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.673500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.673700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.571700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.439100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.294200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.149100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.753200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,700 | Num Epochs = 1 | Total steps = 27\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 8 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 360,710,144 of 1,596,524,544 (22.59% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 02:20, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.190200</td>\n",
       "      <td>0.203788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.197200</td>\n",
       "      <td>0.202827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.197100</td>\n",
       "      <td>0.200821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.190100</td>\n",
       "      <td>0.198634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.189400</td>\n",
       "      <td>0.196192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.187700</td>\n",
       "      <td>0.193983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.180700</td>\n",
       "      <td>0.192829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.177900</td>\n",
       "      <td>0.192507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.180400</td>\n",
       "      <td>0.191928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.176800</td>\n",
       "      <td>0.191075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.168200</td>\n",
       "      <td>0.192314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.168300</td>\n",
       "      <td>0.194161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.165000</td>\n",
       "      <td>0.194213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.165000</td>\n",
       "      <td>0.194040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.174000</td>\n",
       "      <td>0.190851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.163900</td>\n",
       "      <td>0.190848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.169600</td>\n",
       "      <td>0.191254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.158700</td>\n",
       "      <td>0.191026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.171900</td>\n",
       "      <td>0.189341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.160400</td>\n",
       "      <td>0.190524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.165800</td>\n",
       "      <td>0.192553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.164500</td>\n",
       "      <td>0.193646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.167700</td>\n",
       "      <td>0.194261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.176500</td>\n",
       "      <td>0.192478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.174300</td>\n",
       "      <td>0.191259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.173100</td>\n",
       "      <td>0.190301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.177900</td>\n",
       "      <td>0.190223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 300/300 [01:44<00:00,  2.87it/s, acc=37.67 %, lev=5.26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy final: 113/300 = 37.67 %\n",
      "Levenshtein final: 5.26\n",
      "--- SUPER EPOCH 7 / 10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 400 | Num Epochs = 1 | Total steps = 7\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 8 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 360,710,144 of 1,596,524,544 (22.59% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:11, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.977300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.972700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.879800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.761400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.634800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.512200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.359500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,700 | Num Epochs = 1 | Total steps = 27\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 8 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 360,710,144 of 1,596,524,544 (22.59% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 02:19, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.275300</td>\n",
       "      <td>0.284888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.277800</td>\n",
       "      <td>0.284657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.270600</td>\n",
       "      <td>0.279966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.271600</td>\n",
       "      <td>0.270288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.262900</td>\n",
       "      <td>0.258669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.241200</td>\n",
       "      <td>0.240801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.230300</td>\n",
       "      <td>0.221548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.207900</td>\n",
       "      <td>0.200535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.182200</td>\n",
       "      <td>0.184468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.168700</td>\n",
       "      <td>0.174899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.154600</td>\n",
       "      <td>0.169757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.147800</td>\n",
       "      <td>0.167715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.146300</td>\n",
       "      <td>0.167940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.145900</td>\n",
       "      <td>0.168565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.146100</td>\n",
       "      <td>0.169433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.142700</td>\n",
       "      <td>0.169974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.147600</td>\n",
       "      <td>0.169426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.140500</td>\n",
       "      <td>0.170393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.149300</td>\n",
       "      <td>0.170020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.137100</td>\n",
       "      <td>0.171080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.146300</td>\n",
       "      <td>0.170883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.148200</td>\n",
       "      <td>0.170222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.148600</td>\n",
       "      <td>0.169312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.151900</td>\n",
       "      <td>0.169727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.149300</td>\n",
       "      <td>0.172913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.156400</td>\n",
       "      <td>0.173500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.157200</td>\n",
       "      <td>0.172153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 300/300 [01:53<00:00,  2.64it/s, acc=70.33 %, lev=2.70]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy final: 211/300 = 70.33 %\n",
      "Levenshtein final: 2.70\n",
      "--- SUPER EPOCH 8 / 10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 400 | Num Epochs = 1 | Total steps = 7\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 8 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 360,710,144 of 1,596,524,544 (22.59% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:11, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.712800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.719800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.636900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.559000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.452300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.333400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.007200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,700 | Num Epochs = 1 | Total steps = 27\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 8 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 360,710,144 of 1,596,524,544 (22.59% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 02:19, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.156600</td>\n",
       "      <td>0.169831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.169468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.156100</td>\n",
       "      <td>0.168208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.158400</td>\n",
       "      <td>0.165877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.148900</td>\n",
       "      <td>0.164060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.144200</td>\n",
       "      <td>0.162686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.149600</td>\n",
       "      <td>0.161607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.147300</td>\n",
       "      <td>0.160549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.145800</td>\n",
       "      <td>0.160460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.144600</td>\n",
       "      <td>0.160870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.143200</td>\n",
       "      <td>0.160432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.141200</td>\n",
       "      <td>0.160536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.139500</td>\n",
       "      <td>0.160632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.160902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.140200</td>\n",
       "      <td>0.161265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.136300</td>\n",
       "      <td>0.162161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.138200</td>\n",
       "      <td>0.163325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.139100</td>\n",
       "      <td>0.164643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.146400</td>\n",
       "      <td>0.164363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.137100</td>\n",
       "      <td>0.163927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.139100</td>\n",
       "      <td>0.163661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.139000</td>\n",
       "      <td>0.163376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.140900</td>\n",
       "      <td>0.164409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.143200</td>\n",
       "      <td>0.166465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.148700</td>\n",
       "      <td>0.164923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.148300</td>\n",
       "      <td>0.167483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.146600</td>\n",
       "      <td>0.168172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 300/300 [01:52<00:00,  2.68it/s, acc=88.33 %, lev=1.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy final: 265/300 = 88.33 %\n",
      "Levenshtein final: 1.06\n",
      "--- SUPER EPOCH 9 / 10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 400 | Num Epochs = 1 | Total steps = 7\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 8 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 360,710,144 of 1,596,524,544 (22.59% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:11, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.323200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.328500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.254800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.165300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.045200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.926000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.719700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,700 | Num Epochs = 1 | Total steps = 27\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 8 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 360,710,144 of 1,596,524,544 (22.59% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 02:19, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.147900</td>\n",
       "      <td>0.163098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.147000</td>\n",
       "      <td>0.163057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.145900</td>\n",
       "      <td>0.162494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.146400</td>\n",
       "      <td>0.161635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.145000</td>\n",
       "      <td>0.160619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.141400</td>\n",
       "      <td>0.160314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.143000</td>\n",
       "      <td>0.159596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.143100</td>\n",
       "      <td>0.159110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.141600</td>\n",
       "      <td>0.158497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.140600</td>\n",
       "      <td>0.159410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.138900</td>\n",
       "      <td>0.160804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.138500</td>\n",
       "      <td>0.160402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.136100</td>\n",
       "      <td>0.160215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.135100</td>\n",
       "      <td>0.160651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.137600</td>\n",
       "      <td>0.161618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.133000</td>\n",
       "      <td>0.162342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.135700</td>\n",
       "      <td>0.162114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.135300</td>\n",
       "      <td>0.162908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.137400</td>\n",
       "      <td>0.163533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.129500</td>\n",
       "      <td>0.163414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.135000</td>\n",
       "      <td>0.164267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.133500</td>\n",
       "      <td>0.166533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.137900</td>\n",
       "      <td>0.168597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.143200</td>\n",
       "      <td>0.169534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.146100</td>\n",
       "      <td>0.169311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.142700</td>\n",
       "      <td>0.171308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.151000</td>\n",
       "      <td>0.170784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 300/300 [01:45<00:00,  2.85it/s, acc=91.67 %, lev=0.73]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy final: 275/300 = 91.67 %\n",
      "Levenshtein final: 0.73\n",
      "--- SUPER EPOCH 10 / 10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 400 | Num Epochs = 1 | Total steps = 7\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 8 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 360,710,144 of 1,596,524,544 (22.59% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:11, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.855600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.836600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.779200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.692600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.590300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.486300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.367300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,700 | Num Epochs = 1 | Total steps = 27\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 8 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 360,710,144 of 1,596,524,544 (22.59% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 02:19, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.239000</td>\n",
       "      <td>0.256639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.235900</td>\n",
       "      <td>0.254685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.235800</td>\n",
       "      <td>0.251143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.230400</td>\n",
       "      <td>0.242479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.219400</td>\n",
       "      <td>0.232565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.207300</td>\n",
       "      <td>0.218279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.196000</td>\n",
       "      <td>0.202223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.181400</td>\n",
       "      <td>0.184405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.163000</td>\n",
       "      <td>0.171183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>0.162905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.140100</td>\n",
       "      <td>0.160373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.136300</td>\n",
       "      <td>0.159523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.134300</td>\n",
       "      <td>0.158831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.133100</td>\n",
       "      <td>0.159281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.133000</td>\n",
       "      <td>0.160562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.131200</td>\n",
       "      <td>0.162796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.134100</td>\n",
       "      <td>0.162004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.133100</td>\n",
       "      <td>0.162252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.134900</td>\n",
       "      <td>0.162820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.127400</td>\n",
       "      <td>0.162657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.130200</td>\n",
       "      <td>0.163413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.131500</td>\n",
       "      <td>0.164234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.137200</td>\n",
       "      <td>0.165034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.130500</td>\n",
       "      <td>0.169169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.129900</td>\n",
       "      <td>0.173732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.140200</td>\n",
       "      <td>0.174451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.123000</td>\n",
       "      <td>0.175420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 300/300 [01:44<00:00,  2.86it/s, acc=95.33 %, lev=0.50]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy final: 286/300 = 95.33 %\n",
      "Levenshtein final: 0.50\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Control-C detected -- Run data was not synced\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 17\u001b[0m\n\u001b[1;32m      8\u001b[0m     acc, lev \u001b[38;5;241m=\u001b[39m test_model_accuracy(model, tokenizer, prompts_retrieval_test, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlog({\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuper_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: _ \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: acc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss_it\u001b[39m\u001b[38;5;124m\"\u001b[39m: trainer_it_stats\u001b[38;5;241m.\u001b[39mtraining_loss,\n\u001b[1;32m     15\u001b[0m     })\n\u001b[0;32m---> 17\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinish\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:4102\u001b[0m, in \u001b[0;36mfinish\u001b[0;34m(exit_code, quiet)\u001b[0m\n\u001b[1;32m   4085\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Finish a run and upload any remaining data.\u001b[39;00m\n\u001b[1;32m   4086\u001b[0m \n\u001b[1;32m   4087\u001b[0m \u001b[38;5;124;03mMarks the completion of a W&B run and ensures all data is synced to the server.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4099\u001b[0m \u001b[38;5;124;03m    quiet: Deprecated. Configure logging verbosity using `wandb.Settings(quiet=...)`.\u001b[39;00m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39mrun:\n\u001b[0;32m-> 4102\u001b[0m     \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexit_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexit_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:399\u001b[0m, in \u001b[0;36m_log_to_run.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m     run_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attach_id\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m wb_logging\u001b[38;5;241m.\u001b[39mlog_to_run(run_id):\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:444\u001b[0m, in \u001b[0;36m_attach.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m         _is_attaching \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 444\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:2260\u001b[0m, in \u001b[0;36mRun.finish\u001b[0;34m(self, exit_code, quiet)\u001b[0m\n\u001b[1;32m   2251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m quiet \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2252\u001b[0m     deprecate\u001b[38;5;241m.\u001b[39mdeprecate(\n\u001b[1;32m   2253\u001b[0m         field_name\u001b[38;5;241m=\u001b[39mDeprecated\u001b[38;5;241m.\u001b[39mrun__finish_quiet,\n\u001b[1;32m   2254\u001b[0m         warning_message\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2258\u001b[0m         run\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2259\u001b[0m     )\n\u001b[0;32m-> 2260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexit_code\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:399\u001b[0m, in \u001b[0;36m_log_to_run.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m     run_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attach_id\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m wb_logging\u001b[38;5;241m.\u001b[39mlog_to_run(run_id):\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:2288\u001b[0m, in \u001b[0;36mRun._finish\u001b[0;34m(self, exit_code)\u001b[0m\n\u001b[1;32m   2285\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wl\u001b[38;5;241m.\u001b[39mremove_active_run(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   2287\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2288\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_atexit_cleanup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexit_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexit_code\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2290\u001b[0m     \u001b[38;5;66;03m# Run hooks that should happen after the last messages to the\u001b[39;00m\n\u001b[1;32m   2291\u001b[0m     \u001b[38;5;66;03m# internal service, like detaching the logger.\u001b[39;00m\n\u001b[1;32m   2292\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_teardown_hooks:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:2483\u001b[0m, in \u001b[0;36mRun._atexit_cleanup\u001b[0;34m(self, exit_code)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         os\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_settings\u001b[38;5;241m.\u001b[39mresume_fname)\n\u001b[1;32m   2482\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2483\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2485\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39mwandb_agent\u001b[38;5;241m.\u001b[39m_is_running():  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:2737\u001b[0m, in \u001b[0;36mRun._on_finish\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2730\u001b[0m     exit_handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39minterface\u001b[38;5;241m.\u001b[39mdeliver_finish_without_exit()\n\u001b[1;32m   2732\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m progress\u001b[38;5;241m.\u001b[39mprogress_printer(\n\u001b[1;32m   2733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printer,\n\u001b[1;32m   2734\u001b[0m     default_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinishing up...\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2735\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m progress_printer:\n\u001b[1;32m   2736\u001b[0m     \u001b[38;5;66;03m# Wait for the run to complete.\u001b[39;00m\n\u001b[0;32m-> 2737\u001b[0m     \u001b[43mwait_with_progress\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2738\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexit_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2740\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisplay_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunctools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2741\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_display_finish_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2742\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_printer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2743\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2744\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2746\u001b[0m poll_exit_handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39minterface\u001b[38;5;241m.\u001b[39mdeliver_poll_exit()\n\u001b[1;32m   2747\u001b[0m result \u001b[38;5;241m=\u001b[39m poll_exit_handle\u001b[38;5;241m.\u001b[39mwait_or(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/mailbox/wait_with_progress.py:23\u001b[0m, in \u001b[0;36mwait_with_progress\u001b[0;34m(handle, timeout, display_progress)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait_with_progress\u001b[39m(\n\u001b[1;32m     14\u001b[0m     handle: MailboxHandle[_T],\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m     16\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     17\u001b[0m     display_progress: Callable[[], Coroutine[Any, Any, \u001b[38;5;28;01mNone\u001b[39;00m]],\n\u001b[1;32m     18\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _T:\n\u001b[1;32m     19\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wait for a handle, possibly displaying progress to the user.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m    Equivalent to passing a single handle to `wait_all_with_progress`.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwait_all_with_progress\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisplay_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/mailbox/wait_with_progress.py:77\u001b[0m, in \u001b[0;36mwait_all_with_progress\u001b[0;34m(handle_list, timeout, display_progress)\u001b[0m\n\u001b[1;32m     70\u001b[0m             remaining_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _wait_handles_async(\n\u001b[1;32m     73\u001b[0m             handle_list,\n\u001b[1;32m     74\u001b[0m             timeout\u001b[38;5;241m=\u001b[39mremaining_timeout,\n\u001b[1;32m     75\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprogress_loop_with_timeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/lib/asyncio_manager.py:136\u001b[0m, in \u001b[0;36mAsyncioManager.run\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    133\u001b[0m future \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schedule(fn, daemon\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mCancelledError:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RunCancelledError \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for _ in range(EPOCHS):\n",
    "    print(f\"--- SUPER EPOCH {_+1} / {EPOCHS} ---\")\n",
    "    trainer_sft_stats = trainer_auto.train() \n",
    "    trainer_it_stats = trainer_it.train()\n",
    "    # evaluate accuracy after each super epoch\n",
    "    acc, lev = test_model_accuracy(model, tokenizer, prompts_retrieval_test, device=\"cuda\")\n",
    "    wandb.log({\n",
    "        \"super_epoch\": _ + 1,\n",
    "        \"accuracy\": acc,\n",
    "        \"levenshtein\": lev,\n",
    "        \"train_loss_sft\": trainer_sft_stats.training_loss,\n",
    "        \"train_loss_it\": trainer_it_stats.training_loss,\n",
    "    })\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47228fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El número 736 615 948 pertenece a Leo Pérez.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# test the model in streaming mode\n",
    "from transformers import TextStreamer\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=False)\n",
    "_ = model.generate(\n",
    "    **tokenizer(text_for_testing, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 64, # Increase for longer outputs!\n",
    "    do_sample = False,\n",
    "    top_p = 0.1,\n",
    "    temperature = 0.,\n",
    "    streamer = streamer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a9e347",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 300/300 [01:45<00:00,  2.84it/s, acc=91.00 %, lev=0.80]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy final: 273/300 = 91.00 %\n",
      "Levenshtein final: 241/300 = 0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "acc, lev = test_model_accuracy(model, tokenizer, prompts_retrieval_test, device=\"cuda\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
