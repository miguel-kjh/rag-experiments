{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "473b050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_from_disk\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import torch\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9d8b356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.9.6: Fast Qwen3 patching. Transformers: 4.55.4. vLLM: 0.10.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070 SUPER. Num GPUs = 1. Max memory: 11.994 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = 8192,\n",
    "    load_in_4bit = False,\n",
    "    load_in_8bit = False,\n",
    ")\n",
    "RANK = 64\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = RANK,           # Choose any number > 0! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = RANK*2,  # Best to choose alpha = rank or rank*2\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = SEED,\n",
    "    use_rslora = False,   # We support rank stabilized LoRA\n",
    "    loftq_config = None,  # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b956a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    ")\n",
    "\n",
    "db = FAISS.load_local(\n",
    "    \"../data/db/parliament_db/parliament_all_docs_embeddings_sentence-transformers_paraphrase-multilingual-mpnet-base-v2\",\n",
    "    embedding_model,\n",
    "    allow_dangerous_deserialization=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08fbfd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 11162\n"
     ]
    }
   ],
   "source": [
    "#quiero la lista de documentos\n",
    "docs = db.docstore._dict.values()\n",
    "documents = list(docs)\n",
    "print(f\"Number of documents: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbda5e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_AUTORE = \"../data/processed/parliament_qa\"\n",
    "dataset = load_from_disk(FOLDER_AUTORE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1efe551",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5126e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompt_for_indexing(documents: list):\n",
    "    prompt = \"\"\"\n",
    "    Este documento tiene el DOCID:{doc_id}.\n",
    "    Contenido del documento:\n",
    "    {doc}\n",
    "    \"\"\"\n",
    "    for doc in documents:\n",
    "        document = doc.page_content\n",
    "        doc_id = doc.metadata.get(\"id\", \"unknown\")\n",
    "        yield prompt.format(doc=document, doc_id=doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1f36a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_it(tokenizer, system_prompt: str, prompt: str) -> str:\n",
    "    \"\"\"Builds the chat prompt for a single example using the tokenizer chat template.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\",   \"content\": prompt},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d4d4258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompts_for_retrieval(dataset, tokenizer):\n",
    "    system_prompt = \"\"\"Eres un módulo de recuperación. Tu única tarea es devolver el identificador del documento correspondiente a la consulta dada.\n",
    "Sigue estrictamente estas reglas:\n",
    "1) Devuelve EXACTAMENTE una línea con el formato: DOCID:{<id>}.\n",
    "2) No incluyas palabras, explicaciones o puntuación extra antes o después de las llaves.\n",
    "3) Si múltiples documentos son plausibles, elige el mejor ID.\n",
    "4) Nunca inventes un ID fuera del espacio permitido. Mantente dentro de los prefijos válidos.\n",
    "5) No respondas a la pregunta; solo devuelve el docid.\"\n",
    "\"\"\"\n",
    "    prompts = []\n",
    "    for item in dataset:\n",
    "        prompt = \"\"\"\n",
    "        Dada la siguiente consulta, recupera los identificadores de los documentos relevantes. \n",
    "        Consulta: {QUERY}\n",
    "        Proporciona los identificadores de los documentos en el siguiente formato, uno por línea:\n",
    "        \"\"\"\n",
    "        docid_prompt = \"DOCID:{docid}\"\n",
    "        question = item[\"question\"]\n",
    "        prompt = prompt.format(QUERY=question)\n",
    "        prompt += docid_prompt.format(docid=item[\"id\"]) + \"\\n\"\n",
    "        prompts.append(build_prompt_it(tokenizer, system_prompt, prompt))\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b744a024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prompts: 11162\n"
     ]
    }
   ],
   "source": [
    "prompts = list(prepare_prompt_for_indexing(documents))\n",
    "print(f\"Number of prompts: {len(prompts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed65b3b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 11162\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataset from prompts\n",
    "from datasets import Dataset\n",
    "indexing_dataset = Dataset.from_dict({\"text\": prompts})\n",
    "indexing_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cda96468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Este documento tiene el DOCID:6596_4.\\n    Contenido del documento:\\n    Esta sesión del parlamento se realizó el 2024-05-07. 11L/PO/P-0750 PREGUNTA DEL SEÑOR DIPUTADO DON NICASIO JESÚS GALVÁN SASIA, DEL GRUPO PARLAMENTARIO VOX, SOBRE MEDIDAS QUE SE VAN A LLEVAR A CABO PARA DEMOCRATIZAR Y REDISTRIBUIR LA RIQUEZA DEL SECTOR TURÍSTICO, DIRIGIDA A LA PRESIDENCIA DEL GOBIERNO La señora PRESIDENTA: Siguiente pregunta, del señor diputado don Nicasio Galván Sasia, del Grupo Parlamentario VOX, sobre medidas que se van a llevar a cabo para democratizar y redistribuir la riqueza del sector turístico, dirigida al señor presidente del Gobierno. Cuando quiera. El señor GALVÁN SASIA (desde su escaño): Buenos días, señor Clavijo, buenos días. Escuchándole en la rueda de prensa posterior a la Conferencia de Presidentes nos han surgido varias preguntas, y nos consta que no solo a nosotros. Se le oía escuchar hablar de la democratización y la redistribución de la riqueza del sector turístico y culpabilizó al turismo -sí, a esas 500 000 personas que han venido a vivir a Canarias en los últimos veinte años- de la presión de los servicios públicos y de la demanda de vivienda. No le escuchamos hablar de esas 200 000 viviendas que hay vacías en nuestras islas. Habló también del reto demográfico, pero desde un punto de vista neomalthusiano, de escasez y agotamiento de los recursos por causa del incremento de la población. Esas tesis, señor Clavijo, se han demostrado erróneas, porque no han tenido en cuenta la capacidad de inventiva ni innovación del ser humano. Desde nuestro punto de vista, se demuestra se demuestra que no hay agenda clara de gobierno, que parece que ustedes pretenden gobernar a base de titular de prensa y como reacción a las manifestaciones. Les recuerdo, también, que desde la tribuna de este Parlamento en el pasado pleno su grupo parlamentario faltó al respeto a las universidades canarias y señaló directamente a una empresa privada, yo creo que atacando al sector económico más importante de Canarias, a nuestras universidades y a las empresas privadas, no vamos a lograr incrementar el producto interior bruto canario ni a lograr subir el salario medio ni tampoco mejorar la investigación, desarrollo e innovación tecnológica, tampoco enviaremos un mensaje sobre seguridad jurídica y, por supuesto, no atraeremos inversión nacional e internacional. Recordarle también que el año pasado se incrementó la burocracia en Canarias cerca de un catorce por ciento. Señor Clavijo, así no. Nosotros creemos que así no se van a revertir esos datos de pobreza y de desempleo que tenemos en Canarias, no se van a generar ese ahorro y esa inversión que necesitamos para alcanzar la tan ansiada diversificación económica y productividad en nuestras islas. Señor Clavijo, comentarle que el lenguaje es muy importante a la hora de trasladar mensajes al mercado, sobre todo de un presidente de comunidad autónoma. Y créame si le digo, por favor, créame que hablar de democratizar y redistribuir la riqueza del sector turístico no ayuda, no ayuda, sino causa -recordemos que el sector turístico es iniciativa privada-, no ayuda, sino causa el efecto contrario de lo que se pretende, que espero que en este caso sea el crecimiento de Canarias y la riqueza de nuestras islas, por supuesto, respetando el medio ambiente. Muchas gracias. La señora PRESIDENTA: Señor presidente, tiene la palabra. El señor PRESIDENTE DE CANARIAS (Clavijo Batlle) (desde su escaño): Gracias, señora presidenta. Señor Galván, créame, créame que, desde luego, las tesis que salieron de la Conferencia de Presidentes desde luego no tiene nada que ver con el ser neomalthusianos o..., porque todo lo contrario, lo que se habla es de regular nuestros recursos naturales para que no se agoten y para que puedan disfrutarlos, ¿no?, porque el paisaje, la naturaleza, si la protegemos y la cuidamos, seguirán viniendo. Ahora, si hacemos un uso desmedido y agotamos esos recursos o los antropizamos, es decir, lo consumimos y lo transformamos por la acción del hombre, la realidad es que al final dejaremos de ser unas islas afortunadas y la gente se irá a otro sitio. Pero, mire, este Gobierno ha heredado una Canarias más injusta, más insolidaria, nunca antes los ricos en Canarias, después del pacto de las flores, han sido tan ricos y los pobres tan pobres. Y cuando hablamos de democratizar, cuando hablamos de distribuir la riqueza lo que este Gobierno quiere decir, y si lo ha llevado a confusión le pido disculpas, yo no soy un gran orador, pero sí lo que le puedo decir es que lo que buscamos es que de los 22 000 millones de euros de facturación del sector turístico se quede la mayor cantidad de dinero aquí. Hablamos de que las familias puedan llegar a final de mes, hablamos de mejorar la productividad, por supuesto que sí, porque en la medida que mejoremos la productividad seremos más competitivos, pero eso tiene que ir de la mano con las mejoras salariales, porque no nos resignamos a que en Canarias tengamos los salarios más bajos de toda España, no nos resignamos a que tengamos las cotas de exclusión social. Y para eso, y para eso hemos establecido esta Conferencia de Presidentes, para eso vamos a hacer los grupos de trabajo, para eso van a poder participar los sindicatos y los empresarios en las soluciones. Las soluciones, no tenemos una varita mágica, es más, somos muy respetuosos con la concertación social para que patronal y sindicatos se pongan de acuerdo en las retribuciones y en los convenios colectivos; ya se están dando pasos, ya, previsiblemente, antes de que acabe el año se va a sacar adelante un convenio en el sector turístico donde hay mejoras salariales. Eso es lo que perseguimos, ni somos un Gobierno intervencionista, porque ya se ha visto lo que ocurrió con el Gobierno anterior y las consecuencias, ni, desde luego, somos responsables de que la ley... porque votamos en contra de que la ley de vivienda aprobada en Madrid lo que haya hecho es sacar vivienda del mercado porque el propietario tiene miedo de ponerla a alquilar porque no tiene las garantías jurídicas. Créame que ese es el camino, no se confunda. Y, desde luego, en esto espero que nos ayude. La señora PRESIDENTA: Gracias, señor presidente. Tiempo de réplica, señor Galván, cuando quiera. El señor GALVÁN SASIA (desde su escaño): Señor Clavijo, pues le pido que nos invite a nosotros, a este grupo parlamentario, a participar en esas conversaciones de esos grupos, porque creo que se evitarían muchas malas interpretaciones, si es como usted dice, y, de todas maneras, controlaríamos mejor esa acción de gobierno. Muchas gracias. La señora PRESIDENTA: Gracias, señor Galván.\\n    '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexing_dataset[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ee44269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of retrieval prompts: 614\n",
      "Number of retrieval prompts: 161\n"
     ]
    }
   ],
   "source": [
    "prompts_retrieval_train = prepare_prompts_for_retrieval(dataset[\"train\"], tokenizer)\n",
    "prompts_retrieval_val = prepare_prompts_for_retrieval(dataset[\"validation\"], tokenizer)\n",
    "\n",
    "print(f\"Number of retrieval prompts: {len(prompts_retrieval_train)}\")\n",
    "print(f\"Number of retrieval prompts: {len(prompts_retrieval_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ab6c0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nEres un módulo de recuperación. Tu única tarea es devolver el identificador del documento correspondiente a la consulta dada.\\nSigue estrictamente estas reglas:\\n1) Devuelve EXACTAMENTE una línea con el formato: DOCID:{<id>}.\\n2) No incluyas palabras, explicaciones o puntuación extra antes o después de las llaves.\\n3) Si múltiples documentos son plausibles, elige el mejor ID.\\n4) Nunca inventes un ID fuera del espacio permitido. Mantente dentro de los prefijos válidos.\\n5) No respondas a la pregunta; solo devuelve el docid.\"\\n<|im_end|>\\n<|im_start|>user\\n\\n        Dada la siguiente consulta, recupera los identificadores de los documentos relevantes. \\n        Consulta: ¿Qué argumentos presentó el grupo parlamentario que intervino en la sesión del 22 de octubre de 2024, en relación con la propuesta de alteración del orden del día y su impacto en el desarrollo de las comparecencias del Gobierno?\\n        Proporciona los identificadores de los documentos en el siguiente formato, uno por línea:\\n        DOCID:6592_1\\n<|im_end|>\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts_retrieval_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97de98e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset from prompts train, val, test\n",
    "retrieval_train_dataset = Dataset.from_dict({\"text\": prompts_retrieval_train})\n",
    "retrieval_val_dataset = Dataset.from_dict({\"text\": prompts_retrieval_val})\n",
    "\n",
    "retrieval_dataset = {\n",
    "    \"train\": retrieval_train_dataset,\n",
    "    \"validation\": retrieval_val_dataset,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60314e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function_autoregressive(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a0505aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 11162/11162 [00:09<00:00, 1169.20 examples/s]\n"
     ]
    }
   ],
   "source": [
    "indexing_dataset_tokenizer = indexing_dataset.map(tokenize_function_autoregressive, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa230b46",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01d9a429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=32): 100%|██████████| 614/614 [00:04<00:00, 151.56 examples/s]\n",
      "Unsloth: Tokenizing [\"text\"] (num_proc=32): 100%|██████████| 161/161 [00:02<00:00, 62.52 examples/s] \n"
     ]
    }
   ],
   "source": [
    "# sft training\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "auto_config = SFTConfig(\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
    "    save_steps=10,\n",
    "    warmup_steps = 5,\n",
    "    #num_train_epochs = 1, # Set this for 1 full training run.\n",
    "    max_steps = 60,\n",
    "    learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
    "    logging_steps = 1,\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    seed = SEED,\n",
    "    report_to = \"none\", # Use this for WandB etc\n",
    "    output_dir=\"../models/qwen3-0.6b-rag-indexer\",\n",
    ")\n",
    "\n",
    "it_config = SFTConfig(\n",
    "    dataset_text_field=\"text\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,         # <-- añade eval batch size\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=5,\n",
    "    save_steps=5,\n",
    "    eval_steps=5,\n",
    "    eval_strategy=\"steps\",         # <-- activa evaluación periódica\n",
    "    #num_train_epochs=1,             # <-- opcional: usa epochs en lugar de max_steps\n",
    "    max_steps=60,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=1,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    seed=SEED,\n",
    "    report_to=\"none\",\n",
    "    output_dir=\"../models/qwen3-0.6b-rag-retriever\",\n",
    "    load_best_model_at_end=True,          # <-- opcional\n",
    "    metric_for_best_model=\"eval_loss\",    # <-- opcional\n",
    "    greater_is_better=False,              # <-- opcional\n",
    ")\n",
    "\n",
    "trainer_auto = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=indexing_dataset_tokenizer,\n",
    "    tokenizer=tokenizer,\n",
    "    args=auto_config,\n",
    ")\n",
    "\n",
    "trainer_it = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=retrieval_dataset[\"train\"],\n",
    "    eval_dataset=retrieval_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    args=it_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "176683a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4070 SUPER. Max memory = 11.994 GB.\n",
      "2.438 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f708de6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 11,162 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 40,370,176 of 636,420,096 (6.34% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 01:48, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.291200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.097900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.469200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.803700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.408800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.950800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.714700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.699700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.341200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.865200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.506700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>4.956800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.751600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>4.509500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.011300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>4.170700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.741800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.357700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.574500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>4.506200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.129600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.894400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.705000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>3.053400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>3.938600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>3.854500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>4.101400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.269600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>3.425200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>3.305800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>3.366100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>3.819400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>4.057800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>3.754800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>4.446000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2.902300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>3.728500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.246800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>3.091700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>2.823500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>3.728100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>3.278500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>3.013300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>3.853300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>3.591800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>3.868800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>3.218600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.810400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>3.027500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>2.855900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>2.909900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>2.672700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>3.259800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>3.856000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>3.224600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>3.586900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>4.641900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.021000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 614 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 40,370,176 of 636,420,096 (6.34% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 02:27, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.793900</td>\n",
       "      <td>1.377428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.559500</td>\n",
       "      <td>0.542438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.541200</td>\n",
       "      <td>0.479699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.453000</td>\n",
       "      <td>0.448664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.435900</td>\n",
       "      <td>0.431838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.429500</td>\n",
       "      <td>0.417642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.424900</td>\n",
       "      <td>0.410696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.363200</td>\n",
       "      <td>0.402644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.447900</td>\n",
       "      <td>0.396807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.388300</td>\n",
       "      <td>0.393479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.371400</td>\n",
       "      <td>0.391775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.435500</td>\n",
       "      <td>0.391227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen3ForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n",
      "/home/miguel/miniconda3/envs/RAG/lib/python3.11/site-packages/peft/utils/other.py:1228: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 6f49ee65-9246-4860-a8cf-08accb6a571b)') - silently ignoring the lookup for the file config.json in unsloth/Qwen3-0.6B.\n",
      "  warnings.warn(\n",
      "/home/miguel/miniconda3/envs/RAG/lib/python3.11/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in unsloth/Qwen3-0.6B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/miguel/miniconda3/envs/RAG/lib/python3.11/site-packages/peft/utils/other.py:1228: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 27aeeaf9-35d5-40df-9e74-f8f421d3415f)') - silently ignoring the lookup for the file config.json in unsloth/Qwen3-0.6B.\n",
      "  warnings.warn(\n",
      "/home/miguel/miniconda3/envs/RAG/lib/python3.11/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in unsloth/Qwen3-0.6B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 11,162 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 40,370,176 of 636,420,096 (6.34% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 01:46, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.631800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.335800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.693700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.477500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.903300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.517800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.219300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.315600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.997600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.552500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.221600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>4.728500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.917600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.522300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>4.324000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.799900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>4.007200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.571200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.177800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.414800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>4.379900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.959900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.736100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.558300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.674900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.918900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>3.823700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>3.734000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>4.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.138100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>3.314100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>3.186700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>3.256000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>3.718400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>3.961100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>3.650800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>4.365700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2.796200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>3.648100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.156900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>2.990100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>2.734200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>3.650600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>3.193000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>2.914600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>3.780700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>3.510600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>3.792000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>3.123500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.748600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>2.934400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>2.754800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>2.814900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>2.562400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>3.166600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>3.762800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>3.120500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>3.476800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>4.551600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.900400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 614 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 40,370,176 of 636,420,096 (6.34% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 01:59, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.364000</td>\n",
       "      <td>0.416069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.314200</td>\n",
       "      <td>0.403918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.368800</td>\n",
       "      <td>0.404568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.335700</td>\n",
       "      <td>0.400162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.324900</td>\n",
       "      <td>0.390632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.380925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.341200</td>\n",
       "      <td>0.374968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.315400</td>\n",
       "      <td>0.368071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.362400</td>\n",
       "      <td>0.362807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.328600</td>\n",
       "      <td>0.360062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.305400</td>\n",
       "      <td>0.358668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.304100</td>\n",
       "      <td>0.358412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPOCHS = 2\n",
    "for _ in range(EPOCHS):\n",
    "    trainer_sft_stats = trainer_auto.train()\n",
    "    trainer_it_stats = trainer_it.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d6735bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak reserved memory = 5.129 GB.\n",
      "Peak reserved memory for training = 2.691 GB.\n",
      "Peak reserved memory % of max memory = 42.763 %.\n",
      "Peak reserved memory for training % of max memory = 22.436 %.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d51434cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompts_for_testing(dataset, tokenizer):\n",
    "    system_prompt = \"\"\"Eres un módulo de recuperación. Tu única tarea es devolver el identificador del documento correspondiente a la consulta dada.\n",
    "Sigue estrictamente estas reglas:\n",
    "1) Devuelve EXACTAMENTE una línea con el formato: DOCID:{<id>}.\n",
    "2) No incluyas palabras, explicaciones o puntuación extra antes o después de las llaves.\n",
    "3) Si múltiples documentos son plausibles, elige el mejor ID.\n",
    "4) Nunca inventes un ID fuera del espacio permitido. Mantente dentro de los prefijos válidos.\n",
    "5) No respondas a la pregunta; solo devuelve el docid.\"\n",
    "\"\"\"\n",
    "    def build_prompt_it(tokenizer, system_prompt: str, prompt: str) -> str:\n",
    "        \"\"\"Builds the chat prompt for a single example using the tokenizer chat template.\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\",   \"content\": prompt},\n",
    "        ]\n",
    "        return tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False,\n",
    "        )\n",
    "    prompts = []\n",
    "    for item in dataset:\n",
    "        prompt = \"\"\"\n",
    "        Dada la siguiente consulta, recupera los identificadores de los documentos relevantes. \n",
    "        Consulta: {QUERY}\n",
    "        Proporciona los identificadores de los documentos en el siguiente formato, uno por línea:\n",
    "        \"\"\"\n",
    "        question = item[\"question\"]\n",
    "        prompt = prompt.format(QUERY=question)\n",
    "        prompts.append(\n",
    "            ( \n",
    "                build_prompt_it(tokenizer, system_prompt, prompt),\n",
    "                item[\"id\"],\n",
    "            )\n",
    "        )\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4f12665",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_retrieval_test = prepare_prompts_for_testing(dataset[\"test\"], tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ba62589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5402_2\n"
     ]
    }
   ],
   "source": [
    "text = prompts_retrieval_test[0][0]\n",
    "doc_id_targets = prompts_retrieval_test[0][1]\n",
    "print(doc_id_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47228fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "Eres un módulo de recuperación. Tu única tarea es devolver el identificador del documento correspondiente a la consulta dada.\n",
      "Sigue estrictamente estas reglas:\n",
      "1) Devuelve EXACTAMENTE una línea con el formato: DOCID:{<id>}.\n",
      "2) No incluyas palabras, explicaciones o puntuación extra antes o después de las llaves.\n",
      "3) Si múltiples documentos son plausibles, elige el mejor ID.\n",
      "4) Nunca inventes un ID fuera del espacio permitido. Mantente dentro de los prefijos válidos.\n",
      "5) No respondas a la pregunta; solo devuelve el docid.\"\n",
      "\n",
      "user\n",
      "\n",
      "        Dada la siguiente consulta, recupera los identificadores de los documentos relevantes. \n",
      "        Consulta: ¿Cuántas votaciones se realizaron en la sesión plenaria del 29 de marzo de 2007 sobre la proposición no de ley del Grupo Parlamentario Popular relativa al sector vitivinícola, y cuál fue el resultado de dicha votación?\n",
      "        Proporciona los identificadores de los documentos en el siguiente formato, uno por línea:\n",
      "        \n",
      "assistant\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Gracias por tu pregunta. Recuperamos los identificadores de los documentos relevantes. El documento correspondiente a la proposición no de ley del Grupo Parlamentario Popular sobre el sector vitivinícola es el siguiente identificador: 6000_10.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test the model in streaming mode\n",
    "from transformers import TextStreamer\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=False, skip_special_tokens=True)\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 64, # Increase for longer outputs!\n",
    "    temperature = 0.001,\n",
    "    streamer = streamer,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
