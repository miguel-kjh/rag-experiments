{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "473b050c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miguel/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 11-12 11:17:53 [__init__.py:216] Automatically detected platform cuda.\n",
      "WARNING 11-12 11:17:54 [interface.py:391] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import torch\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9d8b356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.9.8: Fast Llama patching. Transformers: 4.56.2. vLLM: 0.10.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.988 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.9.8 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 360,710,144 || all params: 1,596,524,544 || trainable%: 22.5935\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B-instruct\"\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = MAX_LENGTH,\n",
    "    full_finetuning=False,\n",
    "    load_in_4bit = True,\n",
    "    load_in_8bit = False,\n",
    ")\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.eos_token_id\n",
    "RANK = 512\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = RANK,           # Choose any number > 0! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = RANK*2,  # Best to choose alpha = rank or rank*2\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = SEED,\n",
    "    use_rslora = False,   # We support rank stabilized LoRA\n",
    "    loftq_config = None,  # And LoftQ\n",
    ")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb336663",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e47c08f",
   "metadata": {},
   "source": [
    "### Simple dataset loading example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2974d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSAMPLE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6bb16e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400 documents.\n",
      "First document: page_content='Nombre: Alba Alonso\n",
      "TelÃ©fono: 632 322 183' metadata={'id': '7500_1'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain.schema import Document\n",
    "dataset_knowledge = pd.read_csv(\"../notebooks/data/contacts_docs.csv\")\n",
    "documents = []\n",
    "for index, row in dataset_knowledge.iterrows():\n",
    "    doc = f\"Nombre: {row['name']}\\nTelÃ©fono: {row['phone']}\"\n",
    "    documents.append(Document(page_content=doc, metadata={\"id\": f\"{row['id']}\" } ))\n",
    "print(f\"Loaded {len(documents)} documents.\")\n",
    "print(f\"First document: {documents[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3d5204e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_dataset_train = pd.read_csv(\"../notebooks/data/contacts_queries_train.csv\")\n",
    "query_dataset_val = pd.read_csv(\"../notebooks/data/contacts_queries_val.csv\")\n",
    "query_dataset_test = pd.read_csv(\"../notebooks/data/contacts_queries_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50f936fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = {\n",
    "    \"train\": query_dataset_train,\n",
    "    \"validation\": query_dataset_val,\n",
    "    \"test\": query_dataset_test,\n",
    "}\n",
    "\n",
    "#to hugginface dataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "dataset_qa = {}\n",
    "for split in all_data:\n",
    "    dataset_qa[split] = Dataset.from_pandas(all_data[split])\n",
    "dataset_qa = DatasetDict(dataset_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e7f1812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'id', 'respuesta'],\n",
       "        num_rows: 1400\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'id', 'respuesta'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'id', 'respuesta'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7952bc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JOIN TRAIN AND VAL DATASETS\n",
    "from datasets import concatenate_datasets\n",
    "dataset_qa[\"train\"] = concatenate_datasets([dataset_qa[\"train\"], dataset_qa[\"validation\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "950c17d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename \"respuesta\" column to \"answer\"\n",
    "dataset_qa = dataset_qa.rename_column(\"respuesta\", \"answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c12e213c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'id', 'answer'],\n",
       "        num_rows: 1700\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'id', 'answer'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'id', 'answer'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_qa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1efe551",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "680e0009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_it(tokenizer, system_prompt: str, prompt: str, response: str) -> str:\n",
    "    \"\"\"Builds the chat prompt for a single example using the tokenizer chat template.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\",   \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": response}\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5126e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_knowledge_injection_prompts(documents: list):\n",
    "    prompt = \"\"\"{doc}\"\"\"\n",
    "    for doc in documents:\n",
    "        yield prompt.format(doc=doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d4d4258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_prompts(dataset, tokenizer):\n",
    "    system_prompt = \"\"\"\n",
    "    Eres un modelo de lenguaje entrenado para responder preguntas.\n",
    "    \"\"\"\n",
    "    prompts = []\n",
    "    for item in dataset:\n",
    "        prompt = \"\"\"{QUERY}\"\"\"\n",
    "        response = \"{response}\"\n",
    "        question = item[\"question\"]\n",
    "        prompt = prompt.format(QUERY=question)\n",
    "        prompts.append(build_prompt_it(tokenizer, system_prompt, prompt, response.format(response=item[\"answer\"])))\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b744a024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prompts: 400\n"
     ]
    }
   ],
   "source": [
    "prompts = list(generate_knowledge_injection_prompts(documents))\n",
    "print(f\"Number of prompts: {len(prompts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7b9ed38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nombre: Alba Alonso\\nTelÃ©fono: 632 322 183'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d86e8e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 15 \n",
      "\n",
      "Tokens: ['Nombre', ':', 'Ä Al', 'ba', 'Ä Alonso', 'ÄŠ', 'Tel', 'ÃƒÂ©fono', ':', 'Ä ', '632', 'Ä ', '322', 'Ä ', '183']\n"
     ]
    }
   ],
   "source": [
    "# QUIERO VER LOS TOKENS\n",
    "def print_tokens(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    print(\"Number of tokens:\", len(tokens), \"\\n\")\n",
    "    print(\"Tokens:\", tokens)\n",
    "\n",
    "print_tokens(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed65b3b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 400\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataset from prompts\n",
    "from datasets import Dataset\n",
    "knowledge_dataset = Dataset.from_dict({\"text\": prompts})\n",
    "knowledge_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cda96468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nombre: Alba Alonso\\nTelÃ©fono: 632 322 183'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knowledge_dataset[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ee44269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of retrieval prompts: 1700\n",
      "Number of retrieval prompts: 300\n"
     ]
    }
   ],
   "source": [
    "prompts_qa_train = generate_qa_prompts(dataset_qa[\"train\"], tokenizer)\n",
    "prompts_qa_val = generate_qa_prompts(dataset_qa[\"test\"], tokenizer)\n",
    "\n",
    "print(f\"Number of retrieval prompts: {len(prompts_qa_train)}\")\n",
    "print(f\"Number of retrieval prompts: {len(prompts_qa_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ab6c0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 12 Nov 2025\n",
      "\n",
      "Eres un modelo de lenguaje entrenado para responder preguntas.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Necesito el contacto asociado al 620 152 344. â€”consulta internaâ€”<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "El nÃºmero 620 152 344 pertenece a Alejandro Vega.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(prompts_qa_train[0], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87e1cfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 87 \n",
      "\n",
      "Tokens: ['<|begin_of_text|>', '<|start_header_id|>', 'system', '<|end_header_id|>', 'ÄŠÄŠ', 'Cut', 'ting', 'Ä Knowledge', 'Ä Date', ':', 'Ä December', 'Ä ', '202', '3', 'ÄŠ', 'Today', 'Ä Date', ':', 'Ä ', '12', 'Ä Nov', 'Ä ', '202', '5', 'ÄŠÄŠ', 'E', 'res', 'Ä un', 'Ä modelo', 'Ä de', 'Ä l', 'engu', 'aje', 'Ä entren', 'ado', 'Ä para', 'Ä responder', 'Ä preg', 'untas', '.', '<|eot_id|>', '<|start_header_id|>', 'user', '<|end_header_id|>', 'ÄŠÄŠ', 'N', 'ec', 'es', 'ito', 'Ä el', 'Ä contacto', 'Ä asoci', 'ado', 'Ä al', 'Ä ', '620', 'Ä ', '152', 'Ä ', '344', '.', 'Ä Ã¢Ä¢Ä¶', 'consulta', 'Ä intern', 'a', 'Ã¢Ä¢Ä¶', '<|eot_id|>', '<|start_header_id|>', 'assistant', '<|end_header_id|>', 'ÄŠÄŠ', 'El', 'Ä nÃƒÂºmero', 'Ä ', '620', 'Ä ', '152', 'Ä ', '344', 'Ä pert', 'ene', 'ce', 'Ä a', 'Ä Alejandro', 'Ä Vega', '.', '<|eot_id|>']\n"
     ]
    }
   ],
   "source": [
    "print_tokens(prompts_qa_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97de98e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset from prompts train, val, test\n",
    "qa_train_dataset = Dataset.from_dict({\"text\": prompts_qa_train})\n",
    "qa_val_dataset = Dataset.from_dict({\"text\": prompts_qa_val})\n",
    "\n",
    "qa_dataset = {\n",
    "    \"train\": qa_train_dataset,\n",
    "    \"validation\": qa_val_dataset,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60314e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function_autoregressive(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a0505aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:00<00:00, 17538.57 examples/s]\n"
     ]
    }
   ],
   "source": [
    "knowledge_dataset_tokenizer = knowledge_dataset.map(tokenize_function_autoregressive, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "898f2e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1700/1700 [00:00<00:00, 20599.52 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 15839.32 examples/s]\n"
     ]
    }
   ],
   "source": [
    "qa_train_dataset_tokenizer = qa_dataset[\"train\"].map(tokenize_function_autoregressive, batched=True)\n",
    "qa_val_dataset_tokenizer = qa_dataset[\"validation\"].map(tokenize_function_autoregressive, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7136411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_prompts_testing(dataset, tokenizer):\n",
    "    def build_prompt_it_generation(tokenizer, system_prompt: str, prompt: str) -> str:\n",
    "        \"\"\"Builds the chat prompt for a single example using the tokenizer chat template.\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\",   \"content\": prompt},\n",
    "        ]\n",
    "        return tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False,\n",
    "        )\n",
    "    system_prompt = \"\"\"\n",
    "    Eres un modelo de lenguaje entrenado para responder preguntas.\n",
    "    \"\"\"\n",
    "    prompts = []\n",
    "    for item in dataset:\n",
    "        prompt = \"\"\"{QUERY}\"\"\"\n",
    "        question = item[\"question\"]\n",
    "        answer = item[\"answer\"]\n",
    "        prompt = prompt.format(QUERY=question)\n",
    "        prompts.append(\n",
    "            (\n",
    "                build_prompt_it_generation(tokenizer, system_prompt, prompt),\n",
    "                answer,\n",
    "            )\n",
    "        )\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8379bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 12 Nov 2025\n",
      "\n",
      "Eres un modelo de lenguaje entrenado para responder preguntas.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Â¿A quiÃ©n pertenece el telÃ©fono 736 615 948?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts_retrieval_test = generate_qa_prompts_testing(dataset_qa[\"test\"], tokenizer)\n",
    "text_for_testing = prompts_retrieval_test[0][0]\n",
    "print(text_for_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4632b63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tqdm\n",
    "import torch\n",
    "\n",
    "def levenshtein(s: str, t: str) -> int:\n",
    "    \"\"\"\n",
    "    Distancia de Levenshtein con memoria O(min(len(s), len(t))).\n",
    "    \"\"\"\n",
    "    # Asegura que t es la mÃ¡s corta para usar menos memoria\n",
    "    if len(s) < len(t):\n",
    "        s, t = t, s\n",
    "    previous = list(range(len(t) + 1))\n",
    "\n",
    "    for i, cs in enumerate(s, start=1):\n",
    "        current = [i]\n",
    "        for j, ct in enumerate(t, start=1):\n",
    "            costo = 0 if cs == ct else 1\n",
    "            current.append(min(\n",
    "                current[-1] + 1,          # inserciÃ³n\n",
    "                previous[j] + 1,          # borrado\n",
    "                previous[j-1] + costo     # sustituciÃ³n\n",
    "            ))\n",
    "        previous = current\n",
    "    return previous[-1]\n",
    "\n",
    "\n",
    "def test_model_accuracy(model, tokenizer, prompts_retrieval_test, device=\"cuda\", \n",
    "                        max_new_tokens=64, temperature=0.0, top_p=1.0):\n",
    "    \n",
    "    acc = 0\n",
    "    lev = 0\n",
    "    total = 0\n",
    "\n",
    "    progress_bar = tqdm.tqdm(prompts_retrieval_test, desc=\"Testing\")\n",
    "\n",
    "    for text, answer in progress_bar:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p\n",
    "            )\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = generated_text.split(\"assistant\")[-1].strip()\n",
    "\n",
    "        if response == answer:\n",
    "            acc += 1\n",
    "        lev += levenshtein(response, answer)\n",
    "        total += 1\n",
    "\n",
    "        progress_bar.set_postfix({\n",
    "            \"acc\": f\"{acc/total*100:.2f} %\",\n",
    "            \"lev\": f\"{lev/total:.2f}\"\n",
    "        })\n",
    "\n",
    "    final_acc = acc / total * 100 if total > 0 else 0.0\n",
    "    print(f\"Accuracy final: {acc}/{total} = {final_acc:.2f} %\")\n",
    "    final_lev = lev / total if total > 0 else 0.0\n",
    "    print(f\"Levenshtein final: {final_lev:.2f}\")\n",
    "    return final_acc, final_lev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18dd53aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Genera grÃ¡ficos de la evoluciÃ³n de la accuracy y las pÃ©rdidas (SFT e IT) durante el entrenamiento.\n",
    "\n",
    "    ParÃ¡metros:\n",
    "        history (list[dict]): lista de diccionarios, donde cada elemento debe contener:\n",
    "            - \"super_epoch\": nÃºmero de super-Ã©poca.\n",
    "            - \"accuracy\": precisiÃ³n alcanzada.\n",
    "            - \"trainer_sft_stats\".training_loss: pÃ©rdida SFT.\n",
    "            - \"trainer_it_stats\".training_loss: pÃ©rdida IT.\n",
    "    \"\"\"\n",
    "    # Extraer mÃ©tricas del historial\n",
    "    super_epochs = [h[\"super_epoch\"] for h in history]\n",
    "    accuracies = [h[\"accuracy\"] for h in history]\n",
    "    losses_sft = [h[\"trainer_sft_stats\"].training_loss for h in history]\n",
    "    losses_it = [h[\"trainer_it_stats\"].training_loss for h in history]\n",
    "\n",
    "    # --- GrÃ¡fico de Accuracy ---\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(super_epochs, accuracies, marker='o', color='tab:blue')\n",
    "    plt.xlabel(\"Super Epoch\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.title(\"Accuracy History\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- GrÃ¡fico de Losses ---\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(super_epochs, losses_sft, marker='o', label=\"SFT Loss\", color='tab:orange')\n",
    "    plt.plot(super_epochs, losses_it, marker='s', label=\"IT Loss\", color='tab:green')\n",
    "    plt.xlabel(\"Super Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss History\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa230b46",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ace130b",
   "metadata": {},
   "source": [
    "### Iterative training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01d9a429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sft training\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "auto_config = UnslothTrainingArguments(\n",
    "    per_device_train_batch_size = 8,\n",
    "    gradient_accumulation_steps = 8, # Use GA to mimic batch size!\n",
    "    save_strategy=\"no\",\n",
    "    save_total_limit=0,\n",
    "    warmup_steps = 5,\n",
    "    num_train_epochs = 1, # Set this for 1 full training run.\n",
    "    #max_steps = 60,\n",
    "    learning_rate = 1e-4, # Reduce to 2e-5 for long training runs\n",
    "    logging_steps = 1,\n",
    "    # 32 bits\n",
    "    optim = \"paged_adamw_32bit\",\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    seed = SEED,\n",
    "    report_to = \"none\", # Use this for WandB etc\n",
    "    output_dir=\"../models/qwen3-0.6b-rag-indexer\",\n",
    ")\n",
    "\n",
    "it_config = SFTConfig(\n",
    "    dataset_text_field=\"text\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,        # <-- aÃ±ade eval batch size\n",
    "    gradient_accumulation_steps=8,\n",
    "    warmup_steps=25,\n",
    "    save_strategy=\"no\",\n",
    "    save_total_limit=0,\n",
    "    eval_steps=1,\n",
    "    eval_strategy=\"steps\",         # <-- activa evaluaciÃ³n periÃ³dica\n",
    "    num_train_epochs=1,             # <-- opcional: usa epochs en lugar de max_steps\n",
    "    #max_steps=30,\n",
    "    learning_rate=1e-4,\n",
    "    logging_steps=1,\n",
    "    optim = \"paged_adamw_32bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    seed=SEED,\n",
    "    report_to=\"none\",\n",
    "    output_dir=\"../models/qwen3-0.6b-rag-retriever\",\n",
    "    load_best_model_at_end=False,          # <-- opcional\n",
    "    metric_for_best_model=\"eval_loss\",    # <-- opcional\n",
    "    greater_is_better=False,              # <-- opcional\n",
    ")\n",
    "\n",
    "trainer_auto = UnslothTrainer(\n",
    "    model=model,\n",
    "    train_dataset=knowledge_dataset_tokenizer,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    args=auto_config,\n",
    ")\n",
    "\n",
    "trainer_it = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=qa_train_dataset_tokenizer,\n",
    "    eval_dataset=qa_val_dataset_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    args=it_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b15ec644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 360,710,144 || all params: 1,596,524,544 || trainable%: 22.5935\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c9a8816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmiguel_kjh\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/miguel/projects/rag-experiments/notebooks/wandb/run-20251112_111819-qxos7vrh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/miguel_kjh/qa_iterativo_agenda/runs/qxos7vrh' target=\"_blank\">meta-llama_Llama-3.2-1B-instruct_r512_qa_iterativo_agenda</a></strong> to <a href='https://wandb.ai/miguel_kjh/qa_iterativo_agenda' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/miguel_kjh/qa_iterativo_agenda' target=\"_blank\">https://wandb.ai/miguel_kjh/qa_iterativo_agenda</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/miguel_kjh/qa_iterativo_agenda/runs/qxos7vrh' target=\"_blank\">https://wandb.ai/miguel_kjh/qa_iterativo_agenda/runs/qxos7vrh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference, langchain, openai] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/miguel_kjh/qa_iterativo_agenda/runs/qxos7vrh?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7d04d7e24b80>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "name = f\"{model_name.replace('/', '_')}_r{RANK}_qa_iterativo_agenda\"\n",
    "\n",
    "# Inicia la sesiÃ³n de wandb\n",
    "wandb.init(\n",
    "    project=\"qa_iterativo_agenda\",\n",
    "    name=name,  # opcional\n",
    "    config={\n",
    "        \"super_epochs\": 10,\n",
    "        \"model\": model_name,\n",
    "        \"r\": RANK,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f708de6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SUPER EPOCH 1 / 10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 400 | Num Epochs = 1 | Total steps = 7\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 8 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 360,710,144 of 1,596,524,544 (22.59% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:11, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.456800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.408200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.169100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.797400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.203600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.899700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.934600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,700 | Num Epochs = 1 | Total steps = 27\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 8 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 360,710,144 of 1,596,524,544 (22.59% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 02:20, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.475700</td>\n",
       "      <td>6.465154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.415300</td>\n",
       "      <td>5.048048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.052300</td>\n",
       "      <td>3.762906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.724800</td>\n",
       "      <td>2.771720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.750500</td>\n",
       "      <td>2.150628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.148200</td>\n",
       "      <td>1.666684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.632200</td>\n",
       "      <td>1.280813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.268300</td>\n",
       "      <td>0.945208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.982500</td>\n",
       "      <td>0.794286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.816600</td>\n",
       "      <td>0.761761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.763200</td>\n",
       "      <td>0.735973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.723000</td>\n",
       "      <td>0.688336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.703000</td>\n",
       "      <td>0.669382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.667700</td>\n",
       "      <td>0.633649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.638600</td>\n",
       "      <td>0.632779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.622400</td>\n",
       "      <td>0.609683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.607800</td>\n",
       "      <td>0.598173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.607200</td>\n",
       "      <td>0.577288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.580400</td>\n",
       "      <td>0.540132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.525300</td>\n",
       "      <td>0.497534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.497400</td>\n",
       "      <td>0.418326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.419000</td>\n",
       "      <td>0.421465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.424300</td>\n",
       "      <td>0.405449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.409300</td>\n",
       "      <td>0.388112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.387300</td>\n",
       "      <td>0.395284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.393000</td>\n",
       "      <td>0.403028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.401700</td>\n",
       "      <td>0.396117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=27, training_loss=1.505070862946687, metrics={'train_runtime': 145.223, 'train_samples_per_second': 11.706, 'train_steps_per_second': 0.186, 'total_flos': 3482965455667200.0, 'train_loss': 1.505070862946687, 'epoch': 1.0})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [01:57<00:00,  2.55it/s, acc=0.00 %, lev=8.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy final: 0/300 = 0.00 %\n",
      "Levenshtein final: 8.84\n",
      "--- SUPER EPOCH 2 / 10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 400 | Num Epochs = 1 | Total steps = 7\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 8 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 360,710,144 of 1,596,524,544 (22.59% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:11, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.852500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.813000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.311600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.180400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.046000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.934400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.670400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,700 | Num Epochs = 1 | Total steps = 27\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 8 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 360,710,144 of 1,596,524,544 (22.59% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 3/27 00:05 < 02:06, 0.19 it/s, Epoch 0.08/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.691100</td>\n",
       "      <td>0.691466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/38 00:00 < 00:02, 14.79 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- SUPER EPOCH \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m trainer_sft_stats \u001b[38;5;241m=\u001b[39m trainer_auto\u001b[38;5;241m.\u001b[39mtrain() \n\u001b[0;32m----> 6\u001b[0m trainer_it_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer_it\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(trainer_it_stats)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# evaluate accuracy after each super epoch\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/rag-experiments/notebooks/unsloth_compiled_cache/UnslothSFTTrainer.py:53\u001b[0m, in \u001b[0;36mprepare_for_training_mode.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor_training\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfor_training()\n\u001b[0;32m---> 53\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Return inference mode\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor_inference\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2328\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2326\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:407\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:3227\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[1;32m   3225\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 3227\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3228\u001b[0m     is_new_best_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_determine_best_metric(metrics\u001b[38;5;241m=\u001b[39mmetrics, trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m   3230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_strategy \u001b[38;5;241m==\u001b[39m SaveStrategy\u001b[38;5;241m.\u001b[39mBEST:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:3176\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   3175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 3176\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3177\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   3179\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:4469\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4466\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   4468\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 4469\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4470\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4472\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   4473\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   4474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4477\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4479\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   4480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:4722\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4719\u001b[0m         all_inputs\u001b[38;5;241m.\u001b[39mto_cpu_and_numpy()\n\u001b[1;32m   4721\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m losses, logits, labels, inputs\n\u001b[0;32m-> 4722\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4724\u001b[0m \u001b[38;5;66;03m# After all calls to `.gather_function`, reset to `gather_for_metrics`:\u001b[39;00m\n\u001b[1;32m   4725\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather_for_metrics\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/cuda/memory.py:224\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 224\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._post_run_cell_hook of <wandb.sdk.wandb_init._WandbInit object at 0x7d04e21cc040>> (for post_run_cell), with arguments args (<ExecutionResult object at 7d04e21cf040, execution_count=34 error_before_exec=None error_in_exec= info=<ExecutionInfo object at 7d04d8ac25f0, raw_cell=\"EPOCHS = 10\n",
      "\n",
      "for _ in range(EPOCHS):\n",
      "    print(f\"-..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://wsl%2Bubuntu/home/miguel/projects/rag-experiments/notebooks/train_llm_retriver.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "socket.send() raised exception.\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for _ in range(EPOCHS):\n",
    "    print(f\"--- SUPER EPOCH {_+1} / {EPOCHS} ---\")\n",
    "    trainer_sft_stats = trainer_auto.train() \n",
    "    trainer_it_stats = trainer_it.train()\n",
    "    print(trainer_it_stats)\n",
    "    # evaluate accuracy after each super epoch\n",
    "    acc, lev = test_model_accuracy(model, tokenizer, prompts_retrieval_test, device=\"cuda\")\n",
    "    wandb.log({\n",
    "        \"super_epoch\": _ + 1,\n",
    "        \"accuracy\": acc,\n",
    "        \"levenshtein\": lev,\n",
    "        \"train_loss_sft\": trainer_sft_stats.training_loss,\n",
    "        \"train_loss_it\": trainer_it_stats.training_loss,\n",
    "    })\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871e0002",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer_it_stats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer_it_stats\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer_it_stats' is not defined"
     ]
    }
   ],
   "source": [
    "trainer_it_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47228fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El nÃºmero 736 615 948 pertenece a Leo PÃ©rez.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# test the model in streaming mode\n",
    "from transformers import TextStreamer\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=False)\n",
    "_ = model.generate(\n",
    "    **tokenizer(text_for_testing, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 64, # Increase for longer outputs!\n",
    "    do_sample = False,\n",
    "    top_p = 0.1,\n",
    "    temperature = 0.,\n",
    "    streamer = streamer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a9e347",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [01:45<00:00,  2.84it/s, acc=91.00 %, lev=0.80]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy final: 273/300 = 91.00 %\n",
      "Levenshtein final: 241/300 = 0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "acc, lev = test_model_accuracy(model, tokenizer, prompts_retrieval_test, device=\"cuda\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
