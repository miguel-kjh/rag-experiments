{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81f2475e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miguel/miniconda3/envs/RAG/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "SEED = 123\n",
    "FOLDER = \"../data/processed/legal-qa-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be2245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32ba1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the dataset\n",
    "print(dataset)\n",
    "print(dataset['train'][0])\n",
    "print(dataset['validation'][0])\n",
    "print(dataset['test'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e2c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e060b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(example):\n",
    "    # ejemplo: poner preguntas en mayúsculas y respuestas en minúsculas\n",
    "    new_question = example[\"question\"].replace(\"Q:\", \"\").strip()\n",
    "    new_answer = example[\"answer\"].replace(\"A:\", \"\").strip()\n",
    "\n",
    "    return {\n",
    "        \"question\": new_question,\n",
    "        \"answer\": new_answer\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f182abeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = dataset.map(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413ae487",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transformed_dataset[\"question\"][0])\n",
    "print(transformed_dataset[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d472793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into train and validation\n",
    "train_test = transformed_dataset.train_test_split(test_size=0.2, seed=SEED)\n",
    "print(train_test)\n",
    "train_val = train_test[\"test\"].train_test_split(test_size=0.5, seed=SEED)\n",
    "transformed_dataset = {\n",
    "    \"train\": train_test[\"train\"],\n",
    "    \"val\": train_val[\"test\"],\n",
    "    \"test\": train_val[\"train\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c6d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef6830f",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e462fcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miguel/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "class SentenceTransformerEmbeddings(Embeddings):\n",
    "\n",
    "    def __init__(self, model: str, device: str = 'cuda'):\n",
    "        self.model = SentenceTransformer(model, device=device)\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed search docs.\"\"\"\n",
    "        return self.model.encode(texts).tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed query text.\"\"\"\n",
    "        return self.model.encode_query(text).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97d99403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vector store contains 128518 vectors\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model = HuggingFaceEmbeddings(\n",
    "    model_name=\"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    model_kwargs={\"device\": \"cuda\"}\n",
    ")\n",
    "\n",
    "loaded_vectorstore=FAISS.load_local(\n",
    "    \"../data/db/parliament_db/parliament_all_docs_embeddings_Qwen_Qwen3-Embedding-0.6B_chunked_max_length-512\",\n",
    "    model,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "print(f\"Loaded vector store contains {loaded_vectorstore.index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "719abab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All embeddings shape: (128518, 1024)\n"
     ]
    }
   ],
   "source": [
    "# get all embeddings in numpy array\n",
    "import numpy as np\n",
    "all_embeddings = np.array([emb for emb in loaded_vectorstore.index.reconstruct_n(0, loaded_vectorstore.index.ntotal)])\n",
    "print(f\"All embeddings shape: {all_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e820e759",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 61.5 GiB for an array with shape (128518, 128518) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[0;32m----> 6\u001b[0m cosine_similarities \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# get the upper triangle of the cosine similarities matrix, without the diagonal\u001b[39;00m\n\u001b[1;32m      8\u001b[0m upper_triangle_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtriu_indices_from(cosine_similarities, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:1687\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1685\u001b[0m     Y_normalized \u001b[38;5;241m=\u001b[39m normalize(Y, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 1687\u001b[0m K \u001b[38;5;241m=\u001b[39m \u001b[43msafe_sparse_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_normalized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdense_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdense_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m K\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/extmath.py:205\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    203\u001b[0m         ret \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(a, b)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    208\u001b[0m     sparse\u001b[38;5;241m.\u001b[39missparse(a)\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(b)\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m dense_output\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ret, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoarray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    212\u001b[0m ):\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39mtoarray()\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 61.5 GiB for an array with shape (128518, 128518) and data type float32"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# plot histogram of the embeddings cosine similarity distribution\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarities = cosine_similarity(all_embeddings)\n",
    "# get the upper triangle of the cosine similarities matrix, without the diagonal\n",
    "upper_triangle_indices = np.triu_indices_from(cosine_similarities, k=1)\n",
    "upper_triangle_values = cosine_similarities[upper_triangle_indices]\n",
    "\n",
    "plt.hist(upper_triangle_values, bins=50)\n",
    "plt.grid()\n",
    "plt.title(\"Cosine Similarity Distribution of Embeddings\")\n",
    "plt.xlabel(\"Cosine Similarity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54e48a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cosine Similarity: 0.7246\n",
      "Standard Deviation of Cosine Similarity: 0.1009\n"
     ]
    }
   ],
   "source": [
    "# print the mean and standard deviation of the cosine similarities\n",
    "mean_cosine_similarity = np.mean(upper_triangle_values)\n",
    "std_cosine_similarity = np.std(upper_triangle_values)\n",
    "# print the results rounded to 4 decimal places\n",
    "print(f\"Mean Cosine Similarity: {mean_cosine_similarity:.4f}\")\n",
    "print(f\"Standard Deviation of Cosine Similarity: {std_cosine_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1f3e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# load train/validation/test splits of individual subset\n",
    "ragbench_hotpotqa = load_dataset(\"rungalileo/ragbench\", \"hotpotqa\")\n",
    "print(ragbench_hotpotqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e65861",
   "metadata": {},
   "outputs": [],
   "source": [
    "ragbench = {}\n",
    "for dataset in ['covidqa', 'cuad', 'delucionqa', 'emanual', 'expertqa', 'finqa', 'hagrid', 'hotpotqa', 'msmarco', 'pubmedqa', 'tatqa', 'techqa']:\n",
    "  ragbench[dataset] = load_dataset(\"rungalileo/ragbench\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4861e3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ragbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9723d038",
   "metadata": {},
   "outputs": [],
   "source": [
    "ragbench[\"covidqa\"][\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6e8c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "## quiero solo estas columnas\n",
    "columns_to_keep = [\"id\", \"question\", \"documents\", \"response\"]\n",
    "for subset in ragbench:\n",
    "    ragbench[subset] = ragbench[subset].remove_columns([col for col in ragbench[subset]['train'].column_names if col not in columns_to_keep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc09d298",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = []\n",
    "for split in ragbench[\"covidqa\"]:\n",
    "    for docs in ragbench[\"covidqa\"][split][\"documents\"]:\n",
    "        for doc in docs:\n",
    "            document.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c931bce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a23e1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unique documents in alphabetical order\n",
    "len(set(document))\n",
    "unique_documents = list(set(document))\n",
    "unique_documents.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734422a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column with the idx of the document in the list of unique documents, the idx should be a list of integers and the name of the column should be \"document_ids\"\n",
    "unique_documents = list(set(document))\n",
    "unique_documents.sort()\n",
    "document_idx_map = {doc: idx for idx, doc in enumerate(unique_documents)}\n",
    "for split in ragbench[\"covidqa\"]:\n",
    "    ragbench[\"covidqa\"][split] = ragbench[\"covidqa\"][split].add_column(\n",
    "        \"document_ids\",\n",
    "        [[document_idx_map[doc] for doc in docs] for docs in ragbench[\"covidqa\"][split][\"documents\"]]\n",
    "    )\n",
    "ragbench[\"covidqa\"][\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09a9632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk the list of unique documents in jsonl format\n",
    "import os\n",
    "import json\n",
    "\n",
    "with open(\"../data/processed/ragbench/covidqa_unique_documents.jsonl\", \"w\") as f:\n",
    "    for doc in unique_documents:\n",
    "        f.write(json.dumps({\"document\": doc}) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a821a1",
   "metadata": {},
   "source": [
    "# Parliamentary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae110394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "FOLDER_AUTORE = \"../data/raw/ORDERS_PARLIAMENT\" # Entrenamiento autoregresivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7ba703",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_aut = load_from_disk(FOLDER_AUTORE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b423a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_aut[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5eba81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_aut['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c347b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['PK', 'question', 'answer', 'cost', 'context', 'type', 'retrieved_pks', 'oracle_context', 'formatted_context'],\n",
      "        num_rows: 614\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['PK', 'question', 'answer', 'cost', 'context', 'type', 'retrieved_pks', 'oracle_context', 'formatted_context'],\n",
      "        num_rows: 161\n",
      "    })\n",
      "})\n",
      "Dataset({\n",
      "    features: ['PK', 'question', 'answer', 'cost', 'context', 'type', 'retrieved_pks', 'oracle_context', 'formatted_context'],\n",
      "    num_rows: 614\n",
      "})\n",
      "{\n",
      "  \"PK\": \"6521_1\",\n",
      "  \"question\": \"\\u00bfQu\\u00e9 argumentos expuso el grupo parlamentario que se opuso a la propuesta de modificaci\\u00f3n del orden del d\\u00eda en la sesi\\u00f3n del 26 de septiembre de 2023, que implicaba la convalidaci\\u00f3n del decreto relativo al impuesto de sucesiones y donaciones?\",\n",
      "  \"answer\": \"El fragmento del Diario de Sesiones no proporciona argumentos espec\\u00edficos de ning\\u00fan grupo parlamentario que se oponga a la propuesta de modificaci\\u00f3n del orden del d\\u00eda relacionada con la convalidaci\\u00f3n del decreto sobre el impuesto de sucesiones y donaciones. La presidenta menciona que se somete la propuesta a votaci\\u00f3n por asentimiento y no se registran objeciones ni intervenciones en contra. Por lo tanto, no se pueden extraer argumentos de oposici\\u00f3n de los grupos parlamentarios en este caso.\",\n",
      "  \"cost\": 0.00011774999999999999,\n",
      "  \"context\": \"Esta sesi\\u00f3n del parlamento se realiz\\u00f3 el 2023-09-26. PROPUESTA DE ALTERACI\\u00d3N DEL ORDEN DEL D\\u00cdA La se\\u00f1ora PRESIDENTA: Como cuesti\\u00f3n previa s\\u00ed nos gustar\\u00eda plantear una propuesta de modificaci\\u00f3n del orden del d\\u00eda, que, como bien saben, debo someterla a votaci\\u00f3n por asentimiento. Por tanto, simplemente que los grupos, los portavoces de los grupos parlamentarios asientan sobre esta propuesta de modificaci\\u00f3n del orden del d\\u00eda. \\u00bfAlguna cuesti\\u00f3n en contra de la propuesta de modificaci\\u00f3n? Como bien saben, se trataba de modificar y sustituir el decreto, la convalidaci\\u00f3n del decreto relativo al impuesto de sucesiones y donaciones, poder debatirlo despu\\u00e9s de las preguntas y antes de las comparecencias. Esa es la modificaci\\u00f3n del orden del d\\u00eda que les planteo, y, por tanto, por asentimiento imagino que no habr\\u00e1 ning\\u00fan problema, \\u00bfverdad? (Asentimiento). Bueno, pues no habiendo nada que alegar, pasamos al orden del d\\u00eda.\",\n",
      "  \"type\": \"motivation\",\n",
      "  \"retrieved_pks\": [\n",
      "    \"6521_1\",\n",
      "    \"6521_27\",\n",
      "    \"6516_15\",\n",
      "    \"6123_19\",\n",
      "    \"6553_8\",\n",
      "    \"6593_11\",\n",
      "    \"6598_19\"\n",
      "  ],\n",
      "  \"oracle_context\": true,\n",
      "  \"formatted_context\": \"[Documento]:\\n\\nEsta sesi\\u00f3n del parlamento se realiz\\u00f3 el 2023-09-26. PROPUESTA DE ALTERACI\\u00d3N DEL ORDEN DEL D\\u00cdA La se\\u00f1ora PRESIDENTA: Como cuesti\\u00f3n previa s\\u00ed nos gustar\\u00eda plantear una propuesta de modificaci\\u00f3n del orden del d\\u00eda, que, como bien saben, debo someterla a votaci\\u00f3n por asentimiento. Por tanto, simplemente que los grupos, los portavoces de los grupos parlamentarios asientan sobre esta propuesta de modificaci\\u00f3n del orden del d\\u00eda. \\u00bfAlguna cuesti\\u00f3n en contra de la propuesta de modificaci\\u00f3n? Como bien saben, se trataba de modificar y sustituir el decreto, la convalidaci\\u00f3n del decreto relativo al impuesto de sucesiones y donaciones, poder debatirlo despu\\u00e9s de las preguntas y antes de las comparecencias. Esa es la modificaci\\u00f3n del orden del d\\u00eda que les planteo, y, por tanto, por asentimiento imagino que no habr\\u00e1 ning\\u00fan problema, \\u00bfverdad? (Asentimiento). Bueno, pues no habiendo nada que alegar, pasamos al orden del d\\u00eda.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "FOLDER_QA = \"../data/raw/QA_PARLIAMENT_TRAIN\"\n",
    "\n",
    "dataset_qa = load_from_disk(FOLDER_QA)\n",
    "print(dataset_qa)\n",
    "print(dataset_qa[\"train\"])\n",
    "print(json.dumps(dataset_qa['train'][3], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18614b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = dataset_qa[\"train\"][10][\"context\"]\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3734167",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.replace(\"[Documento]:\", \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d696e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "FOLDER_QA = \"../data/raw/QA_PARLIAMENT_TEST\"\n",
    "\n",
    "dataset_qa = load_from_disk(FOLDER_QA)\n",
    "print(dataset_qa[\"test\"])\n",
    "print(json.dumps(dataset_qa['test'][3], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb42da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = dataset_qa[\"test\"][3][\"formatted_context\"]\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac73a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "pks = dataset_qa[\"test\"][:]['PK']\n",
    "print(pks)\n",
    "# hay algunos pk vacíos?\n",
    "pks.count(\"\")\n",
    "\n",
    "pk_1 = pks[1]\n",
    "\n",
    "# buscar en dataset_aut \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d45bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(\"../data/processed/parliament_all_docs\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737655ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dataset:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
